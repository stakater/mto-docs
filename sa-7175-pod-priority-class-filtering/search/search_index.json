{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to the Docs","text":"<p>Sharing Kubernetes clusters can significantly reduce costs and streamline administration. It enables efficient resource utilization, reduces configuration overhead, and simplifies the sharing of internal cluster resources among tenants. However, achieving secure and functional multi-tenancy presents challenges such as ensuring security, maintaining fairness, and mitigating the impact of noisy neighbors.</p> <p>Kubernetes is inherently designed as a single-tenant platform. Managed Kubernetes services like AKS, EKS, GKE, and OpenShift have improved security through \"secure by default\" concepts, but designing and orchestrating all the moving parts required for a secure multi-tenant platform remains a complex task. This complexity makes it challenging for cluster administrators to host multiple tenants effectively within a single cluster.</p> <p>Clusters can be shared in various ways:</p> <ul> <li>Different applications might run in the same cluster.</li> <li>Multiple instances of the same application could operate within a single cluster, one for each end user.</li> </ul> <p>These scenarios are collectively referred to as multi-tenancy. While Kubernetes and many managed applications provide foundational resources to achieve multi-tenancy, leveraging these primitives requires professional expertise and deep knowledge of the platform.</p> <p>The Multi-Tenant Operator (MTO) builds on Kubernetes' capabilities, simplifying the orchestration of secure and efficient multi-tenancy. By addressing the unique needs of shared clusters, MTO helps cluster administrators overcome the inherent complexities of multi-tenancy, enabling them to harness its full potential.</p>"},{"location":"index.html#installation","title":"Installation","text":"<p>Refer to the installation guide for setting up MTO.</p>"},{"location":"changelog.html","title":"Changelog","text":""},{"location":"changelog.html#v12x","title":"v1.2.x","text":""},{"location":"changelog.html#v120","title":"v1.2.0","text":"<ul> <li>Newly released on RedHat OperatorHub for OpenShift 4.17 and 4.18 as well.</li> </ul>"},{"location":"changelog.html#features","title":"Features","text":"<ul> <li>Added the option to select Ingress class per tenant via Tenant CR</li> <li>Added CRUD for Quota and Tenant CR via console.</li> <li>Added support for AWS CloudCosts integration in Opencost</li> <li>Added support for node label filtering on Capacity Planning page</li> </ul>"},{"location":"changelog.html#enhancements","title":"Enhancements","text":"<ul> <li>Addition of percentage column in Cost Analysis data for better insights.</li> <li>Keycloak upgrade from <code>24.0.5</code> to <code>25.0.6</code>.</li> <li>Minor dependencies upgrade.</li> </ul>"},{"location":"changelog.html#pre-upgrade-checklist","title":"Pre-Upgrade Checklist","text":"<p>Before upgrading to v1.2.0, perform the following steps:</p> <ul> <li>Disable <code>console</code> in the integration config.</li> </ul>"},{"location":"changelog.html#post-upgrade-checklist","title":"Post-Upgrade Checklist","text":"<ul> <li>Enable <code>console</code> in the integration config. Link</li> <li>Delete <code>PersistentVolumeClaim</code> named <code>postgresql-data-mto-postgresql-0</code> and Pod named <code>mto-postgresql-0</code> for keycloak to reconfigure itself on the newer version.</li> <li>Delete all pods in <code>multi-tenant-operator</code> namespace having <code>controller</code> in their name, so data can be populated into the console again.</li> </ul>"},{"location":"changelog.html#v11x","title":"v1.1.x","text":""},{"location":"changelog.html#v110","title":"v1.1.0","text":""},{"location":"changelog.html#features_1","title":"Features","text":"<ul> <li>Added Azure Pricing support for Opencost via Integration Config.</li> <li>Added option to disable <code>Intra-tenant Networking</code> via Integration Config.</li> <li>Added Storage class per tenant support via Tenant CR.</li> <li>Added option to override component images.</li> <li>Added support to add/update <code>Casbin</code> policies via <code>tenant-operator-casbin-config</code> Configmap.</li> </ul>"},{"location":"changelog.html#enhancements_1","title":"Enhancements","text":"<ul> <li>Refactor <code>Casbin</code> model handling to use embedded files and simplify configuration.</li> <li>Switched to <code>Bitnami</code> images from <code>RedHat</code> images.</li> <li>Dependencies upgrade to kubernetes 1.29.8 and controller-runtime 0.17.6.</li> </ul>"},{"location":"changelog.html#fixes","title":"Fixes","text":"<ul> <li>Fix components under IC for tenant-operator chart.</li> <li>Fix for IC controller where reconciliation request was triggered with added user's name.</li> <li>Fix a bug in Vault policies where editor role had incorrect permissions.</li> <li>Fix a bug in sandbox creations where sandbox namespaces were not being created if there were only groups in the tenant.</li> <li>Fix for IC controller where it would not reconcile if the cluster had too many Configmaps, making the informer cache too big to be watched.</li> </ul>"},{"location":"changelog.html#pre-upgrade-checklist_1","title":"Pre-Upgrade Checklist","text":"<p>Before upgrading to v1.1.0, perform the following steps:</p> <ul> <li>Disable <code>console</code> in the integration config.</li> <li>Remove the <code>tenant-operator-casbin-config</code> Configmap from the <code>multi-tenant-operator</code> namespace, if it exists.</li> </ul>"},{"location":"changelog.html#post-upgrade-checklist_1","title":"Post-Upgrade Checklist","text":"<ul> <li>Enable <code>console</code> in the integration config. Link</li> <li>If the <code>prometheus-server</code> pod is failing, ensure that only one <code>prometheus-server</code> deployment exists in the <code>multi-tenant-operator</code> namespace. If multiple deployments exist, delete the older one.</li> </ul>"},{"location":"changelog.html#components","title":"Components","text":"Name Tag Image <code>tenant-operator</code> v1.1.0 <code>ghcr.io/stakater/public/mto/tenant-operator</code> <code>mto-console</code> 1.0.147 <code>ghcr.io/stakater/public/mto/mto-console</code> <code>mto-gateway</code> 1.0.134 <code>ghcr.io/stakater/public/mto/mto-gateway</code> <code>showback</code> v0.0.15 <code>ghcr.io/stakater/public/mto/showback</code> <code>keycloak</code> 24.0.5 <code>ghcr.io/stakater/public/mto/keycloak</code> <code>kube-state-metrics</code> v2.8.0 <code>ghcr.io/stakater/public/mto/kube-state-metrics</code> <code>postgresql</code> 15.0.0-debian-11-r1 <code>ghcr.io/stakater/public/mto/postgresql</code> <code>kube-rbac-proxy</code> v0.16.0 <code>ghcr.io/stakater/public/mto/kube-rbac-proxy</code> <code>opencost</code> 1.113.0 <code>ghcr.io/stakater/public/mto/opencost</code> <code>prometheus</code> 2.55.1-debian-12-r4 <code>ghcr.io/stakater/public/mto/prometheus</code>"},{"location":"changelog.html#v10x","title":"v1.0.x","text":""},{"location":"changelog.html#v100","title":"v1.0.0","text":""},{"location":"changelog.html#features_2","title":"Features","text":"<ul> <li>Added capacity planning feature on MTO Console to view resource usage of tenants based in their request and limits</li> <li>Added hibernation feature on MTO Console to view and manage hibernated namespaces and hibernate/unhibernate namespaces</li> </ul>"},{"location":"changelog.html#enhancements_2","title":"Enhancements","text":"<ul> <li>Increased the retention period of Prometheus to seven days</li> <li>Changed client for quota template IC pod to enable debugging</li> <li>Removed finalizers from namespaces: Having finalizers on namespaces have caused problems in the past especially when MTO previously ran on the cluster but is removed afterward, making namespaces stuck in deletion due to the presence of finalizers added by MTO</li> <li>Optimize cache for IC controller: Updated IC controller to not cache full CM objects but only their metadata, optimizing the size of the actual cache being created</li> <li>Added basic implementation for privileged users</li> <li>Removed limits from resources deployed via Pilot Controller</li> <li>Added user info support for database operations</li> <li>Updated image versions for Keycloak, MTO Console, and MTO Gateway</li> <li>Added job to create user for <code>Casbin</code> and embed configurations</li> </ul>"},{"location":"changelog.html#fixes_1","title":"Fixes","text":"<ul> <li>Updated configmap predicate to not compare data</li> <li>Restructure and fix <code>Casbin</code> rule creation: <code>Casbin</code> rules for privileged user and groups were missing that caused issue while accessing MTO Console</li> </ul>"},{"location":"changelog.html#components_1","title":"Components","text":"Name Tag Image <code>tenant-operator</code> v1.0.0 <code>ghcr.io/stakater/public/tenant-operator</code> <code>mto-console</code> 1.0.134 <code>ghcr.io/stakater/mto-console</code> <code>mto-gateway</code> 1.0.118 <code>ghcr.io/stakater/mto-gateway</code> <code>keycloak</code> 24.0.5 <code>ghcr.io/stakater/mto/keycloak</code> <code>kube-state-metrics</code> v2.8.0 <code>ghcr.io/stakater/mto/kube-state-metrics</code> <code>postgresql-15</code> 1-15 <code>ghcr.io/stakater/mto/postgresql-15</code> <code>showback</code> v0.0.12 <code>ghcr.io/stakater/showback</code> <code>configmap-reload</code> v0.13.0 <code>ghcr.io/jimmidyson/configmap-reload</code> <code>kube-rbac-proxy</code> v0.11.0 <code>gcr.io/kubebuilder/kube-rbac-proxy</code> <code>kubecost-cost-model</code> 1.108.0 <code>quay.io/kubecost1/kubecost-cost-model</code> <code>prometheus</code> v2.41.0 <code>quay.io/prometheus/prometheus</code> <code>pushgateway</code> v1.5.1 <code>quay.io/prometheus/pushgateway</code>"},{"location":"changelog.html#v012x","title":"v0.12.x","text":""},{"location":"changelog.html#v01219","title":"v0.12.19","text":""},{"location":"changelog.html#fix","title":"Fix","text":"<ul> <li>Fixed a recurring issue in the Extensions controller where status changes were triggering unnecessary reconciliation loops.</li> <li>Resolved a visibility issue where labels and annotations for sandbox namespaces were not appearing in the extension's status.</li> <li>Addressed an issue where AppProject was being deleted upon extension CR deletion, regardless of the <code>onDeletePurgeAppProject</code> field value.</li> <li>Optimized memory usage for Keycloak to address high consumption issues.</li> <li>Resolved an issue that was causing a panic in the Extension Controller when the IntegrationConfig (IC) was not present.</li> <li>Fixed an issue where the status was not being correctly updated when the entire Metadata block was removed from the Tenant specification.</li> </ul>"},{"location":"changelog.html#v01213","title":"v0.12.13","text":""},{"location":"changelog.html#fix_1","title":"Fix","text":"<ul> <li>Resolved an issue that was preventing Vault from authenticating using the <code>kubernetes</code> authentication method.</li> <li>Addressed an issue where changes to the IntegrationConfig were not updating the destination namespaces in ArgoCD's AppProject.</li> <li>Fixed a problem where the tenant controller was preventing namespace deletion if it failed to delete external dependencies, such as Vault.</li> </ul>"},{"location":"changelog.html#v0121","title":"v0.12.1","text":""},{"location":"changelog.html#fix_2","title":"Fix","text":"<ul> <li>Resolved memory consumption problems in multiple controllers by reducing the number of reconciliations.</li> </ul>"},{"location":"changelog.html#v0120","title":"v0.12.0","text":""},{"location":"changelog.html#feature","title":"Feature","text":""},{"location":"changelog.html#enhanced","title":"Enhanced","text":"<ul> <li>Updated Tenant CR to v1beta3, more details in Tenant CRD</li> <li>Added custom pricing support for Opencost, more details in Opencost</li> </ul>"},{"location":"changelog.html#fix_3","title":"Fix","text":"<ul> <li>Resolved an issue in Templates that prevented the deployment of public helm charts.</li> </ul>"},{"location":"changelog.html#v011x","title":"v0.11.x","text":""},{"location":"changelog.html#v0110","title":"v0.11.0","text":""},{"location":"changelog.html#feature_1","title":"Feature","text":"<ul> <li>Added support for configuring external keycloak in <code>integrationconfig</code>.</li> <li>Added free tier support that allows creation of 2 tenants without license.</li> </ul>"},{"location":"changelog.html#v010x","title":"v0.10.x","text":""},{"location":"changelog.html#v0106","title":"v0.10.6","text":""},{"location":"changelog.html#fix_4","title":"Fix","text":"<ul> <li>Fixed broken logs for namespace webhook where username and namespace were interchangeably used after a recent update</li> </ul>"},{"location":"changelog.html#enhanced_1","title":"Enhanced","text":"<ul> <li>Made log messages more elaborative and consistent on one format for namespace webhook</li> </ul>"},{"location":"changelog.html#v0105","title":"v0.10.5","text":""},{"location":"changelog.html#fix_5","title":"Fix","text":"<ul> <li><code>TemplateGroupInstance</code> controller now correctly updates the <code>TemplateGroupInstance</code> custom resource status and the namespace count upon the deletion of a namespace.</li> <li>Conflict between <code>TemplateGroupInstance</code> controller and <code>kube-contoller-manager</code> over mentioning of secret names in <code>secrets</code> or <code>imagePullSecrets</code> field in <code>ServiceAccounts</code> has been fixed by temporarily ignoring updates to or from <code>ServiceAccounts</code>.</li> </ul>"},{"location":"changelog.html#enhanced_2","title":"Enhanced","text":"<ul> <li>Privileged service accounts mentioned in the <code>IntegrationConfig</code> have now access over all types of namespaces. Previously operations were denied on orphaned namespaces (the namespaces which are not part of both privileged and tenant scope). More info in Troubleshooting Guide</li> <li><code>TemplateGroupInstance</code> controller now ensures that its underlying resources are force-synced when a namespace is created or deleted.</li> <li>Optimizations were made to ensure the reconciler in the TGI controller runs only once per watch event, reducing reconcile times.</li> <li>The <code>TemplateGroupInstance</code> reconcile flow has been refined to process only the namespace for which the event was received, streamlining resource creation/deletion and improving overall efficiency.</li> <li>Introduced new metrics to enhance the monitoring capabilities of the operator. Details at TGI Metrics Explanation</li> </ul>"},{"location":"changelog.html#v0100","title":"v0.10.0","text":""},{"location":"changelog.html#feature_2","title":"Feature","text":"<ul> <li>Added support for caching for MTO Console using PostgreSQL as caching layer.</li> <li>Added support for custom metrics with Template, Template Instance and Template Group Instance.</li> <li>Graph visualization of Tenant and its associated resources on MTO Console.</li> <li>Tenant and Admin level authz/authn support within MTO Console and Gateway.</li> <li>Now in MTO console you can view cost of different Tenant resources with different date, resource type and additional filters.</li> <li>MTO can now create a default keycloak realm, client and <code>mto-admin</code> user for Console.</li> <li>Implemented Cluster Resource Quota for vanilla Kubernetes platform type.</li> <li>Dependency of TLS secrets for MTO Webhook.</li> <li>Added Helm Chart that would be used for installing MTO over Kubernetes.<ul> <li>And it comes with default Cert Manager manifests for certificates.</li> </ul> </li> <li>Support for MTO e2e.</li> </ul>"},{"location":"changelog.html#fix_6","title":"Fix","text":"<ul> <li>Updated CreateMergePatch to MergeMergePatches to address issues caused by losing <code>resourceVersion</code> and UID when converting <code>oldObject</code> to <code>newObject</code>. This prevents problems when the object is edited by another controller.</li> <li>In Template Resource distribution for Secret type, we now consider the source's Secret field type, preventing default creation as Opaque regardless of the source's actual type.</li> <li>Enhanced admin permissions for tenant role in Vault to include Create, Update, Delete alongside existing Read and List privileges for the common-shared-secrets path. Viewers now have Read permission.</li> </ul>"},{"location":"changelog.html#enhanced_3","title":"Enhanced","text":"<ul> <li>Started to support Kubernetes along with OpenShift as platform type.</li> <li>Support of MTO's PostgreSQL instance as persistent storage for keycloak.</li> <li><code>kube:admin</code> is now bypassed by default to perform operations, earlier <code>kube:admin</code> needed to be mentioned in respective tenants to give it access over namespaces.</li> </ul>"},{"location":"changelog.html#v09x","title":"v0.9.x","text":""},{"location":"changelog.html#v094","title":"v0.9.4","text":"<ul> <li>enhance: Removed Quota's default support of adding it to Tenant CR in <code>spec.quota</code>, if <code>quota.tenantoperator.stakater.com/is-default: \"true\"</code> annotation is present</li> <li>fix: ValidatingWebhookConfiguration CRs are now owned by OLM, to handle cleanup upon operator uninstall</li> <li>enhance: TemplateGroupInstance CRs now actively watch the resources they apply, and perform functions to make sure they are in sync with the state mentioned in their respective Templates</li> </ul> <p>More information about TemplateGroupInstance's sync at Sync Resources Deployed by TemplateGroupInstance</p>"},{"location":"changelog.html#v092","title":"v0.9.2","text":"<ul> <li>fix: Values within TemplateInstances created via Tenants will no longer be duplicated on Tenant CR update</li> <li>fix: Fixed a bug that made private namespaces become public</li> </ul>"},{"location":"changelog.html#v091","title":"v0.9.1","text":"<ul> <li>fix: Allow namespace controller to reconcile without crashing, if no IC exists</li> <li>fix: In case a group mentioned in IC doesn't exist, it won't block reconciliation or editing of MTO's manifests</li> </ul>"},{"location":"changelog.html#v090","title":"v0.9.0","text":"<ul> <li>feat: Added console for tenants, templates and integration config</li> <li>feat: Added support for custom realm name for RHSSO integration in Integration Config</li> <li>feat: Add multiple status conditions to tenant and TGI for success and failure cases</li> <li>feat: Show error messages with tenant and TGI status</li> <li>fix: Stop reconciliation breaking for tenant and TGI, instead continue and show warnings</li> <li>fix: Disable TGI/TI reconcile if mentioned template is not found.</li> <li>fix: Disable repeated users webhook in tenant</li> <li>enhance: Reduced API calls</li> <li>enhance: General enhancements and improvements</li> <li>chore: Update dependencies</li> </ul>"},{"location":"changelog.html#enabling-console","title":"Enabling console","text":"<ul> <li>To enable console visit Installation, and add config to subscription for OperatorHub based installation.</li> </ul>"},{"location":"changelog.html#v08x","title":"v0.8.x","text":""},{"location":"changelog.html#v083","title":"v0.8.3","text":"<ul> <li>fix: Reconcile namespaces when the group spec for tenants is changed, so new <code>rolebindings</code> can be created for them</li> </ul>"},{"location":"changelog.html#v081","title":"v0.8.1","text":"<ul> <li>fix: Updated release pipelines</li> </ul>"},{"location":"changelog.html#v080","title":"v0.8.0","text":"<ul> <li>feat: Allow custom roles for each tenant via label selector, more details in custom roles document<ul> <li>Roles mapping is a required field in MTO's IntegrationConfig. By default, it will always be filled with OpenShift's admin/edit/view roles</li> <li>Ensure that mentioned roles exist within the cluster</li> <li>Remove coupling with OpenShift's built-in admin/edit/view roles</li> </ul> </li> <li>feat: Removed coupling of ResourceSupervisor and Tenant resources<ul> <li>Added list of namespaces to hibernate within the ResourceSupervisor resource</li> <li>Ensured that the same namespace cannot be added to two different Resource Supervisors</li> <li>Moved ResourceSupervisor into a separate pod</li> <li>Improved logs</li> </ul> </li> <li>fix: Remove bug from tenant's common and specific metadata</li> <li>fix: Add missing field to Tenant's conversion webhook</li> <li>fix: Fix panic in ResourceSupervisor sleep functionality due to sending on closed channel</li> <li>chore: Update dependencies</li> </ul>"},{"location":"changelog.html#v07x","title":"v0.7.x","text":""},{"location":"changelog.html#v074","title":"v0.7.4","text":"<ul> <li>maintain: Automate certification of new MTO releases on RedHat's Operator Hub</li> </ul>"},{"location":"changelog.html#v073","title":"v0.7.3","text":"<ul> <li>feat: Updated Tenant CR to provide Tenant level AppProject permissions</li> </ul>"},{"location":"changelog.html#v072","title":"v0.7.2","text":"<ul> <li>feat: Add support to map secrets/configmaps from one namespace to other namespaces using TI. Secrets/configmaps will only be mapped if their namespaces belong to same Tenant</li> </ul>"},{"location":"changelog.html#v071","title":"v0.7.1","text":"<ul> <li>feat: Add option to keep AppProjects created by Multi Tenant Operator in case Tenant is deleted. By default, AppProjects get deleted</li> <li>fix: Status now updates after namespaces are created</li> <li>maintain: Changes to Helm chart's default behaviour</li> </ul>"},{"location":"changelog.html#v070","title":"v0.7.0","text":"<ul> <li>feat: Add support to map secrets/configmaps from one namespace to other namespaces using TGI. Resources can be mapped from one Tenant's namespaces to some other Tenant's namespaces</li> <li>feat: Allow creation of sandboxes that are private to the user</li> <li>feat: Allow creation of namespaces without tenant prefix from within tenant spec</li> <li>fix: Webhook changes will now be updated without manual intervention</li> <li>maintain: Updated Tenant CR version from v1beta1 to v1beta2. Conversion webhook is added to facilitate transition to new version<ul> <li>see Tenant spec for updated spec</li> </ul> </li> <li>enhance: Better automated testing</li> </ul>"},{"location":"changelog.html#v06x","title":"v0.6.x","text":""},{"location":"changelog.html#v061","title":"v0.6.1","text":"<ul> <li>fix: Update MTO service-account name in environment variable</li> </ul>"},{"location":"changelog.html#v060","title":"v0.6.0","text":"<ul> <li>feat: Add support to ArgoCD AppProjects created by Tenant Controller to have their sync disabled when relevant namespaces are hibernating</li> <li>feat: Add validation webhook for ResourceSupervisor</li> <li>fix: Delete ResourceSupervisor when hibernation is removed from tenant CR</li> <li>fix: CRQ and limit range not updating when quota changes</li> <li>fix: ArgoCD AppProjects created by Tenant Controller not updating when Tenant label is added to an existing namespace</li> <li>fix: Namespace workflow for TGI</li> <li>fix: ResourceSupervisor deletion workflow</li> <li>fix: Update RHSSO user filter for Vault integration</li> <li>fix: Update regex of namespace names in tenant CRD</li> <li>enhance: Optimize TGI and TI performance under load</li> <li>maintain: Bump Operator-SDK and Dependencies version</li> </ul>"},{"location":"changelog.html#v05x","title":"v0.5.x","text":""},{"location":"changelog.html#v054","title":"v0.5.4","text":"<ul> <li>fix: Update Helm dependency to v3.8.2</li> </ul>"},{"location":"changelog.html#v053","title":"v0.5.3","text":"<ul> <li>fix: Add support for parameters in Helm <code>chartRepository</code> in templates</li> </ul>"},{"location":"changelog.html#v052","title":"v0.5.2","text":"<ul> <li>fix: Add service name prefix for webhooks</li> </ul>"},{"location":"changelog.html#v051","title":"v0.5.1","text":"<ul> <li>fix: ResourceSupervisor CR no longer requires a field for the Tenant name</li> </ul>"},{"location":"changelog.html#v050","title":"v0.5.0","text":"<ul> <li>feat: Add support for tenant namespaces off-boarding.</li> <li> <p>feat: Add tenant webhook for spec validation</p> </li> <li> <p>fix: TemplateGroupInstance now cleans up leftover Template resources from namespaces that are no longer part of TGI namespace selector</p> </li> <li> <p>fix: Fixed hibernation sync issue</p> </li> <li> <p>enhance: Update tenant spec for applying common/specific namespace labels/annotations. For more details check out commonMetadata &amp; SpecificMetadata</p> </li> <li> <p>enhance: Add support for multi-pod architecture for Operator-Hub</p> </li> <li> <p>chore: Remove conversion webhook for Quota and Tenant</p> </li> </ul>"},{"location":"changelog.html#v04x","title":"v0.4.x","text":""},{"location":"changelog.html#v047","title":"v0.4.7","text":"<ul> <li>feat: Add hibernation of StatefulSets and Deployments based on a timer</li> <li>feat: New custom resource that handles hibernation</li> </ul>"},{"location":"changelog.html#v046","title":"v0.4.6","text":"<ul> <li>fix: Revert v0.4.4</li> </ul>"},{"location":"changelog.html#v045","title":"v0.4.5","text":"<ul> <li>feat: Add support for applying labels/annotation on specific namespaces</li> </ul>"},{"location":"changelog.html#v044","title":"v0.4.4","text":"<ul> <li>fix: Update <code>privilegedNamespaces</code> regex</li> </ul>"},{"location":"changelog.html#v043","title":"v0.4.3","text":"<ul> <li>fix: IntegrationConfig will now be synced in all pods</li> </ul>"},{"location":"changelog.html#v042","title":"v0.4.2","text":"<ul> <li>feat: Added support to distribute common labels and annotations to tenant namespaces</li> </ul>"},{"location":"changelog.html#v041","title":"v0.4.1","text":"<ul> <li>fix: Update dependencies to latest version</li> </ul>"},{"location":"changelog.html#v040","title":"v0.4.0","text":"<ul> <li>feat: Controllers are now separated into individual pods</li> </ul>"},{"location":"changelog.html#v03x","title":"v0.3.x","text":""},{"location":"changelog.html#v0333","title":"v0.3.33","text":"<ul> <li>fix: Optimize namespace reconciliation</li> </ul>"},{"location":"changelog.html#v0333_1","title":"v0.3.33","text":"<ul> <li>fix: Revert v0.3.29 change till webhook network issue isn't resolved</li> </ul>"},{"location":"changelog.html#v0333_2","title":"v0.3.33","text":"<ul> <li>fix: Execute webhook and controller of matching custom resource in same pod</li> </ul>"},{"location":"changelog.html#v0330","title":"v0.3.30","text":"<ul> <li>feat: Namespace controller will now trigger TemplateGroupInstance when a new matching namespace is created</li> </ul>"},{"location":"changelog.html#v0329","title":"v0.3.29","text":"<ul> <li>feat: Controllers are now separated into individual pods</li> </ul>"},{"location":"changelog.html#v0328","title":"v0.3.28","text":"<ul> <li>fix: Enhancement of TemplateGroupInstance Namespace event listener</li> </ul>"},{"location":"changelog.html#v0327","title":"v0.3.27","text":"<ul> <li>feat: TemplateGroupInstance will create resources instantly whenever a Namespace with matching labels is created</li> </ul>"},{"location":"changelog.html#v0326","title":"v0.3.26","text":"<ul> <li>fix: Update reconciliation frequency of TemplateGroupInstance</li> </ul>"},{"location":"changelog.html#v0325","title":"v0.3.25","text":"<ul> <li>feat: TemplateGroupInstance will now directly create template resources instead of creating TemplateInstances</li> </ul>"},{"location":"changelog.html#migrating-from-pervious-version","title":"Migrating from pervious version","text":"<ul> <li>To migrate to Tenant-Operator:v0.3.25 perform the following steps<ul> <li>Downscale Tenant-Operator deployment by setting the replicas count to 0</li> <li>Delete TemplateInstances created by TemplateGroupInstance (Naming convention of TemplateInstance created by TemplateGroupInstance is <code>group-{Template.Name}</code>)</li> <li>Update version of Tenant-Operator to v0.3.25 and set the replicas count to 2. After Tenant-Operator pods are up TemplateGroupInstance will create the missing resources</li> </ul> </li> </ul>"},{"location":"changelog.html#v0324","title":"v0.3.24","text":"<ul> <li>feat: Add feature to allow ArgoCD to sync specific cluster scoped custom resources, configurable via Integration Config. More details in relevant docs</li> </ul>"},{"location":"changelog.html#v0323","title":"v0.3.23","text":"<ul> <li>feat: Added concurrent reconcilers for template instance controller</li> </ul>"},{"location":"changelog.html#v0322","title":"v0.3.22","text":"<ul> <li>feat: Added validation webhook to prevent Tenant owners from creating RoleBindings with kind 'Group' or 'User'</li> <li>fix: Removed redundant logs for namespace webhook</li> <li>fix: Added missing check for users in a tenant owner's groups in namespace validation webhook</li> <li>fix: General enhancements and improvements</li> </ul> <p>\u26a0\ufe0f Known Issues</p> <ul> <li><code>caBundle</code> field in validation webhooks is not being populated for newly added webhooks. A temporary fix is to edit the validation webhook configuration manifest without the <code>caBundle</code> field added in any webhook, so OpenShift can add it to all fields simultaneously<ul> <li>Edit the <code>ValidatingWebhookConfiguration</code> <code>multi-tenant-operator-validating-webhook-configuration</code> by removing all the <code>caBundle</code> fields of all webhooks</li> <li>Save the manifest</li> <li>Verify that all <code>caBundle</code> fields have been populated</li> <li>Restart Tenant-Operator pods</li> </ul> </li> </ul>"},{"location":"changelog.html#v0321","title":"v0.3.21","text":"<ul> <li>feat: Added ClusterRole manager rules extension</li> </ul>"},{"location":"changelog.html#v0320","title":"v0.3.20","text":"<ul> <li>fix: Fixed the recreation of underlying template resources, if resources were deleted</li> </ul>"},{"location":"changelog.html#v0319","title":"v0.3.19","text":"<ul> <li>feat: Namespace webhook FailurePolicy is now set to Ignore instead of Fail</li> <li>fix: Fixed config not being updated in namespace webhook when Integration Config is updated</li> <li>fix: Fixed a crash that occurred in case of ArgoCD in Integration Config was not set during deletion of Tenant resource</li> </ul> <p>\u26a0\ufe0f ApiVersion <code>v1alpha1</code> of Tenant and Quota custom resources has been deprecated and is scheduled to be removed in the future. The following links contain the updated structure of both resources</p> <ul> <li>Quota v1beta1</li> <li>Tenant v1beta1</li> </ul>"},{"location":"changelog.html#v0318","title":"v0.3.18","text":"<ul> <li>fix: Add ArgoCD namespace to destination namespaces for App Projects</li> </ul>"},{"location":"changelog.html#v0317","title":"v0.3.17","text":"<ul> <li>fix: Cluster administrator's permission will now have higher precedence on privileged namespaces</li> </ul>"},{"location":"changelog.html#v0316","title":"v0.3.16","text":"<ul> <li>fix: Add groups mentioned in Tenant CR to ArgoCD App Project manifests' RBAC</li> </ul>"},{"location":"changelog.html#v0315","title":"v0.3.15","text":"<ul> <li>feat: Add validation webhook for TemplateInstance &amp; TemplateGroupInstance to prevent their creation in case the Template they reference does not exist</li> </ul>"},{"location":"changelog.html#v0314","title":"v0.3.14","text":"<ul> <li>feat: Added Validation Webhook for Quota to prevent its deletion when a reference to it exists in any Tenant</li> <li>feat: Added Validation Webhook for Template to prevent its deletion when a reference to it exists in any Tenant, TemplateGroupInstance or TemplateInstance</li> <li>fix: Fixed a crash that occurred in case Integration Config was not found</li> </ul>"},{"location":"changelog.html#v0313","title":"v0.3.13","text":"<ul> <li>feat: Multi Tenant Operator will now consider all namespaces to be managed if any default Integration Config is not found</li> </ul>"},{"location":"changelog.html#v0312","title":"v0.3.12","text":"<ul> <li>fix: General enhancements and improvements</li> </ul>"},{"location":"changelog.html#v0311","title":"v0.3.11","text":"<ul> <li>fix: Fix Quota's conversion webhook converting the wrong LimitRange field</li> </ul>"},{"location":"changelog.html#v0310","title":"v0.3.10","text":"<ul> <li>fix: Fix Quota's LimitRange to its intended design by being an optional field</li> </ul>"},{"location":"changelog.html#v039","title":"v0.3.9","text":"<ul> <li>feat: Add ability to prevent certain resources from syncing via ArgoCD</li> </ul>"},{"location":"changelog.html#v038","title":"v0.3.8","text":"<ul> <li>feat: Add default annotation to OpenShift Projects that show description about the Project</li> </ul>"},{"location":"changelog.html#v037","title":"v0.3.7","text":"<ul> <li>fix: Fix a typo in Multi Tenant Operator's Helm release</li> </ul>"},{"location":"changelog.html#v036","title":"v0.3.6","text":"<ul> <li>fix: Fix ArgoCD's <code>destinationNamespaces</code> created by Multi Tenant Operator</li> </ul>"},{"location":"changelog.html#v035","title":"v0.3.5","text":"<ul> <li>fix: Change sandbox creation from 1 for each group to 1 for each user in a group</li> </ul>"},{"location":"changelog.html#v034","title":"v0.3.4","text":"<ul> <li>feat: Support creation of sandboxes for each group</li> </ul>"},{"location":"changelog.html#v033","title":"v0.3.3","text":"<ul> <li>feat: Add ability to create namespaces from a list of namespace prefixes listed in the Tenant CR</li> </ul>"},{"location":"changelog.html#v032","title":"v0.3.2","text":"<ul> <li>refactor: Restructure Quota CR, more details in relevant docs</li> <li>feat: Add support for adding LimitRanges in Quota</li> <li>feat: Add conversion webhook to convert existing v1alpha1 versions of quota to v1beta1</li> </ul>"},{"location":"changelog.html#v031","title":"v0.3.1","text":"<ul> <li>feat: Add ability to create ArgoCD AppProjects per tenant, more details in relevant docs</li> </ul>"},{"location":"changelog.html#v030","title":"v0.3.0","text":"<ul> <li>feat: Add support to add groups in addition to users as tenant members</li> </ul>"},{"location":"changelog.html#v02x","title":"v0.2.x","text":""},{"location":"changelog.html#v0233","title":"v0.2.33","text":"<ul> <li>refactor: Restructure Tenant spec, more details in relevant docs</li> <li>feat: Add conversion webhook to convert existing v1alpha1 versions of tenant to v1beta1</li> </ul>"},{"location":"changelog.html#v0232","title":"v0.2.32","text":"<ul> <li>refactor: Restructure integration config spec, more details in relevant docs</li> <li>feat: Allow users to input custom regex in certain fields inside of integration config, more details in relevant docs</li> </ul>"},{"location":"changelog.html#v0231","title":"v0.2.31","text":"<ul> <li>feat: Add limit range for <code>kube-RBAC-proxy</code></li> </ul>"},{"location":"eula.html","title":"EULA","text":"<p>Last revision date: 12 December 2022</p> <p>IMPORTANT: THIS SOFTWARE END-USER LICENSE AGREEMENT (\"EULA\") IS A LEGAL AGREEMENT (\"Agreement\") BETWEEN YOU (THE CUSTOMER, EITHER AS AN INDIVIDUAL OR, IF PURCHASED OR OTHERWISE ACQUIRED BY OR FOR AN ENTITY, AS AN ENTITY) AND Stakater AB OR ITS SUBSIDUARY (\"COMPANY\"). READ IT CAREFULLY BEFORE COMPLETING THE INSTALLATION PROCESS AND USING MULTI TENANT OPERATOR (\"SOFTWARE\"). IT PROVIDES A LICENSE TO USE THE SOFTWARE AND CONTAINS WARRANTY INFORMATION AND LIABILITY DISCLAIMERS. BY INSTALLING AND USING THE SOFTWARE, YOU ARE CONFIRMING YOUR ACCEPTANCE OF THE SOFTWARE AND AGREEING TO BECOME BOUND BY THE TERMS OF THIS AGREEMENT.</p> <p>In order to use the Software under this Agreement, you must receive a license key at the time of purchase, in accordance with the scope of use and other terms specified and as set forth in Section 1 of this Agreement.</p>"},{"location":"eula.html#1-license-grant","title":"1. License Grant","text":"<ul> <li> <p>1.1 General Use. This Agreement grants you a non-exclusive, non-transferable, limited license to the use rights for the Software, subject to the terms and conditions in this Agreement. The Software is licensed, not sold.</p> </li> <li> <p>1.2 Electronic Delivery. All Software and license documentation shall be delivered by electronic means unless otherwise specified on the applicable invoice or at the time of purchase. Software shall be deemed delivered when it is made available for download for you by the Company (\"Delivery\").</p> </li> </ul>"},{"location":"eula.html#2-modifications","title":"2. Modifications","text":"<ul> <li> <p>2.1 No Modifications may be created of the original Software. \"Modification\" means:</p> <ul> <li> <p>(a) Any addition to or deletion from the contents of a file included in the original Software</p> </li> <li> <p>(b) Any new file that contains any part of the original Software</p> </li> </ul> </li> </ul>"},{"location":"eula.html#3-restricted-uses","title":"3. Restricted Uses","text":"<ul> <li> <p>3.1 You shall not (and shall not allow any third party to):</p> <ul> <li> <p>(a) reverse engineer the Software or attempt to reconstruct or discover any source code, underlying ideas, algorithms, file formats or programming interfaces of the Software by any means whatsoever (except and only to the extent that applicable law prohibits or restricts reverse engineering restrictions);</p> </li> <li> <p>(b) distribute, sell, sub-license, rent, lease or use the Software for time sharing, hosting, service provider or like purposes, except as expressly permitted under this Agreement;</p> </li> <li> <p>(c) redistribute the Software;</p> </li> <li> <p>(d) remove any product identification, proprietary, copyright or other notices contained in the Software;</p> </li> <li> <p>(e) modify any part of the Software, create a derivative work of any part of the Software (except as permitted in Section 4), or incorporate the Software, except to the extent expressly authorized in writing by the Company;</p> </li> <li> <p>(f) publicly disseminate performance information or analysis (including, without limitation, benchmarks) from any source relating to the Software;</p> </li> <li> <p>(g) utilize any equipment, device, software, or other means designed to circumvent or remove any form of Source URL or copy protection used by the Company in connection with the Software, or use the Software together with any authorization code, Source URL, serial number, or other copy protection device not supplied by the Company;</p> </li> <li> <p>(h) use the Software to develop a product which is competitive with any of the Company's product offerings;</p> </li> <li> <p>(i) use unauthorized Source URLs or license key(s) or distribute or publish Source URLs or license key(s), except as may be expressly permitted by the Company in writing. If your unique license is ever published, the Company reserves the right to terminate your access without notice.</p> </li> </ul> </li> <li> <p>3.2 Under no circumstances may you use the Software as part of a product or service that provides similar functionality to the Software itself.</p> </li> </ul>"},{"location":"eula.html#4-ownership","title":"4. Ownership","text":"<ul> <li>4.1 Notwithstanding anything to the contrary contained herein, except for the limited license rights expressly provided herein, the Company and its suppliers have and will retain all rights, title and interest (including, without limitation, all patent, copyright, trademark, trade secret and other intellectual property rights) in and to the Software and all copies, modifications and derivative works thereof (including any changes which incorporate any of your ideas, feedback or suggestions). You acknowledge that you are obtaining only a limited license right to the Software, and that irrespective of any use of the words \"purchase\", \"sale\" or like terms hereunder no ownership rights are being conveyed to you under this Agreement or otherwise.</li> </ul>"},{"location":"eula.html#5-fees-and-payment","title":"5. Fees and Payment","text":"<ul> <li>5.1 The Software license fees will be due and payable in full as set forth in the applicable invoice or at the time of purchase. You shall be responsible for all taxes, with-holdings, duties and levies arising from the order (excluding taxes based on the net income of the Company).</li> </ul>"},{"location":"eula.html#6-support-maintenance-and-services","title":"6. Support, Maintenance and Services","text":"<ul> <li>6.1 Subject to the terms and conditions of this Agreement, as set forth in your invoice, and as set forth on the Stakater support page, support and maintenance services may be included with the purchase of your license subscription.</li> </ul>"},{"location":"eula.html#7-disclaimer-of-warranties","title":"7. Disclaimer of Warranties","text":"<ul> <li> <p>7.1 The Software is provided \"as is\", with all faults, defects and errors, and without warranty of any kind. The Company does not warrant that the Software will be free of bugs, errors, or other defects, and the Company shall have no liability of any kind for the use of or inability to use the Software, the Software content or any associated service, and you acknowledge that it is not technically practicable for the Company to do so.</p> </li> <li> <p>7.2 To the maximum extent permitted by applicable law, the Company disclaims all warranties, express, implied, arising by law or otherwise, regarding the Software, the Software content and their respective performance or suitability for your intended use, including without limitation any implied warranty of merchantability, fitness for a particular purpose.</p> </li> </ul>"},{"location":"eula.html#8-limitation-of-liability","title":"8. Limitation of Liability","text":"<ul> <li> <p>8.1 In no event will the Company be liable for any direct, indirect, consequential, incidental, special, exemplary, or punitive damages or liabilities whatsoever arising from or relating to the Software, the Software content or this Agreement, whether based on contract, tort (including negligence), strict liability or other theory, even if the Company has been advised of the possibility of such damages.</p> </li> <li> <p>8.2 In no event will the Company's liability exceed the Software license price as indicated in the invoice. The existence of more than one claim will not enlarge or extend this limit.</p> </li> </ul>"},{"location":"eula.html#9-remedies","title":"9. Remedies","text":"<ul> <li> <p>9.1 Your exclusive remedy and the Company's entire liability for breach of this Agreement shall be limited, at the Company's sole and exclusive discretion, to:</p> <ul> <li> <p>(a) replacement of any defective software or documentation; or</p> </li> <li> <p>(b) refund of the license fee paid to the Company</p> </li> </ul> </li> </ul>"},{"location":"eula.html#10-acknowledgements","title":"10. Acknowledgements","text":"<ul> <li> <p>10.1 Consent to the Use of Data. You agree that the Company and its affiliates may collect and use technical information gathered as part of the product support services. The Company may use this information solely to improve products and services and will not disclose this information in a form that personally identifies individuals or organizations.</p> </li> <li> <p>10.2 Government End Users. If the Software and related documentation are supplied to or purchased by or on behalf of a Government, then the Software is deemed to be \"commercial software\" as that term is used in the acquisition regulation system.</p> </li> </ul>"},{"location":"eula.html#11-third-party-software","title":"11. Third Party Software","text":"<ul> <li> <p>11.1 Examples included in Software may provide links to third party libraries or code (collectively \"Third Party Software\") to implement various functions. Third Party Software does not comprise part of the Software. In some cases, access to Third Party Software may be included along with the Software delivery as a convenience for demonstration purposes. Licensee acknowledges:</p> <ul> <li> <p>(1) That some part of Third Party Software may require additional licensing of copyright and patents from the owners of such, and</p> </li> <li> <p>(2) That distribution of any of the Software referencing or including any portion of a Third Party Software may require appropriate licensing from such third parties</p> </li> </ul> </li> </ul>"},{"location":"eula.html#12-miscellaneous","title":"12. Miscellaneous","text":"<ul> <li> <p>12.1 Entire Agreement. This Agreement sets forth our entire agreement with respect to the Software and the subject matter hereof and supersedes all prior and contemporaneous understandings and agreements whether written or oral.</p> </li> <li> <p>12.2 Amendment. The Company reserves the right, in its sole discretion, to amend this Agreement from time. Amendments are managed as described in General Provisions.</p> </li> <li> <p>12.3 Assignment. You may not assign this Agreement or any of its rights under this Agreement without the prior written consent of The Company and any attempted assignment without such consent shall be void.</p> </li> <li> <p>12.4 Export Compliance. You agree to comply with all applicable laws and regulations, including laws, regulations, orders or other restrictions on export, re-export or redistribution of software.</p> </li> <li> <p>12.5 Indemnification. You agree to defend, indemnify, and hold harmless the Company from and against any lawsuits, claims, losses, damages, fines and expenses (including attorneys' fees and costs) arising out of your use of the Software or breach of this Agreement.</p> </li> <li> <p>12.6 Attorneys' Fees and Costs. The prevailing party in any action to enforce this Agreement will be entitled to recover its attorneys' fees and costs in connection with such action.</p> </li> <li> <p>12.7 Severability. If any provision of this Agreement is held by a court of competent jurisdiction to be invalid, illegal, or unenforceable, the remainder of this Agreement will remain in full force and effect.</p> </li> <li> <p>12.8 Waiver. Failure or neglect by either party to enforce at any time any of the provisions of this license Agreement shall not be construed or deemed to be a waiver of that party's rights under this Agreement.</p> </li> <li> <p>12.9 Audit. The Company may, at its expense, appoint its own personnel or an independent third party to audit the numbers of installations of the Software in use by you. Any such audit shall be conducted upon thirty (30) days prior notice, during regular business hours and shall not unreasonably interfere with your business activities.</p> </li> <li> <p>12.10 Headings. The headings of sections and paragraphs of this Agreement are for convenience of reference only and are not intended to restrict, affect or be of any weight in the interpretation or construction of the provisions of such sections or paragraphs.</p> </li> </ul>"},{"location":"eula.html#13-contact-information","title":"13. Contact Information","text":"<ul> <li>13.1 If you have any questions about this EULA, or if you want to contact the Company for any reason, please direct correspondence to <code>sales@stakater.com</code>.</li> </ul>"},{"location":"pricing.html","title":"Pricing","text":"<p>Multi Tenant Operator (MTO) is available in two versions: Basic and Enterprise.</p> <p>The Basic version is free.</p> <p>The Enterprise version is priced according to information on the Multi Tenant Operator website.</p>"},{"location":"pricing.html#feature-difference","title":"Feature Difference","text":"<p>The difference between the versions are highlighted here:</p> Feature Basic Enterprise Number of possible Tenants Two (2) Unlimited Support Community support through community Slack Stakater dedicated support with Key Account Manager to build strong relationships and strong understanding of your requirements and needs Custom development requests Not possible Possible through support requests, which will be prioritized Price Free Pricing available on the Multi Tenant Operator website"},{"location":"pricing.html#demo","title":"Demo","text":"<p>A web application demo of MTO is available on the Multi Tenant Operator website, it showcases MTO Console which is aimed at providing a more intuitive and user-friendly way for administrators and tenant users to manage tenants and their resources.</p> <p>Contact <code>sales@stakater.com</code> to request a custom demo.</p>"},{"location":"pricing.html#support","title":"Support","text":"<p>See Stakater Support for information about support for the Enterprise Version of MTO.</p>"},{"location":"pricing.html#contact","title":"Contact","text":"<p>For more info, contact <code>sales@stakater.com</code>.</p>"},{"location":"troubleshooting.html","title":"Troubleshooting Guide","text":""},{"location":"troubleshooting.html#operatorhub-upgrade-error","title":"OperatorHub Upgrade Error","text":""},{"location":"troubleshooting.html#operator-is-stuck-in-upgrade-if-upgrade-approval-is-set-to-automatic","title":"Operator is stuck in upgrade if upgrade approval is set to Automatic","text":""},{"location":"troubleshooting.html#problem","title":"Problem","text":"<p>If operator upgrade is set to Automatic Approval on OperatorHub, there may be scenarios where it gets blocked.</p>"},{"location":"troubleshooting.html#resolution","title":"Resolution","text":"<p>Information</p> <pre><code>If upgrade approval is set to manual, and you want to skip upgrade of a specific version, then delete the InstallPlan created for that specific version. Operator Lifecycle Manager (OLM) will create the latest available InstallPlan which can be approved then.\n</code></pre> <p>As OLM does not allow to upgrade or downgrade from a version stuck because of error, the only possible fix is to uninstall the operator from the cluster. When the operator is uninstalled it removes all of its resources i.e., ClusterRoles, ClusterRoleBindings, and Deployments etc., except Custom Resource Definitions (CRDs), so none of the Custom Resources (CRs), Tenants, Templates etc., will be removed from the cluster. If any CRD has a conversion webhook defined then that webhook should be removed before installing the stable version of the operator. This can be achieved via removing the <code>.spec.conversion</code> block from the CRD schema.</p> <p>As an example, if you have installed v0.8.0 of Multi Tenant Operator on your cluster, then it'll stuck in an error <code>error validating existing CRs against new CRD's schema for \"integrationconfigs.tenantoperator.stakater.com\": error validating custom resource against new schema for IntegrationConfig multi-tenant-operator/tenant-operator-config: [].spec.tenantRoles: Required value</code>. To resolve this issue, you'll first uninstall the MTO from the cluster. Once you uninstall the MTO, check Tenant CRD which will have a conversion block, which needs to be removed. After removing the conversion block from the Tenant CRD, install the latest available version of MTO from OperatorHub.</p>"},{"location":"troubleshooting.html#permission-issues","title":"Permission Issues","text":""},{"location":"troubleshooting.html#vault-user-permissions-are-not-updated-if-the-user-is-added-to-a-tenant-and-the-user-does-not-exist-in-rhsso","title":"Vault user permissions are not updated if the user is added to a Tenant, and the user does not exist in RHSSO","text":""},{"location":"troubleshooting.html#problem_1","title":"Problem","text":"<p>If a user is added to tenant resource, and the user does not exist in RHSSO, then RHSSO is not updated with the user's Vault permission.</p>"},{"location":"troubleshooting.html#reproduction-steps","title":"Reproduction steps","text":"<ol> <li>Add a new user to Tenant CR</li> <li>Attempt to log in to Vault with the added user</li> <li>Vault denies that the user exists, and signs the user up via RHSSO. User is now created on RHSSO (you may check for the user on RHSSO).</li> </ol>"},{"location":"troubleshooting.html#resolution_1","title":"Resolution","text":"<p>If the user does not exist in RHSSO, then MTO does not create the tenant access for Vault in RHSSO.</p> <p>The user now needs to go to Vault, and sign up using OIDC. Then the user needs to wait for MTO to reconcile the updated tenant (reconciliation period is currently 1 hour). After reconciliation, MTO will add relevant access for the user in RHSSO.</p> <p>If the user needs to be added immediately and it is not feasible to wait for next MTO reconciliation, then: add a label or annotation to the user, or restart the Tenant controller pod to force immediate reconciliation.</p>"},{"location":"troubleshooting.html#pod-creation-error","title":"Pod Creation Error","text":""},{"location":"troubleshooting.html#q-errors-in-replicaset-events-about-pods-not-being-able-to-schedule-on-openshift-because-scc-annotation-is-not-found","title":"Q. Errors in ReplicaSet Events about pods not being able to schedule on OpenShift because SCC annotation is not found","text":"<pre><code>unable to find annotation openshift.io/sa.scc.uid-range\n</code></pre> <p>Answer. OpenShift recently updated its process of handling SCC, and it's now managed by annotations like <code>openshift.io/sa.scc.uid-range</code> on the namespaces. Absence  of them wont let pods schedule. The fix for the above error is to make sure ServiceAccount <code>system:serviceaccount:openshift-infra.</code> regex is always mentioned in <code>Privileged.serviceAccounts</code> section of <code>IntegrationConfig</code>. This regex will allow operations from all <code>ServiceAccounts</code> present in <code>openshift-infra</code> namespace. More info at Privileged Service Accounts</p>"},{"location":"troubleshooting.html#namespace-admission-webhook","title":"Namespace Admission Webhook","text":""},{"location":"troubleshooting.html#q-error-received-while-performing-create-update-or-delete-action-on-namespace","title":"Q. Error received while performing Create, Update or Delete action on Namespace","text":"<pre><code>Cannot CREATE namespace test-john without label stakater.com/tenant\n</code></pre> <p>Answer. Error occurs when a user is trying to perform create, update, delete action on a namespace without the required <code>stakater.com/tenant</code> label. This label is used by the operator to see that authorized users can perform that action on the namespace. Just add the label with the tenant name so that MTO knows which tenant the namespace belongs to, and who is authorized to perform create/update/delete operations. For more details please refer to Namespace use-case.</p>"},{"location":"troubleshooting.html#q-error-received-while-performing-create-update-or-delete-action-on-openshift-project","title":"Q. Error received while performing Create, Update or Delete action on OpenShift Project","text":"<pre><code>Cannot CREATE namespace testing without label stakater.com/tenant. User: system:serviceaccount:openshift-apiserver:openshift-apiserver-sa\n</code></pre> <p>Answer. This error occurs because we don't allow Tenant members to do operations on OpenShift Project, whenever an operation is done on a project, <code>openshift-apiserver-sa</code> tries to do the same request onto a namespace. That's why the user sees <code>openshift-apiserver-sa</code> Service Account instead of its own user in the error message.</p> <p>The fix is to try the same operation on the namespace manifest instead.</p>"},{"location":"troubleshooting.html#q-error-received-while-doing-kubectl-apply-f-namespaceyaml","title":"Q. Error received while doing <code>kubectl apply -f namespace.yaml</code>","text":"<pre><code>Error from server (Forbidden): error when retrieving current configuration of:\nResource: \"/v1, Resource=namespaces\", GroupVersionKind: \"/v1, Kind=Namespace\"\nName: \"ns1\", Namespace: \"\"\nfrom server for: \"namespace.yaml\": namespaces \"ns1\" is forbidden: User \"muneeb\" cannot get resource \"namespaces\" in API group \"\" in the namespace \"ns1\"\n</code></pre> <p>Answer. Tenant members will not be able to use <code>kubectl apply</code> because <code>apply</code> first gets all the instances of that resource, in this case namespaces, and then does the required operation on the selected resource. To maintain tenancy, tenant members do not the access to get or list all the namespaces.</p> <p>The fix is to create namespaces with <code>kubectl create</code> instead.</p>"},{"location":"troubleshooting.html#mto-argocd-integration","title":"MTO - ArgoCD Integration","text":""},{"location":"troubleshooting.html#q-how-do-i-deploy-cluster-scoped-resource-via-the-argocd-integration","title":"Q. How do I deploy cluster-scoped resource via the ArgoCD integration?","text":"<p>Answer. Multi-Tenant Operator's ArgoCD Integration allows configuration of which cluster-scoped resources can be deployed, both globally and on a per-tenant basis. For a global allow-list that applies to all tenants, you can add both resource <code>group</code> and  <code>kind</code> to the IntegrationConfig's <code>spec.integrations.argocd.clusterResourceWhitelist</code> field. Alternatively, you can set this up on a tenant level by configuring the same details within a Tenant's <code>spec.integrations.argocd.appProject.clusterResourceWhitelist</code> field. For more details, check out the ArgoCD integration use cases</p>"},{"location":"troubleshooting.html#q-invalidspecerror-application-repo-repo-is-not-permitted-in-project-project","title":"Q. InvalidSpecError: application repo \\&lt;repo&gt; is not permitted in project \\&lt;project&gt;","text":"<p>Answer. The above error can occur if the ArgoCD Application is syncing from a source that is not allowed the referenced AppProject. To solve this, verify that you have referred to the correct project in the given ArgoCD Application, and that the repoURL used for the Application's source is valid. If the error still appears, you can add the URL to the relevant Tenant's <code>spec.integrations.argocd.sourceRepos</code> array.</p>"},{"location":"troubleshooting.html#mto-opencost","title":"MTO - OpenCost","text":""},{"location":"troubleshooting.html#q-why-are-there-mto-showback-pods-failing-in-my-cluster","title":"Q. Why are there <code>mto-showback-*</code> pods failing in my cluster?","text":"<p>Answer. The <code>mto-showback-*</code> pods are used to calculate the cost of the resources used by each tenant. These pods are created by the Multi-Tenant Operator and are scheduled to run every 10 minutes. If the pods are failing, it is likely that the operator's necessary to calculate cost are not present in the cluster. To solve this, you can navigate to <code>Operators</code> -&gt; <code>Installed Operators</code> in the OpenShift console and check if the MTO-OpenCost and MTO-Prometheus operators are installed. If they are in a pending state, you can manually approve them to install them in the cluster.</p>"},{"location":"about/benefits.html","title":"Benefits","text":"<p>Kubernetes multi-tenancy enables organizations to share a single cluster among multiple tenants, driving cost efficiency, operational simplicity, and scalability. By reducing infrastructure duplication and leveraging shared resources, multi-tenancy lowers costs, enhances agility, and streamlines management. It\u2019s a powerful approach that balances efficiency with operational ease, making it an ideal choice for organizations looking to scale without compromising on performance or security.</p>"},{"location":"about/benefits.html#1-cost-optimization-through-resource-sharing","title":"1. Cost Optimization Through Resource Sharing","text":"<p>In a single-tenancy model, each tenant (e.g., user, group, or department) requires a dedicated Kubernetes cluster. This approach incurs significant overhead as infrastructure components must be duplicated for each tenant, leading to higher per-tenant costs.</p> <p>With multi-tenancy, a single cluster is efficiently shared across multiple tenants. This eliminates the need for redundant infrastructure, significantly reducing the cost per tenant while maximizing resource utilization.</p>"},{"location":"about/benefits.html#2-accelerated-agility-for-scaling","title":"2. Accelerated Agility for Scaling","text":"<p>The adoption of Kubernetes often begins with small-scale deployments and grows rapidly as the technology proves its value. While a single-tenant architecture may suffice initially, it can become a bottleneck as organizations scale. Managing multiple clusters for each tenant becomes increasingly complex and expensive.</p> <p>Implementing a robust multi-tenancy strategy early ensures a smoother growth trajectory. It allows organizations to scale effortlessly without the operational and financial challenges associated with managing numerous clusters.</p>"},{"location":"about/benefits.html#3-streamlined-operations","title":"3. Streamlined Operations","text":"<p>Managing a Kubernetes environment with one cluster per tenant introduces considerable operational overhead. Teams must handle monitoring, compliance, security, and maintenance for multiple clusters, multiplying the workload.</p> <p>Conversely, a multi-tenant architecture simplifies operations by consolidating cluster management. Even with automation tools or managed services like EKS, AKS, or GKE, multi-tenancy reduces complexity, enabling operations teams to focus on innovation rather than repetitive tasks.</p>"},{"location":"about/benefits.html#4-enhanced-efficiency","title":"4. Enhanced Efficiency","text":"<p>Effective multi-tenancy maximizes efficiency by enabling better resource utilization and faster onboarding. Adding a new tenant is quicker and less resource-intensive than provisioning an entirely new cluster.</p> <p>Shared components, such as monitoring, logging, and deployment pipelines, further enhance efficiency. Applications can be packed more densely on the underlying hardware, ensuring optimal performance and cost savings.</p>"},{"location":"about/benefits.html#5-simplified-management","title":"5. Simplified Management","text":"<p>With the right tools, multi-tenancy reduces the operational burden associated with managing multiple clusters. Fewer clusters mean a smaller attack surface, less duplication of maintenance efforts, and streamlined patching workflows.</p> <p>While multi-tenancy introduces unique challenges, advancements in Kubernetes tooling are addressing these complexities, making multi-tenant environments simpler and more secure to manage.</p>"},{"location":"about/key-features.html","title":"Key Features","text":"<p>The key features of Multi Tenant Operator (MTO) are described below.</p>"},{"location":"about/key-features.html#core-features","title":"Core Features","text":""},{"location":"about/key-features.html#kubernetes-multitenancy","title":"Kubernetes Multitenancy","text":"<p>RBAC is one of the most complicated and error-prone parts of Kubernetes. With Multi Tenant Operator, you can rest assured that RBAC is configured with the \"least privilege\" mindset and all rules are kept up-to-date with zero manual effort.</p> <p>Multi Tenant Operator binds existing ClusterRoles to the Tenant's Namespaces used for managing access to the Namespaces and the resources they contain. You can also modify the default roles or create new roles to have full control and customize access control for your users and teams.</p> <p>Multi Tenant Operator is also able to leverage existing groups in Kubernetes and OpenShift, or external groups synced from 3rd party identity management systems, for maintaining Tenant membership in your organization's current user management system.</p> <p>More details on Tenant</p>"},{"location":"about/key-features.html#templates-and-template-distribution","title":"Templates and Template distribution","text":"<p>Multi Tenant Operator allows admins/users to define templates for namespaces, so that others can instantiate these templates to provision namespaces with batteries loaded. A template could pre-populate a namespace for certain use cases or with basic tooling required. Templates allow you to define Kubernetes manifests, Helm chart and more to be applied when the template is used to create a namespace.</p> <p>It also allows the parameterizing of these templates for flexibility and ease of use. It also provides the option to enforce the presence of templates in one tenant's or all the tenants' namespaces for configuring secure defaults.</p> <p>Common use cases for namespace templates may be:</p> <ul> <li>Adding networking policies for multitenancy</li> <li>Adding development tooling to a namespace</li> <li>Deploying pre-populated databases with test data</li> <li>Injecting new namespaces with optional credentials such as image pull secrets</li> </ul> <p>More details on Distributing Template Resources</p>"},{"location":"about/key-features.html#resource-management","title":"Resource Management","text":"<p>Multi Tenant Operator provides a mechanism for defining Resource Quotas at the tenant scope, meaning all namespaces belonging to a particular tenant share the defined quota, which is why you are able to safely enable dev teams to self serve their namespaces whilst being confident that they can only use the resources allocated based on budget and business needs.</p> <p>More details on Quota</p>"},{"location":"about/key-features.html#finops-features","title":"FinOps Features","text":""},{"location":"about/key-features.html#showback","title":"Showback","text":"<p>The showback functionality in Multi Tenant Operator (MTO) Console is a significant feature designed to enhance the management of resources and costs in multi-tenant Kubernetes environments. This feature focuses on accurately tracking the usage of resources by each tenant, and/or namespace, enabling organizations to monitor and optimize their expenditures. Furthermore, this functionality supports financial planning and budgeting by offering a clear view of operational costs associated with each tenant. This can be particularly beneficial for organizations that chargeback internal departments or external clients based on resource usage, ensuring that billing is fair and reflective of actual consumption.</p> <p>More details on Showback</p>"},{"location":"about/key-features.html#hibernation","title":"Hibernation","text":"<p>Multi Tenant Operator can downscale Deployments and StatefulSets in a tenant's Namespace according to a defined  sleep schedule. The Deployments and StatefulSets are brought back to their required replicas according to the provided wake schedule.</p> <p>More details on Hibernation and ResourceSupervisor</p>"},{"location":"about/key-features.html#capacity-planning","title":"Capacity Planning","text":"<p>Provides tools to forecast and allocate resources effectively, ensuring optimal usage and preventing over-provisioning.</p>"},{"location":"about/key-features.html#integration-features","title":"Integration Features","text":""},{"location":"about/key-features.html#hashicorp-vault-multitenancy","title":"Hashicorp Vault Multitenancy","text":"<p>Multi Tenant Operator extends the tenants permission model to Hashicorp Vault where it can create Vault paths and greatly ease the overhead of managing RBAC in Vault. Tenant users can manage their own secrets without the concern of someone else having access to their Vault paths.</p> <p>More details on Vault Multitenancy</p>"},{"location":"about/key-features.html#argocd-multitenancy","title":"ArgoCD Multitenancy","text":"<p>Multi Tenant Operator is not only providing strong Multi Tenancy for the Kubernetes internals but also extends the tenants permission model to ArgoCD were it can provision AppProjects and Allowed Repositories for your tenants greatly ease the overhead of managing RBAC in ArgoCD.</p> <p>More details on ArgoCD Multitenancy</p>"},{"location":"about/key-features.html#mattermost-multitenancy","title":"Mattermost Multitenancy","text":"<p>Multi Tenant Operator can manage Mattermost to create Teams for tenant users. All tenant users get a unique team and a list of predefined channels gets created. When a user is removed from the tenant, the user is also removed from the Mattermost team corresponding to tenant.</p> <p>More details on Mattermost</p>"},{"location":"about/key-features.html#developer-and-platform-productivity-features","title":"Developer and Platform Productivity Features","text":""},{"location":"about/key-features.html#mto-console","title":"MTO Console","text":"<p>Multi Tenant Operator Console is a comprehensive user interface designed for both administrators and tenant users to manage multi-tenant environments. The MTO Console simplifies the complexity involved in handling various aspects of tenants and their related resources. It serves as a centralized monitoring hub, offering insights into the current state of tenants, namespaces, templates and quotas. It is designed to provide a quick summary/snapshot of MTO's status and facilitates easier interaction with various resources such as tenants, namespaces, templates, and quotas.</p> <p>More details on Console</p>"},{"location":"about/key-features.html#remote-development-namespaces","title":"Remote Development Namespaces","text":"<p>Multi Tenant Operator can be configured to automatically provision a namespace in the cluster for every member of the specific tenant, that will also be preloaded with any selected templates and consume the same pool of resources from the tenants quota creating safe remote dev namespaces that teams can use as scratch namespace for rapid prototyping and development. So, every developer gets a Kubernetes-based cloud development environment that feel like working on localhost.</p> <p>More details on Sandboxes</p>"},{"location":"about/key-features.html#security-features","title":"Security Features","text":""},{"location":"about/key-features.html#cross-namespace-resource-distribution","title":"Cross Namespace Resource Distribution","text":"<p>Multi Tenant Operator supports cloning of secrets and configmaps from one namespace to another namespace based on label selectors. It uses templates to enable users to provide reference to secrets and configmaps. It uses a template group instance to distribute those secrets and namespaces in matching namespaces, even if namespaces belong to different tenants. If template instance is used then the resources will only be mapped if namespaces belong to same tenant.</p> <p>More details on Copying Secrets and Configmaps</p>"},{"location":"about/key-features.html#self-service","title":"Self-Service","text":"<p>With Multi Tenant Operator, you can empower your users to safely provision namespaces for themselves and their teams (typically mapped to SSO groups). Team-owned namespaces and the resources inside them count towards the team's quotas rather than the user's individual limits and are automatically shared with all team members according to the access rules you configure in Multi Tenant Operator.</p> <p>Also, by leveraging Multi Tenant Operator's templating mechanism, namespaces can be provisioned and automatically pre-populated with any kind of resource or multiple resources such as network policies, docker pull secrets or even Helm charts etc</p>"},{"location":"about/key-features.html#operational-features","title":"Operational Features","text":""},{"location":"about/key-features.html#everything-as-codegitops-ready","title":"Everything as Code/GitOps Ready","text":"<p>Multi Tenant Operator is designed and built to be 100% Kubernetes-native, and to be configured and managed the same familiar way as native Kubernetes resources so it's perfect for modern companies that are dedicated to GitOps as it is fully configurable using Custom Resources.</p>"},{"location":"about/key-features.html#preventing-clusters-sprawl","title":"Preventing Clusters Sprawl","text":"<p>As companies look to further harness the power of cloud-native, they are adopting container technologies at rapid speed, increasing the number of clusters and workloads. As the number of Kubernetes clusters grows, this is an increasing work for the Ops team. When it comes to patching security issues or upgrading clusters, teams are doing five times the amount of work.</p> <p>With Multi Tenant Operator teams can share a single cluster with multiple teams, groups of users, or departments by saving operational and management efforts. This prevents you from Kubernetes cluster sprawl.</p>"},{"location":"about/key-features.html#native-experience","title":"Native Experience","text":"<p>Multi Tenant Operator provides multi-tenancy with a native Kubernetes experience without introducing additional management layers, plugins, or customized binaries.</p>"},{"location":"about/use-cases.html","title":"Use Cases","text":"<p>The first step in deciding how to share your cluster is understanding your specific use case. This understanding helps you evaluate the available patterns and tools best suited to your needs. Broadly, multi-tenancy in Kubernetes clusters can be categorized into two main types, though variations and hybrids often exist.</p>"},{"location":"about/use-cases.html#1-multi-team-tenancy","title":"1. Multi-Team Tenancy","text":"<p>A common use case for multi-tenancy involves sharing a cluster among multiple teams within an organization. Each team may operate one or more workloads, which often need to communicate with:</p> <ul> <li>Other workloads within the same cluster.</li> <li>Workloads located in different clusters.</li> </ul> <p>In this scenario:</p> <ul> <li>Team members usually access Kubernetes resources either directly (e.g., via kubectl) or indirectly through tools like GitOps controllers or release automation systems.</li> <li>There is typically some degree of trust between teams, but safeguards are crucial. Policies such as Role-Based Access Control (RBAC), resource quotas, and network policies are necessary to ensure clusters are shared securely and fairly.</li> </ul>"},{"location":"about/use-cases.html#2-multi-customer-tenancy","title":"2. Multi-Customer Tenancy","text":"<p>Another common use case involves running multiple instances of a workload for customers, often by a Software-as-a-Service (SaaS) provider. This is sometimes referred to as \"SaaS tenancy,\" but a more accurate term might be multi-customer tenancy, as this model is not exclusive to SaaS.</p> <p>In this scenario:</p> <ul> <li>Customers do not have direct access to the cluster. Kubernetes operates behind the scenes, used solely by the vendor to manage workloads.</li> <li>Strong workload isolation is essential to maintain security and prevent resource contention. *Cost optimization is often a primary focus, achieved through Kubernetes policies that ensure efficient and secure resource usage.</li> </ul>"},{"location":"architecture/architecture.html","title":"Architecture","text":"<p>The Multi-Tenant Operator (MTO) is a comprehensive system designed to manage multi-tenancy in Kubernetes environments. Following is the architecture of the MTO:</p> <p></p> <p>MTO consists of multiple controllers and components that work together to provide the functionality of the system. The following is a list of the components that make up the MTO system:</p> Name Type Description Tenant Controller Deployment The Tenant Controller is responsible for managing the creation, deletion, and updating of tenants in the cluster via Tenant CRD. Namespace Controller Deployment The Namespace Controller is responsible for managing the creation, deletion, and updating of namespaces in the cluster. Resource Supervisor Deployment Deployment The Resource Supervisor Controller is responsible for managing sleep and hibernation of namespaces in the cluster via ResourceSupervisor CRD. Extensions Controller Deployment The Extensions Controller enhances MTO's functionality by allowing integration with external services,Currently supports integration with ArgoCD, enabling you to synchronize your repositories and configure AppProjects directly through MTO. It manages extensions via Extension CRD. Template Quota Integration Config Controller Deployment The Template Quota Integration Config Controller manages 3 different CRDs in one controller, Template CRD, Quota CRD, and IntegrationConfig CRD. TemplateInstance Controller Deployment The TemplateInstance Controller is responsible for managing the creation, deletion, and updating of TemplateInstances in the cluster via TemplateInstance CRD. TemplateGroupInstance Controller Deployment The TemplateGroupInstance Controller is responsible for managing the creation, deletion, and updating of TemplateGroupInstances in the cluster via TemplateGroupInstance CRD. Webhook Deployment The Webhook is responsible for managing webhook requests from MTO's resources. Pilot Controller Deployment The Pilot Controller is responsible provisioning and managing the lifecycle of MTO-Console and it's dependencies. Keycloak Deployment Keycloak is an open-source identity and access management solution that provides authentication and authorization services for the MTO Console that can be provisioned via Pilot Controller or can be deployed and managed externally following this guide External Keycloak. PostgreSQL StatefulSet PostgreSQL is an open-source relational database that acts as a caching layer and stores the data for the MTO Console. It is also provisioned via Pilot Controller and is managed internally. Opencost-Gateway Deployment Opencost is an open-source cost management solution that provides cost tracking and reporting for the resources deployed on the cluster. It is also provisioned via Pilot Controller and is managed internally. Prometheus-Server Deployment Prometheus is an open-source monitoring and alerting solution that provides metrics and monitoring for the resources deployed on the cluster. It is also provisioned via Pilot Controller and is managed internally. Kube-State-Metrics Deployment Kube-State-Metrics is a service that listens to the Kubernetes API server and generates metrics about the state of the objects in the cluster. It is also provisioned via Pilot Controller and is managed internally. Showback Cron-job The Showback Cron-job is responsible for generating showback reports based on the resources present on the cluster by querying the Opencost-Gateway and storing the reports in the PostgreSQL database that can be viewed in the MTO Console on the Showback page. MTO-Gateway Deployment The MTO-Gateway is the backend service that provides the REST API for the MTO-Console. MTO-Console Deployment The MTO-Console is the user interface for the MTO system that provides a web-based interface for managing tenants, namespaces, sleep, and more. Details about the MTO-Console can be found here."},{"location":"architecture/concepts.html","title":"Concepts","text":"<p>Here are the key concepts of Multi-Tenant Operator (MTO):</p>"},{"location":"architecture/concepts.html#tenant","title":"Tenant","text":"<p>A Tenant represents a logical grouping of namespaces, users, and resources within a Kubernetes cluster, enabling isolated environments for teams or projects. It defines access controls, resource quotas, and namespace configurations specific to each tenant.</p>"},{"location":"architecture/concepts.html#quota","title":"Quota","text":"<p>The Quota enforces resource limits for tenants, such as CPU, memory, and storage, ensuring fair allocation. It also defines minimum and maximum resource usage per pod or container within tenant namespaces.</p>"},{"location":"architecture/concepts.html#template","title":"Template","text":"<p>A Template is a reusable blueprint in Multi-Tenant Operator (MTO) that defines configurations for Kubernetes resources. It supports raw manifests, Helm charts, or resource mappings, enabling standardization and automation across multiple tenants.</p>"},{"location":"architecture/concepts.html#template-instance-ti","title":"Template Instance (TI)","text":"<p>A Template Instance is a concrete implementation of a Template, created with specific parameters tailored for a particular tenant or use case. It generates actual Kubernetes resources based on the defined template.</p>"},{"location":"architecture/concepts.html#template-group-instance-tgi","title":"Template Group Instance (TGI)","text":"<p>A Template Group Instance works on a particular set of namespaces based on the mentioned labels, taking Template as a reference for the resources to be deployed. It simplifies managing multiple interdependent resources for complex tenant setups.</p>"},{"location":"architecture/concepts.html#extensions","title":"Extensions","text":"<p>Extensions enhance MTO functionality by integrating external services like ArgoCD. They allow seamless configuration of AppProjects for tenants, extending multi-tenant workflows.</p>"},{"location":"architecture/concepts.html#resource-supervisor","title":"Resource Supervisor","text":"<p>The Resource Supervisor manages the hibernation of deployments and stateful sets, enabling scaling down during user defined schedule or by manual trigger, optimizing resource utilization and reducing costs.</p>"},{"location":"architecture/custom-metrics.html","title":"Custom Metrics Support","text":"<p>Multi Tenant Operator now supports custom metrics for templates, template instances and template group instances. This feature allows users to monitor the usage of templates and template instances in their cluster.</p> <p>To enable custom metrics and view them in your OpenShift cluster, you need to follow the steps below:</p> <ul> <li>Ensure that cluster monitoring is enabled in your cluster. You can check this by going to <code>Observe</code> -&gt; <code>Metrics</code> in the OpenShift console.</li> <li>Navigate to <code>Administration</code> -&gt; <code>Namespaces</code> in the OpenShift console. Select the namespace where you have installed Multi Tenant Operator.</li> <li>Add the following label to the namespace: <code>openshift.io/cluster-monitoring=true</code>. This will enable cluster monitoring for the namespace.</li> <li>To ensure that the metrics are being scraped for the namespace, navigate to <code>Observe</code> -&gt; <code>Targets</code> in the OpenShift console. You should see the namespace in the list of targets.</li> <li>To view the custom metrics, navigate to <code>Observe</code> -&gt; <code>Metrics</code> in the OpenShift console. You should see the custom metrics for templates, template instances and template group instances in the list of metrics.</li> </ul> <p>Details of metrics can be found at Metrics and Logs</p>"},{"location":"architecture/logs-metrics.html","title":"Metrics and Logs","text":"<p>This document offers an overview of the Prometheus metrics implemented by the <code>multi_tenant_operator</code> controllers, along with an interpretation guide for the logs and statuses generated by these controllers. Each metric is designed to provide specific insights into the controllers' operational performance, while the log interpretation guide aids in understanding their behavior and workflow processes. Additionally, the status descriptions for custom resources provide operational snapshots. Together, these elements form a comprehensive toolkit for monitoring and enhancing the performance and health of the controllers.</p>"},{"location":"architecture/logs-metrics.html#metrics-list","title":"Metrics List","text":"<p><code>multi_tenant_operator_resources_deployed_total</code></p> <ul> <li>Description: Tracks the total number of resources deployed by the operator.</li> <li>Type: Gauge</li> <li>Labels: <code>kind</code>, <code>name</code>, <code>namespace</code></li> <li>Usage: Helps to understand the overall workload managed by the operator.</li> </ul> <p><code>multi_tenant_operator_resources_deployed</code></p> <ul> <li>Description: Monitors resources currently deployed by the operator.</li> <li>Type: Gauge</li> <li>Labels: <code>kind</code>, <code>name</code>, <code>namespace</code>, <code>type</code></li> <li>Usage: Useful for tracking the current state and type of resources managed by the operator.</li> </ul> <p><code>multi_tenant_operator_reconcile_error</code></p> <ul> <li>Description: Indicates resources in an error state, broken down by resource kind, name, and namespace.</li> <li>Type: Gauge</li> <li>Labels: <code>kind</code>, <code>name</code>, <code>namespace</code>, <code>state</code>, <code>errors</code></li> <li>Usage: Essential for identifying and analyzing errors in resource management.</li> </ul> <p><code>multi_tenant_operator_reconcile_count</code></p> <ul> <li>Description: Counts the number of reconciliations performed for a template group instance, categorized by name.</li> <li>Type: Gauge</li> <li>Labels: <code>kind</code>, <code>name</code></li> <li>Usage: Provides insight into the frequency of reconciliation processes.</li> </ul> <p><code>multi_tenant_operator_reconcile_seconds</code></p> <ul> <li>Description: Represents the cumulative duration, in seconds, taken to reconcile a template group instance, categorized by instance name.</li> <li>Type: Gauge</li> <li>Labels: <code>kind</code>, <code>name</code></li> <li>Usage: Critical for assessing the time efficiency of the reconciliation process.</li> </ul> <p><code>multi_tenant_operator_reconcile_seconds_total</code></p> <ul> <li>Description: Tracks the total duration, in seconds, for all reconciliation processes of a template group instance, categorized by instance name.</li> <li>Type: Gauge</li> <li>Labels: <code>kind</code>, <code>name</code></li> <li>Usage: Useful for understanding the overall time spent on reconciliation processes.</li> </ul>"},{"location":"architecture/logs-metrics.html#custom-resource-status","title":"Custom Resource Status","text":"<p>In this section, we delve into the status of various custom resources managed by our controllers. The <code>kubectl describe</code> command can be used to fetch the status of these resources.</p>"},{"location":"architecture/logs-metrics.html#template-group-instance","title":"Template Group Instance","text":"<p>Status from the <code>templategroupinstances.tenantoperator.stakater.com</code> custom resource:</p> <ul> <li>Current Operational State: Provides a snapshot of the resource's current condition.</li> <li>Conditions: Offers a detailed view of the resource's status, which includes:<ul> <li><code>InstallSucceeded</code>: Indicates the success of the instance's installation.</li> <li><code>Ready</code>: Shows the readiness of the instance, with details on the last reconciliation process, its duration, and relevant messages.</li> <li><code>Running</code>: Reports on active processes like ongoing resource reconciliation.</li> </ul> </li> <li>Deployed Namespaces: Enumerates the namespaces where the instance has been deployed, along with their statuses and associated template manifests.</li> <li>Manifest Hashes: Includes the <code>Template Manifests Hash</code> and <code>Resource Mapping Hash</code>, which provide versioning and change tracking for template manifests and resource mappings.</li> </ul>"},{"location":"architecture/logs-metrics.html#log-interpretation-guide","title":"Log Interpretation Guide","text":""},{"location":"architecture/logs-metrics.html#template-group-instance-controller","title":"Template Group Instance Controller","text":"<p>Logs from the <code>tenant-operator-templategroupinstance-controller</code>:</p> <ul> <li>Reconciliation Process: Logs starting with <code>Reconciling!</code> mark the beginning of a reconciliation process for a TemplateGroupInstance. Subsequent actions like <code>Creating/Updating TemplateGroupInstance</code> and <code>Retrieving list of namespaces Matching to TGI</code> outline the reconciliation steps.</li> <li>Namespace and Resource Management: Logs such as <code>Namespaces test-namespace-1 is new or failed...</code> and <code>Creating/Updating resource...</code> detail the management of Kubernetes resources in specific namespaces.</li> <li>Worker Activities: Logs labeled <code>[Worker X]</code> show tasks being processed in parallel, including steps like <code>Validating parameters</code>, <code>Gathering objects from manifest</code>, and <code>Apply manifests</code>.</li> <li>Reconciliation Completion: Entries like <code>End Reconciling</code> and <code>Defering XXth Reconciling, with duration XXXms</code> indicate the end of a reconciliation process and its duration, aiding in performance analysis.</li> <li>Watcher Events: Logs from <code>Watcher</code> such as <code>Delete call received for object...</code> and <code>Following resource is recreated...</code> are key for tracking changes to Kubernetes objects.</li> </ul> <p>These logs are crucial for tracking the system's behavior, diagnosing issues, and comprehending the resource management workflow.</p>"},{"location":"console/capacity-planning.html","title":"Capacity Planning","text":"<p>The Capacity Planning feature in the app provides insights into resource usage and allocation across the cluster to help manage computing resources efficiently. It consists of three main sections:</p>"},{"location":"console/capacity-planning.html#1-graphical-representation","title":"1. Graphical Representation","text":""},{"location":"console/capacity-planning.html#a-node-filtering-based-on-labels","title":"a. Node filtering based on Labels","text":"<p>This part will allow users to group nodes based on their labels, to change how the data should be visualized for them.</p> <p>To add a node filter, user will be asked to provide a name and node labels in form of MatchExpressions.</p> <p></p> <p>After creation, node filters will look like this.</p> <p></p> <p>On clicking any of the node filters, the data in the next two parts will change accordingly.</p> <p></p>"},{"location":"console/capacity-planning.html#b-tenant-requests-vs-cluster-capacity","title":"b. Tenant Requests vs. Cluster Capacity","text":"<p>This section displays bar charts that compare the current resource requests from tenants (like CPU and memory) with the total available cluster capacity. The charts visually represent how much of the cluster\u2019s resources are currently being utilized versus what is available, helping identify over-utilization or under-utilization scenarios.</p>"},{"location":"console/capacity-planning.html#c-quota-requests-vs-cluster-capacity","title":"c. Quota Requests vs. Cluster Capacity","text":"<p>Similar to the tenant requests, this section compares the quota requests against the total cluster capacity. It allows administrators to see if the quota assigned is in line with the cluster's actual capacity.</p> <p></p>"},{"location":"console/capacity-planning.html#2-worker-pool-details","title":"2. Worker-pool Details","text":"<p>A detailed table lists the worker nodes in the cluster, displaying each node\u2019s CPU and memory capacity along with various labels that indicate the node\u2019s configuration and role (e.g., worker, infra). This information helps in identifying resource distribution across nodes and managing workloads accordingly.</p> <p></p>"},{"location":"console/capacity-planning.html#3-request-details","title":"3. Request Details","text":"<p>This table provides a breakdown of the resource requests from different tenants, displaying both the requested resources (CPU and memory) and the allocated quotas. It helps to monitor if tenant requests align with the quotas set for each tenant, ensuring optimal resource management.</p> <p></p>"},{"location":"console/configuration.html","title":"Configuration","text":""},{"location":"console/configuration.html#user-roles-and-permissions","title":"User Roles and Permissions","text":""},{"location":"console/configuration.html#administrators","title":"Administrators","text":"<p>Administrators have overarching access to the console, including the ability to view all namespaces and tenants. They have exclusive access to the IntegrationConfig, allowing them to view all the settings and integrations.</p> <p></p>"},{"location":"console/configuration.html#tenant-users","title":"Tenant Users","text":"<p>Regular tenant users can monitor and manage their allocated resources. However, they do not have access to the IntegrationConfig and cannot view resources across different tenants, ensuring data privacy and operational integrity.</p>"},{"location":"console/configuration.html#caching-and-database","title":"Caching and Database","text":"<p>MTO integrates a dedicated database to streamline resource management. Now, all resources managed by MTO are efficiently stored in a Postgres database, enhancing the MTO Console's ability to efficiently retrieve all the resources for optimal presentation.</p> <p>The implementation of this feature is facilitated by the Bootstrap controller, streamlining the deployment process. This controller creates the PostgreSQL Database, establishes a service for inter-pod communication, and generates a secret to ensure secure connectivity to the database.</p> <p>Furthermore, the introduction of a dedicated cache layer ensures that there is no added burden on the Kube API server when responding to MTO Console requests. This enhancement not only improves response times but also contributes to a more efficient and responsive resource management system.</p>"},{"location":"console/configuration.html#authentication-and-authorization","title":"Authentication and Authorization","text":""},{"location":"console/configuration.html#keycloak-for-authentication","title":"Keycloak for Authentication","text":"<p>MTO Console incorporates Keycloak, a leading authentication module, to manage user access securely and efficiently. Keycloak is provisioned automatically by our controllers, setting up a new realm, client, and a default user named <code>mto</code>.</p>"},{"location":"console/configuration.html#benefits","title":"Benefits","text":"<ul> <li>Industry Standard: Offers robust, reliable authentication in line with industry standards.</li> <li>Integration with Existing Systems: Enables easy linkage with existing Active Directories or SSO systems, avoiding the need for redundant user management.</li> <li>Administrative Control: Grants administrators full authority over user access to the console, enhancing security and operational integrity.</li> </ul>"},{"location":"console/configuration.html#postgresql-as-persistent-storage-for-keycloak","title":"PostgreSQL as Persistent Storage for Keycloak","text":"<p>MTO Console leverages PostgreSQL as the persistent storage solution for Keycloak, enhancing the reliability and flexibility of the authentication system.</p> <p>It offers benefits such as enhanced data reliability, easy data export and import.</p>"},{"location":"console/configuration.html#benefits_1","title":"Benefits","text":"<ul> <li>Persistent Data Storage: By using PostgreSQL, Keycloak's data, including realms, clients, and user information, is preserved even in the event of a pod restart. This ensures continuous availability and stability of the authentication system.</li> <li>Data Exportability: Customers can easily export Keycloak configurations and data from the PostgreSQL database.</li> <li>Transferability Across Environments: The exported data can be conveniently imported into another cluster or Keycloak instance, facilitating smooth transitions and backups.</li> <li>No Data Loss: Ensures that critical authentication data is not lost during system updates or maintenance.</li> <li>Operational Flexibility: Provides customers with greater control over their authentication data, enabling them to manage and migrate their configurations as needed.</li> </ul>"},{"location":"console/configuration.html#built-in-module-for-authorization","title":"Built-in module for Authorization","text":"<p>The MTO Console is equipped with an authorization module, designed to manage access rights intelligently and securely.</p>"},{"location":"console/configuration.html#benefits_2","title":"Benefits","text":"<ul> <li>User and Tenant Based: Authorization decisions are made based on the user's membership in specific tenants, ensuring appropriate access control.</li> <li>Role-Specific Access: The module considers the roles assigned to users, granting permissions accordingly to maintain operational integrity.</li> <li>Elevated Privileges for Admins: Users identified as administrators or members of the clusterAdminGroups are granted comprehensive permissions across the console.</li> <li>Database Caching: Authorization decisions are cached in the database, reducing reliance on the Kubernetes API server.</li> <li>Faster, Reliable Access: This caching mechanism ensures quicker and more reliable access for users, enhancing the overall responsiveness of the MTO Console.</li> </ul>"},{"location":"console/configuration.html#setting-up-user-access-in-keycloak-for-mto-console","title":"Setting Up User Access in Keycloak for MTO Console","text":"<p>This guide walks you through the process of adding new users in Keycloak and granting them access to Multi Tenant Operator (MTO) Console.</p>"},{"location":"console/configuration.html#accessing-keycloak-console","title":"Accessing Keycloak Console","text":"<ul> <li>Log in to the OpenShift Console.</li> <li>Go to the 'Routes' section within the 'multi-tenant-operator' namespace.</li> </ul> <ul> <li>Click on the Keycloak console link provided in the Routes.</li> <li>Login using the admin credentials (default: admin/admin).</li> </ul>"},{"location":"console/configuration.html#adding-new-users-in-keycloak","title":"Adding new Users in Keycloak","text":"<ul> <li>In the Keycloak console, switch to the <code>mto</code> realm.</li> </ul> <ul> <li>Go to the <code>Users</code> section in the <code>mto</code> realm.</li> <li>Follow the prompts to add a new user.</li> </ul> <ul> <li>Once you add a new user, here is how the Users section would look like</li> </ul>"},{"location":"console/configuration.html#accessing-mto-console","title":"Accessing MTO Console","text":"<ul> <li>Go back to the OpenShift Console, navigate to the Routes section, and get the URL for the MTO Console.</li> <li>Open the MTO Console URL and log in with the newly added user credentials.</li> </ul> <p>Now, at this point, a user will be authenticated to the MTO Console. But in order to get access to view any Tenant resources, the user will need to be part of a Tenant.</p>"},{"location":"console/configuration.html#granting-access-to-tenant-resources","title":"Granting Access to Tenant Resources","text":"<ul> <li>Open Tenant CR: In the OpenShift cluster, locate and open the Tenant Custom Resource (CR) that you wish to give access to. You will see a YAML file similar to the following example:</li> </ul> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: arsenal\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - gabriel@arsenal.com\n      groups:\n        - arsenal\n    editors:\n      users:\n        - hakimi@arsenal.com\n    viewers:\n      users:\n        - neymar@arsenal.com\n</code></pre> <ul> <li>Edit Tenant CR: Add the newly created user's email to the appropriate section (owners, editors, viewers) in the Tenant CR. For example, if you have created a user <code>john@arsenal.com</code> and wish to add them as an editor, the edited section would look like this:</li> </ul> <pre><code>editors:\n  users:\n    - gabriel@arsenal.com\n    - benzema@arsenal.com\n</code></pre> <ul> <li>Save Changes: Save and apply the changes to the Tenant CR.</li> </ul>"},{"location":"console/configuration.html#verifying-access","title":"Verifying Access","text":"<p>Once the above steps are completed, you should be able to access the MTO Console now and see alpha Tenant's details along with all the other resources such as namespaces and templates that John has access to.</p>"},{"location":"console/dashboard.html","title":"Dashboard","text":"<p>The dashboard serves as a centralized monitoring hub, offering insights into the current state of tenants, namespaces, and quotas. It is designed to provide a quick summary/snapshot of MTO resources' status. Additionally, it includes a Showback graph that presents a quick glance of the seven-day cost trends associated with the namespaces/tenants based on the logged-in user.</p> <p>By default, MTO Console will be disabled and has to be enabled by setting the below configuration in IntegrationConfig.</p> <pre><code>components:\n    console: true\n    ingress:\n      ingressClassName: &lt;ingress-class-name&gt;\n      console:\n        host: tenant-operator-console.&lt;hostname&gt;\n        tlsSecretName: &lt;tls-secret-name&gt;\n      gateway:\n        host: tenant-operator-gateway.&lt;hostname&gt;\n        tlsSecretName: &lt;tls-secret-name&gt;\n      keycloak:\n        host: tenant-operator-keycloak.&lt;hostname&gt;\n        tlsSecretName: &lt;tls-secret-name&gt;\n    showback: true\n    trustedRootCert: &lt;root-ca-secret-name&gt;\n</code></pre> <p><code>&lt;hostname&gt;</code> : hostname of the cluster <code>&lt;ingress-class-name&gt;</code> : name of the ingress class <code>&lt;tls-secret-name&gt;</code> : name of the secret that contains the TLS certificate and key <code>&lt;root-ca-secret-name&gt;</code> : name of the secret that contains the root CA certificate</p> <p>Note: <code>trustedRootCert</code> and <code>tls-secret-name</code> are optional. If not provided, MTO will use the default root CA certificate and secrets respectively.</p> <p>Once the above configuration is set on the IntegrationConfig, MTO would start provisioning the required resources for MTO Console to be ready. In a few moments, you should be able to see the Console Ingress in the <code>multi-tenant-operator</code> namespace which gives you access to the Console.</p> <p>For more details on the configuration, please visit here. </p>"},{"location":"console/hibernation.html","title":"Hibernation","text":"<p>The main purpose of the Hibernation workflow is, controlling resource consumption dynamically while enabling efficient management of namespaces based on their usage patterns and requirements.</p>"},{"location":"console/hibernation.html#namespace-list","title":"Namespace List","text":"<p>Displays a list of namespaces associated with a selected tenant. The tenant filter allows users to view namespaces relevant to a specific tenant.</p>"},{"location":"console/hibernation.html#status-columns","title":"Status Columns","text":"<ul> <li>Sleeping: Indicates whether a namespace is currently in a \"sleep\" state, with a checkmark appearing for namespaces that are set to sleep.</li> <li>Hibernated: Shows whether a namespace is in a \"hibernated\" state, marked similarly to the sleeping state.</li> <li>Hibernation Interval: Provides details about the schedule for sleep and wake intervals for specific namespaces. The schedule is represented in a format like \"Sleep Schedule: At 11 minutes past the hour, Wake Schedule: At 15 minutes past the hour.\"</li> </ul>"},{"location":"console/hibernation.html#actions-and-filters","title":"Actions and Filters","text":"<ul> <li>APPLY SLEEP / HIBERNATE: A button to initiate the sleep or hibernation action based on the selected namespaces and applied filters.</li> <li>Schedule Filters: Allows filtering namespaces by their hibernation schedule.</li> </ul>"},{"location":"console/hibernation.html#sleep-tab","title":"Sleep Tab","text":"<p>The \"SLEEP\" tab is active, indicating that namespaces can be put into sleep mode based on certain criteria.</p>"},{"location":"console/hibernation.html#sleep-namespace-by-labels","title":"Sleep Namespace by Labels","text":"<ul> <li>The interface allows namespaces to be put to sleep based on \"LABELS\". In this view, the \"LABELS\" option is selected.</li> <li>This approach allows filtering of namespaces using Kubernetes labels. Users can input label selectors in the text box to search amongst the available labels and target specific namespaces based on the unique labels.</li> <li> <p>In the example below are the selected labels:</p> <ul> <li><code>text kubernetes.io/metadata.name:stakater-tronador</code></li> <li><code>text stakater.com/quota:saap-quota-large</code></li> </ul> </li> <li> <p>After applying the label filters, <code>stakater-tronador</code> appears in the list of matching namespaces. The \"Sleeping\" column shows no indication of sleep state, suggesting it isn't yet asleep.</p> </li> <li>Users can click on the Update button to put the filtered namespace rows to sleep based on labels selected. As this will then be reflected in the table below and the main table.</li> </ul> <p></p>"},{"location":"console/hibernation.html#sleep-namespace-by-name","title":"Sleep Namespace by Name","text":"<ul> <li>The interface shows the \"NAME\" option selected instead of \"LABELS\".</li> <li>Labels selection here will act as a filter for the table rows and allow user to filter the namespaces based on the applied/selected labels.</li> <li>This allows users to select the namespaces by their names individually, after being filtered based on the labels filter.</li> <li>The filter input accepts namespace names or partial matches to narrow down the list. Here, two filters are applied:<ul> <li><code>text stakater.com/kind:flux-system</code></li> <li><code>text stakater.com/quota:saap-quota-large</code></li> </ul> </li> <li>The filtered result displays <code>flux-system</code> as a match. A checkbox is provided next to the namespace, likely allowing selection for sleep actions.</li> <li>Users can click on the Update button to put the filtered namespace rows to sleep based on namespaces selected by name. As this will then be reflected in the table below and the main Hibernation table.</li> </ul> <ul> <li>Another example can be seen where the <code>stakater.com/tenant:alpha</code> is selected and rows based on the selected labels are filtered.</li> <li>User can select the namespaces individually by name to put them to sleep.</li> </ul>"},{"location":"console/hibernation.html#hibernation-tab","title":"Hibernation Tab","text":"<p>The \"HIBERNATE\" tab is active, indicating that namespaces can be put into hibernation mode based on selected schedule.</p> <p>Unlike \"SLEEP\" tab, hibernate requires an interval to be selected in order to do any action on the tab.</p>"},{"location":"console/hibernation.html#creating-an-interval","title":"Creating an Interval","text":"<ul> <li>User can create a hibernation interval with custom schedules for when the namespace should enter sleep mode and when it should wake up.</li> <li>The schedules should be a cron value as it will be reflected below the input.</li> <li>The schedule name after creation will append the tenant name to it's end indicating the tenant it belongs to for clarification.</li> <li>Each interval is defined by a Sleep Schedule and a Wake Schedule in cron format (e.g., <code>\"30 * * * *\"</code> for 30 minutes past each hour).</li> </ul>"},{"location":"console/hibernation.html#selecting-an-interval","title":"Selecting an Interval","text":"<ul> <li>Before applying hibernation to any namespace, an interval must be selected</li> <li>This selection is mandatory; without choosing an interval, the hibernation action cannot proceed, ensuring a clear and structured schedule is always in place.</li> </ul>"},{"location":"console/hibernation.html#hibernate-namespace-by-labels","title":"Hibernate Namespace by Labels","text":"<ul> <li>The interface allows namespaces to be put to hibernation state based on \"LABELS\". In this view, the \"LABELS\" option is selected.</li> <li>This approach allows filtering of namespaces using Kubernetes labels. Users can input label selectors in the text box to search amongst the available labels and target specific namespaces based on the unique labels.</li> <li>In the example below are the selected labels: <code>text stakater.com/kind.stakater-forecastle</code>.</li> <li>After applying the label filters, <code>stakater-forecastle</code> appears in the list of matching namespaces. The \"Hibernated\" column shows no indication of hibernation state, suggesting it isn't yet hibernated.</li> <li>Users can click on the Update button to hibernate the namespace based on labels selected. As this will then be reflected in the table below and the main table.</li> </ul> <ul> <li>After the filtered namespaces are hibernated the hibernation interval can be seen in the column and value can be read by hovering over the interval.</li> </ul>"},{"location":"console/hibernation.html#hibernate-namespace-by-name","title":"Hibernate Namespace by Name","text":"<ul> <li>The interface shows the \"NAME\" option selected instead of \"LABELS\".</li> <li>Labels selection here will act as a filter for the table rows and allow user to filter the namespaces based on the applied/selected labels.</li> <li>The filter input accepts namespace names or partial matches to narrow down the list. Here, one filters are applied: <code>text stakater.com/kind:flux-reloader</code></li> <li>The filtered result displays <code>stakater-reloader</code> as a match. A checkbox is provided next to the namespace, likely allowing selection for hibernating the namespace.</li> <li>Users can click on the Update button to put the filtered namespace rows to sleep based on namespaces selected by name. As this will then be reflected in the table below and the main Hibernation table.</li> </ul> <ul> <li>After the selected namespaces are hibernated the hibernation interval can be seen in the column and value can be read by hovering over the interval.</li> </ul>"},{"location":"console/hibernation.html#common-elements-for-both-tabs","title":"Common Elements for Both Tabs","text":"<ul> <li>Sleep and Hibernate Tabs: Users can toggle between \"SLEEP\" and \"HIBERNATE\" actions, indicating separate states or behaviors for namespaces.</li> <li>Rows per page selection: Users can adjust how many namespaces are displayed on the page, with a default of 5 in this view.</li> <li>Update Button: Located at the bottom, allowing users to apply the changes made, such as putting the selected namespaces to sleep.</li> </ul>"},{"location":"console/namespaces.html","title":"Namespaces","text":"<p>Users can view all the namespaces that belong to their tenant, offering a comprehensive perspective of the accessible namespaces for tenant members. This section also provides options for detailed exploration.</p> <p></p>"},{"location":"console/overview.html","title":"Overview","text":"<p>This documentation is divided into the following sections to guide you through MTO\u2019s Console capabilities:</p> <ul> <li>Dashboard \u2013 Overview of the UI and how to navigate.</li> <li>Tenants \u2013 Managing and configuring tenants.</li> <li>Namespaces \u2013 Creating and organizing namespaces.</li> <li>Hibernation \u2013 Pausing workloads to optimize costs.</li> <li>Cost Analysis \u2013 Monitoring usage and cost breakdowns.</li> <li>Quotas \u2013 Enforcing resource allocation policies.</li> <li>Templates \u2013 Using templates to streamline namespace creation.</li> <li>Capacity Planning \u2013 Strategies for managing cluster capacity efficiently.</li> <li>Configuration \u2013 Configuration of the console.</li> </ul>"},{"location":"console/quotas.html","title":"Quotas","text":"<p>MTO's Quotas are crucial for managing resource allocation. In this section, administrators can assess the quotas assigned to each tenant, ensuring a balanced distribution of resources in line with operational requirements.</p> <p></p>"},{"location":"console/quotas.html#create-quota","title":"Create Quota","text":"<p>The quota creation process allows administrators to define resource limits and optional configurations for containers and pods. This document outlines the steps to create a new quota, including the metadata, resource quota, and limit range configurations.</p>"},{"location":"console/quotas.html#step-1-metadata","title":"Step 1: Metadata","text":"<ul> <li> <p>Name Field</p> <ul> <li>The name field is mandatory and must be unique.</li> <li>If the name already exists, an inline error message is displayed.</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Regex Validation</li> <li>The quota name must conform to the following regex pattern:</li> </ul> <pre><code>    /^[a-z0-9]+(-[a-z0-9]+)*$/\n</code></pre> <ul> <li>This ensures that quota names consist of lowercase alphanumeric characters and hyphens, and do not start or end with a hyphen.</li> <li>The metadata section in the drawer while creating a quota is to provide a name for the quota. Users must ensure the quota name meets the specified criteria. Also, the quota name should not already exist in order to create a new quota.</li> </ul> </li> </ul>"},{"location":"console/quotas.html#step-2-resource-quota","title":"Step 2: Resource Quota","text":"<ol> <li>Adding Resources</li> <li>The Resource dropdown allows the selection of resource types such as:<ul> <li>CPU Requests</li> <li>Memory Requests</li> <li>Config Maps</li> <li>Secrets</li> <li>Services</li> <li>Load Balancer Services</li> </ul> </li> <li> <p>Enter the corresponding Value for the selected resource.</p> </li> <li> <p>Error Handling</p> </li> <li> <p>If an invalid format is entered in the Value field, an inline error message is displayed.</p> </li> <li> <p>Add Resource Button:</p> </li> <li>Allows users to add multiple resources sequentially.</li> </ol>"},{"location":"console/quotas.html#step-3-limit-range-optional","title":"Step 3: Limit Range (Optional)","text":"<p>The limit range section provides optional configurations for containers and pods. It includes:</p> <ol> <li>Container</li> <li> <p>Configure limits for container resources by entering:</p> <ul> <li>Min and Max values for resource types.</li> <li>Default Request and Default Limit values.</li> </ul> </li> <li> <p>Pod</p> </li> <li> <p>Similar to the container section, allows configuration for pod-level resources.</p> </li> <li> <p>Error Handling</p> </li> <li>No validation errors occur unless an invalid value is entered.</li> <li>Inline errors guide users in correcting their inputs.</li> </ol>"},{"location":"console/quotas.html#final-step-save-quota","title":"Final Step: Save Quota","text":"<ol> <li>Add Quota Button</li> <li> <p>Once all configurations are completed, users can click the Add Quota button to save the quota.</p> </li> <li> <p>Completion</p> </li> <li>The system validates all input fields before saving.</li> <li>A confirmation message is displayed once the quota is successfully created.</li> </ol>"},{"location":"console/quotas.html#notes","title":"Notes","text":"<ul> <li>The entire quota creation process is intuitive, with inline validation to guide users.</li> <li>Optional configurations like \"Limit Range\" allow flexibility based on use cases.</li> </ul>"},{"location":"console/quotas.html#update-quota","title":"Update Quota","text":"<p>User can click on the edit button in the table under the action items to open the drawer with all the pre-populated quota configurations.</p> <p>The update process follows a similar flow to the create process. However, the key difference is that the quota name in Step-1 cannot be edited or updated. All other steps and configurations remain the same, allowing users to modify resource quota, limit range values as needed.</p>"},{"location":"console/quotas.html#delete-quota","title":"Delete Quota","text":"<p>By clicking on the delete option in the quota table the user will be able to perform delete operation, a confirmation modal will open which will prompt the user to delete or cancel operation.</p>"},{"location":"console/showback.html","title":"Showback","text":"<p>The Showback feature is an essential financial governance tool, providing detailed insights into the cost implications of resource usage by tenant or namespace or other filters. This facilitates a transparent cost management and internal chargeback or showback process, enabling informed decision-making regarding resource consumption and budgeting.</p> <p></p>"},{"location":"console/templates.html","title":"Templates","text":"<p>The Templates section acts as a repository for standardized resource deployment patterns, which can be utilized to maintain consistency and reliability across tenant environments. Few examples include provisioning specific k8s manifests, helm charts, secrets or configmaps across a set of namespaces.</p> <p> </p>"},{"location":"console/tenants.html","title":"Tenants","text":"<p>Here, admins have a bird's-eye view of all tenants, with the ability to delve into each one for detailed examination and management. This section is pivotal for observing the distribution and organization of tenants within the system. More information on each tenant can be accessed by clicking the view option against each tenant name.</p> <p></p>"},{"location":"console/tenants.html#live-yaml-and-graph-view","title":"Live YAML and Graph View","text":"<p>In the MTO Console, each resource section is equipped with a \"View\" button, revealing the live YAML configuration for complete information on the resource. For Tenant resources, a supplementary \"Graph\" option is available, illustrating the relationships and dependencies of all resources under a Tenant. This dual-view approach empowers users with both the detailed control of YAML and the holistic oversight of the graph view.</p> <p></p> <p>Effortlessly associate tenants with their respective resources using the enhanced graph feature on the MTO Console. This dynamic graph illustrates the relationships between tenants and the resources they create, encompassing both MTO's proprietary resources and native Kubernetes/OpenShift elements.</p> <p>Example Graph:</p> <pre><code>  graph LR;\n      A(alpha)--&gt;B(dev);\n      A--&gt;C(prod);\n      B--&gt;D(limitrange);\n      B--&gt;E(owner-rolebinding);\n      B--&gt;F(editor-rolebinding);\n      B--&gt;G(viewer-rolebinding);\n      C--&gt;H(limitrange);\n      C--&gt;I(owner-rolebinding);\n      C--&gt;J(editor-rolebinding);\n      C--&gt;K(viewer-rolebinding);</code></pre> <p>Explore with an intuitive graph that showcases the relationships between tenants and their resources. The MTO Console's graph feature simplifies the understanding of complex structures, providing you with a visual representation of your tenant's organization.</p> <p>To view the graph of your tenant, follow the steps below:</p> <ul> <li>Navigate to <code>Tenants</code> page on the MTO Console using the left navigation bar. </li> <li>Click on <code>View</code> of the tenant for which you want to view the graph. </li> <li>Click on <code>Graph</code> tab on the tenant details page. </li> </ul>"},{"location":"console/tenants.html#tenant-quota","title":"Tenant Quota","text":"<p>In this view, users can access a dedicated tab to review the quota utilization for their Tenants. Within this tab, users have the option to toggle between two different views: Aggregated Quota and Namespace Quota.</p>"},{"location":"console/tenants.html#aggregated-quota-view","title":"Aggregated Quota View","text":"<p> This view provides users with an overview of the combined resource allocation and usage across all namespaces within their tenant. It offers a comprehensive look at the total limits and usage of resources such as CPU, memory, and other defined quotas. Users can easily monitor and manage resource distribution across their entire tenant environment from this aggregated perspective.</p>"},{"location":"console/tenants.html#namespace-quota-view","title":"Namespace Quota View","text":"<p> Alternatively, users can opt to view quota settings on a per-namespace basis. This view allows users to focus specifically on the resource allocation and usage within individual namespaces. By selecting this option, users gain granular insights into the resource constraints and utilization for each namespace, facilitating more targeted management and optimization of resources at the namespace level.</p>"},{"location":"console/tenants.html#tenant-utilization","title":"Tenant Utilization","text":"<p>In the Utilization tab of the tenant console, users are presented with a detailed table listing all namespaces within their tenant. This table provides essential metrics for each namespace, including CPU and memory utilization. The metrics shown include:</p> <ul> <li>Cost: The cost associated with CPU and memory utilization.</li> <li>Request Average: The average amount of CPU and memory resources requested.</li> <li>Usage Average: The average amount of CPU and memory resources used.</li> <li>Max: The maximum value between CPU and memory requests and used resources, calculated every 30 seconds and averaged over the selected running minutes.</li> </ul> <p>Users can adjust the interval window using the provided selector to customize the time frame for the displayed data. This table allows users to quickly assess resource utilization across all namespaces, facilitating efficient resource management and cost tracking.</p> <p></p> <p>Upon selecting a specific namespace from the utilization table, users are directed to a detailed view that includes CPU and memory utilization graphs along with a workload table. This detailed view provides:</p> <ul> <li>CPU and Memory Graphs: Visual representations of the namespace's CPU and memory usage over time, enabling users to identify trends and potential issues at a glance.</li> <li>Workload Table: A comprehensive list of all workloads within the selected namespace, including pods, deployments, and stateful-sets. The table displays key metrics for each workload, including:<ul> <li>Cost: The cost associated with the workload's CPU and memory utilization.</li> <li>Request Average: The average amount of CPU and memory resources requested by the workload.</li> <li>Usage Average: The average amount of CPU and memory resources used by the workload.</li> <li>Max: The maximum value between CPU and memory requests and used resources, calculated every 30 seconds and averaged over the running minutes.</li> </ul> </li> </ul> <p>This detailed view provides users with in-depth insights into resource utilization at the workload level, enabling precise monitoring and optimization of resource allocation within the selected namespace.</p> <p></p>"},{"location":"console/tenants.html#create-tenant","title":"Create Tenant","text":"<p>The tenant creation process involves a three-step drawer interface. This document outlines the validation checks and the criteria for progressing through these steps.</p>"},{"location":"console/tenants.html#step-1-enter-primary-info","title":"Step 1: Enter Primary Info","text":"<p>The first step in creating a tenant is to provide a name for the tenant. Users must ensure the tenant name meets the specified criteria. The Next button remains disabled until the entered tenant name passes validation and a quota is selected.</p>"},{"location":"console/tenants.html#validation-criteria","title":"Validation Criteria","text":"<ul> <li> <p>Regex Validation</p> <ul> <li>The tenant name must conform to the following regex pattern:</li> </ul> <pre><code>    /^[a-z0-9]+(-[a-z0-9]+)*$/\n</code></pre> <ul> <li>This ensures that tenant names consist of lowercase alphanumeric characters and hyphens, and do not start or end with a hyphen.</li> </ul> </li> <li> <p>Uniqueness Check</p> <ul> <li>The tenant name must be unique.</li> <li>When tenant name is entered and user stops typing the tenant name is verified of its uniqueness through an API call to the backend for confirmation that the name does not already exist. If the name is unique the Next button is enabled and user can click on it. If the name is already taken, an error is displayed, and the user must choose a different name.</li> </ul> </li> </ul>"},{"location":"console/tenants.html#quota-details","title":"Quota Details","text":"<p>The selected quota defines resource limits for the tenant. Example quota details are shown in JSON format as well as a toggle button to view them as YAML:</p> <pre><code>resourcequota:\n  hard:\n    configmaps: '50'\n    requests.cpu: '50'\n    requests.memory: 5Gi\n    secrets: '50'\n    services: '050'\n    services.loadbalancers: '1'\nlimitrange:\n  limits:\n    - type: Pod\n      max:\n        cpu: '1'\n        memory: 1Gi\n      min:\n        cpu: 100m\n        memory: 50Mi\n</code></pre>"},{"location":"console/tenants.html#enabling-the-next-button","title":"Enabling the Next Button","text":"<p>The Next button becomes enabled only when     - The entered tenant name passes both regex validation and the uniqueness check.     - A quota is selected from the dropdown menu.</p>"},{"location":"console/tenants.html#error-handling","title":"Error Handling","text":"<p>Error Handling on step 1 is based on the following factors</p> <ul> <li>If the tenant name fails regex validation, an inline error message indicates the naming rule.</li> <li>If the tenant name already exists, the user is prompted to enter a new name.</li> </ul>"},{"location":"console/tenants.html#step-2-access-control-optional","title":"Step 2: Access Control (Optional)","text":"<p>The second step allows users to configure access control for the tenant. This step is optional and provides three tabs for managing user roles: Owners, Editors, and Viewers.</p> <p>If the user has some data entered into the step 2 section tabs, the skip option will be disabled. Otherwise, user can perform skip action by clicking on the Skip button.</p>"},{"location":"console/tenants.html#input-fields-and-listing-behavior","title":"Input Fields and Listing Behavior","text":"<ul> <li>Input Fields     -Each tab (Owners, Editors, and Viewers) contains two input fields:         - Users: Allows adding individual users.         - Groups: Allows adding groups.<ul> <li>The input fields also act as filters for existing values.</li> <li>If the entered value does not already exist in the list, it can be added.</li> </ul> </li> <li>Listing Added Values<ul> <li>Values entered into the input fields are displayed as a scroll list below the fields.</li> <li>Each added value is displayed with a remove button (\"X\") to allow easy deletion.</li> </ul> </li> </ul>"},{"location":"console/tenants.html#key-features","title":"Key Features","text":"<ul> <li>Optional Configuration: Users can skip this step by clicking the Skip button.</li> <li>Filtering: As users type into the input fields, existing values matching the input are shown as suggestions for quick selection.</li> <li>Add New Values: If the entered value is not in the existing list, users can add it by confirming the input.</li> <li>Scroll Support: The lists have scroll view, ensuring all entries are accessible even when the list grows long.</li> </ul>"},{"location":"console/tenants.html#navigating-to-the-next-step","title":"Navigating to the Next Step","text":"<ul> <li>Users can proceed to the next step by clicking the Next button.</li> <li>The Next button is always enabled for this step as it is optional.</li> </ul>"},{"location":"console/tenants.html#error-handling_1","title":"Error Handling","text":"<p>No specific validations are required for this step since it is optional. However, users can remove or adjust entries as needed using the provided interface.</p>"},{"location":"console/tenants.html#step-3-namespace-optional","title":"Step 3: Namespace (Optional)","text":"<p>The third step allows users to optionally configure namespaces and metadata for the tenant. This step consists of two tabs:</p>"},{"location":"console/tenants.html#namespace","title":"Namespace","text":"<ul> <li>Toggles:<ul> <li>Enable Sandbox: Activates the sandbox environment for the tenant.</li> <li>Private Sandbox: Makes the sandbox environment private.</li> <li>On Delete Purge Namespaces: Ensures namespaces are purged upon deletion.</li> </ul> </li> <li>Input Fields:<ul> <li>With Tenant Prefix:<ul> <li>Allows users to add namespaces with a tenant prefix.</li> <li>Filters data based on existing namespaces.</li> </ul> </li> <li>Without Tenant Prefix:<ul> <li>Allows users to add namespaces without a tenant prefix.</li> <li>Filters data and validates input to check for existing namespaces.</li> <li>Displays an error if the namespace already exists (as shown in the UI).</li> </ul> </li> </ul> </li> <li>List Behavior:<ul> <li>Added namespaces are displayed in a scroll list below the input fields.</li> <li>Each namespace is displayed with a remove button (\"X\") for easy deletion.</li> </ul> </li> </ul>"},{"location":"console/tenants.html#metadata","title":"Metadata","text":"<p>The Metadata tab is divided into three sub-tabs:</p>"},{"location":"console/tenants.html#common-and-sandbox-tabs","title":"Common and Sandbox Tabs","text":"<ul> <li>Both tabs allow users to add annotations and labels using Key and Value input fields.</li> <li>Inputs act as filters for existing values.</li> <li>No validation is required for these fields.</li> <li>Added annotations and labels are displayed in a scroll list with remove buttons (\"X\") for easy deletion.</li> </ul>"},{"location":"console/tenants.html#specific-tab","title":"Specific Tab","text":"<ul> <li>Includes an Add Accordion button.</li> <li>Each accordion contains the following fields:<ul> <li>Namespaces: Users can add and filter namespaces. Each namespace is verified before being added to the list.</li> <li>Annotations: Key-value pairs that act as filters for listed values.</li> <li>Labels: Key-value pairs that act as filters for listed values.</li> </ul> </li> <li>Each field displays added values in a scroll list with remove buttons (\"X\") for easy deletion.</li> </ul>"},{"location":"console/tenants.html#key-features_1","title":"Key Features","text":"<ul> <li>Optional Configuration: Users can skip this step by clicking the Create button directly.</li> <li>Filtering and Validation:<ul> <li>Inputs filter data based on existing values.</li> <li>Namespace inputs validate entries to ensure uniqueness.</li> </ul> </li> <li>Scroll Support: All lists have scroll view, ensuring accessibility for large datasets.</li> </ul>"},{"location":"console/tenants.html#error-handling_2","title":"Error Handling","text":"<ul> <li>The \"Without Tenant Prefix\" input validates namespace uniqueness and displays an error if the namespace already exists.</li> <li>Namespaces in the accordions are being verified before being added. If they already exist they are not added and shown with an error message on the input.</li> <li>No validation errors occur for annotations or labels as they are free-form inputs.</li> </ul>"},{"location":"console/tenants.html#completion","title":"Completion","text":"<ul> <li>Users finalize the tenant creation process by clicking the Create button.</li> </ul>"},{"location":"console/tenants.html#yaml-view","title":"YAML View","text":"<ul> <li>A YAML representation of the configuration can be previewed using the Show YAML button before creation.</li> </ul>"},{"location":"console/tenants.html#update-tenant","title":"Update Tenant","text":"<p>User can click on the edit button in the table under the action items to open the drawer with all the pre-populated tenant configurations.</p> <p>The update process follows a similar flow to the create process. However, the key difference is that the tenant name in Step-1 cannot be edited or updated. All other steps and configurations remain the same, allowing users to modify access control, namespaces, and metadata as needed.</p>"},{"location":"console/tenants.html#delete-tenant","title":"Delete Tenant","text":"<p>By clicking on the delete option in the tenants table the user will be able to perform delete operation, and it may take a short while to delete the tenant.</p>"},{"location":"installation/azure-aks.html","title":"On AKS","text":"<p>This document covers how to deploy Multi Tenant Operator with an AKS (Azure Kubernetes Service) cluster.</p>"},{"location":"installation/azure-aks.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Make sure Azure CLI version 2.0.61 or later is installed and configured. If you need to install or upgrade, see Install Azure CLI. To find the version, run: <code>az --version</code></li> <li>You need kubectl as well, with a minimum version of 1.18.3. If you need to install, see Install kubectl.</li> <li>To install MTO, you need Helm CLI as well. Visit Installing Helm to get Helm CLI</li> <li>You need to have a user in Azure Portal, which we will use as the administrator for creating clusters, users and groups with enough permissions for the respective tasks</li> </ul>"},{"location":"installation/azure-aks.html#create-an-admin-group","title":"Create an Admin Group","text":"<p>Start by creating an admin group which will later be used as the administrator of our AKS cluster. Log in to Azure Portal from CLI:</p> <pre><code>az login\n</code></pre> <pre><code>$ az ad signed-in-user show\n\n{\n  \"@odata.context\": \"https://graph.microsoft.com/v1.0/$metadata#users/$entity\",\n  \"businessPhones\": [],\n  \"displayName\": \"test-admin-user\",\n  \"givenName\": null,\n  \"id\": \"&lt;user-id&gt;\",\n  \"jobTitle\": null,\n  \"mail\": null,\n  \"mobilePhone\": null,\n  \"officeLocation\": null,\n  \"preferredLanguage\": null,\n  \"surname\": null,\n  \"userPrincipalName\": \"&lt;upn&gt;\"\n}\n</code></pre> <p>The user-id will be used later to add our user in the admin group for MTO.</p> <p>Create a group called <code>mto-admins</code>:</p> <pre><code>az ad group create --display-name mto-admins --mail-nickname mto-admins\n</code></pre> <p>Using the above user-id, link user to the newly created group:</p> <pre><code>az ad group member add --group mto-admins --member-id &lt;user-id&gt;\n</code></pre> <p>Use this command to get admin-group-id, this will later be used while provisioning the cluster:</p> <pre><code>$ az ad group show --group mto-admins\n\n{\n  **********\n  \"description\": null,\n  \"displayName\": \"mto-admins\",\n  \"expirationDateTime\": null,\n  \"groupTypes\": [],\n  \"id\": \"&lt;admin-group-id&gt;\",\n  \"isAssignableToRole\": null,\n  **********\n}\n</code></pre>"},{"location":"installation/azure-aks.html#create-an-aks-cluster","title":"Create an AKS Cluster","text":"<p>Create a Resource Group by using the <code>az group create</code> command in your preferred Azure location:</p> <pre><code>az group create --name myResourceGroup --location westus2\n</code></pre> <p>Create a small cluster:</p> <pre><code>az aks create --resource-group myResourceGroup --name myAKSCluster --node-count 1 --vm-set-type VirtualMachineScaleSets --enable-cluster-autoscaler --min-count 1 --max-count 3 --enable-aad --aad-admin-group-object-ids &lt;admin-group-id&gt;\n</code></pre>"},{"location":"installation/azure-aks.html#create-test-groups-in-entra-id","title":"Create test groups in <code>Entra ID</code>","text":"<p>First, store the ID of your AKS cluster in a variable named AKS_ID:</p> <pre><code>AKS_ID=$(az aks show --resource-group myResourceGroup --name myAKSCluster --query id -o tsv)\n</code></pre> <p>Create your first test group named <code>appdev</code> using group command and assign its ID to <code>APPDEV_ID</code> variable:</p> <pre><code>APPDEV_ID=$(az ad group create --display-name appdev --mail-nickname appdev --query id -o tsv)\n</code></pre> <p>Allow the <code>appdev</code> group to interact with the AKS cluster using kubectl by assigning them the Azure Kubernetes Service Cluster User Role:</p> <pre><code>az role assignment create --assignee $APPDEV_ID --role \"Azure Kubernetes Service Cluster User Role\" --scope $AKS_ID\n</code></pre> <p>Create your second test group named <code>opssre</code> using the command and assign its ID to the <code>OPSSRE_ID</code> variable:</p> <pre><code>OPSSRE_ID=$(az ad group create --display-name opssre --mail-nickname opssre --query id -o tsv)\n</code></pre> <p>Allow the <code>opssre</code> group to interact with the AKS cluster using kubectl by assigning them the Azure Kubernetes Service Cluster User Role:</p> <pre><code>az role assignment create --assignee $OPSSRE_ID --role \"Azure Kubernetes Service Cluster User Role\" --scope $AKS_ID\n</code></pre>"},{"location":"installation/azure-aks.html#create-test-users-in-entra-id","title":"Create test users in <code>Entra ID</code>","text":"<p>Set User Principal Name (UPN) and password for your users. The UPN must include the verified domain name of your tenant, for example <code>user@company.com</code>.</p> <p>Following command reads the UPN for the <code>appdev</code> group and stores it in the <code>AAD_DEV_UPN</code> variable:</p> <pre><code>echo \"Please enter the UPN for application developers: \" &amp;&amp; read AAD_DEV_UPN\n</code></pre> <p>For this scope of this blog, we will assume that the entered UPN was <code>aksdev@company.com</code>.</p> <p>Following command reads the password for your user and stores it in the <code>AAD_DEV_PW</code> variable:</p> <pre><code>echo \"Please enter the secure password for application developers: \" &amp;&amp; read AAD_DEV_PW\n</code></pre> <p>Create the user <code>AKS Dev</code> using the previously created variables:</p> <pre><code>AKSDEV_ID=$(az ad user create --display-name \"AKS Dev\" --user-principal-name $AAD_DEV_UPN --password $AAD_DEV_PW --query id -o tsv)\n</code></pre> <p>Add this user to the <code>appdev</code> group that was previously created:</p> <pre><code>az ad group member add --group appdev --member-id $AKSDEV_ID\n</code></pre> <p>Repeat the steps for <code>OPS SRE</code> user.</p> <p>The following command reads the UPN for your user and stores it in the <code>AAD_SRE_UPN</code> variable:</p> <pre><code>echo \"Please enter the UPN for SREs: \" &amp;&amp; read AAD_SRE_UPN\n</code></pre> <p>For this scope of this blog, we will assume that the entered UPN was <code>opssre@company.com</code>.</p> <p>The following command reads the password for your user and stores it in the AAD_SRE_PW variable:</p> <pre><code>echo \"Please enter the secure password for SREs: \" &amp;&amp; read AAD_SRE_PW\n</code></pre> <p>Create the user <code>AKS SRE</code> using above variables</p> <pre><code>AKSSRE_ID=$(az ad user create --display-name \"AKS SRE\" --user-principal-name $AAD_SRE_UPN --password $AAD_SRE_PW --query id -o tsv)\n</code></pre> <p>Add this user to the <code>opssre</code> group that was previously created:</p> <pre><code>az ad group member add --group opssre --member-id $AKSSRE_ID\n</code></pre>"},{"location":"installation/azure-aks.html#installing-cert-manager-and-mto","title":"Installing Cert Manager and MTO","text":"<p>In this section, we will install Multi Tenant Operator (MTO) for tenancy between different users and groups. MTO has several webhooks which need certificates. For automated handling of certs, we will install Cert Manager as a prerequisite.</p> <p>Start by logging in to Azure from CLI by running the following command:</p> <pre><code>kubectl get pods\n</code></pre> <p>Executing the command will take you to a browser window where you can log in from your test-admin-user.</p> <p>Running <code>kubectl auth whoami</code> will show you the user info:</p> <pre><code>$ kubectl auth whoami\n\nATTRIBUTE    VALUE\nUsername     test-admin-user\nGroups       [&lt;mto-admins-id&gt; system:authenticated]\nExtra: oid   [&lt;oid&gt;]\n</code></pre> <p>You will notice that the <code>mto-admins</code> group ID is attached with our test-admin-user user. This user will be used for all the cluster admin level operations.</p>"},{"location":"installation/azure-aks.html#install-cert-manager","title":"Install Cert Manager","text":"<p>Install Cert Manager in the cluster for automated handling of operator webhook certs:</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.4/cert-manager.yaml\n</code></pre> <p>Let's wait for the pods to be up:</p> <pre><code>$ kubectl get pods -n cert-manager --watch\n\nNAME                                       READY   STATUS    RESTARTS   AGE\ncert-manager-7fb948f468-wgcbx              1/1     Running   0          7m18s\ncert-manager-cainjector-75c5fc965c-wxtkp   1/1     Running   0          7m18s\ncert-manager-webhook-757c9d4bb7-wd9g8      1/1     Running   0          7m18s\n</code></pre>"},{"location":"installation/azure-aks.html#install-mto-using-helm","title":"Install MTO using Helm","text":"<p>Helm will be used to install MTO as it is the only available way of installing it on Kubernetes Clusters.</p> <p>Use helm install command to install MTO helm chart. Here, <code>bypassedGroups</code> has to be set as <code>system:masters</code> as it is used by <code>masterclient</code> of AKS and <code>&lt;mto-admins-id&gt;</code>as it is used by <code>test-admin-user</code>:</p> <pre><code>helm install tenant-operator oci://ghcr.io/stakater/public/charts/multi-tenant-operator --version 0.12.62 --namespace multi-tenant-operator --create-namespace --set bypassedGroups='system:masters\\,&lt;mto-admins-id&gt;'\n</code></pre> <p>Wait for the pods to come to a running state:</p> <pre><code>$ kubectl get pods -n multi-tenant-operator --watch\n\nNAME                                                              READY   STATUS    RESTARTS   AGE\ntenant-operator-namespace-controller-768f9459c4-758kb             2/2     Running   0          2m\ntenant-operator-pilot-controller-7c96f6589c-d979f                 2/2     Running   0          2m\ntenant-operator-resourcesupervisor-controller-566f59d57b-xbkws    2/2     Running   0          2m\ntenant-operator-template-quota-intconfig-controller-7fc99462dz6   2/2     Running   0          2m\ntenant-operator-templategroupinstance-controller-75cf68c872pljv   2/2     Running   0          2m\ntenant-operator-templateinstance-controller-d996b6fd-cx2dz        2/2     Running   0          2m\ntenant-operator-tenant-controller-57fb885c84-7ps92                2/2     Running   0          2m\ntenant-operator-webhook-5f8f675549-jv9n8                          2/2     Running   0          2m\n</code></pre>"},{"location":"installation/azure-aks.html#setting-up-tenant-for-users","title":"Setting up Tenant for Users","text":"<p>Start by getting IDs for <code>opssre</code> and <code>appdev</code> groups by running <code>az ad group show</code> command:</p> <pre><code>$ az ad group show --group appdev\n\n{\n  ***********\n  \"displayName\": \"opssre\",\n  \"expirationDateTime\": null,\n  \"groupTypes\": [],\n  \"id\": \"&lt;opssre-group-id&gt;\",\n  \"isAssignableToRole\": null,\n  ************\n}\n</code></pre> <pre><code>$ az ad group show --group appdev\n\n{\n  ***********\n  \"displayName\": \"appdev\",\n  \"expirationDateTime\": null,\n  \"groupTypes\": [],\n  \"id\": \"&lt;appdev-group-id&gt;\",\n  \"isAssignableToRole\": null,\n  ************\n}\n</code></pre> <p>Create a <code>Quota CR</code> with some resource limits:</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: tenantoperator.stakater.com/v1beta1\nkind: Quota\nmetadata:\n  name: small\nspec:\n  limitrange:\n    limits:\n    - max:\n        cpu: 800m\n      min:\n        cpu: 200m\n      type: Container\n  resourcequota:\n    hard:\n      configmaps: \"10\"\nEOF\n</code></pre> <p>Now, mention this <code>Quota</code> in two <code>Tenant</code> CRs, with <code>opssre</code> and <code>appdev</code> group IDs in the groups section of spec:</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: tenant-a\nspec:\n  namespaces:\n    withTenantPrefix:\n    - dev\n    - build\n  accessControl:\n    owners:\n      groups:\n    - &lt;opssre-group-id&gt;\n  quota: small\nEOF\n</code></pre> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: tenant-b\nspec:\n  namespaces:\n    withTenantPrefix:\n    - dev\n    - build\n  accessControl:\n    owners:\n      groups:\n    - &lt;appdev-group-id&gt;\n  quota: small\nEOF\n</code></pre> <p>Notice that the only difference in both tenant specs are the groups.</p> <p>Check if the tenant namespaces have been created:</p> <pre><code>$ kubectl get namespaces\n\nNAME                    STATUS   AGE\ncert-manager            Active   4h26m\ndefault                 Active   5h25m\nkube-node-lease         Active   5h25m\nkube-public             Active   5h25m\nkube-system             Active   5h25m\nmulti-tenant-operator   Active   3h55m\ntenant-a-build          Active   10m\ntenant-a-dev            Active   10m\ntenant-b-build          Active   10m\ntenant-b-dev            Active   10m\n</code></pre> <p>Notice that MTO has created two namespaces under each tenant.</p>"},{"location":"installation/azure-aks.html#users-interaction-with-the-cluster","title":"Users Interaction with the Cluster","text":""},{"location":"installation/azure-aks.html#appdev-group","title":"AppDev group","text":"<p>AppDev is one of the previously created groups, its scope is limited to Tenant A namespaces as we mentioned its group ID in Tenant A. Start by clearing token of <code>test-admin-user</code>:</p> <pre><code>kubelogin remove-tokens\n</code></pre> <p>Use the <code>aksdev</code> user from <code>appdev</code> group to log in to the cluster:</p> <pre><code>kubectl get pods\n</code></pre> <p>This will take you to device login page. After entering the correct code, it will redirect you to Microsoft Login page, here you will enter the email and password of <code>aksdev</code> user created at the start of the article.</p> <p>After successful log in, it will show you the output of your kubectl command:</p> <pre><code>Error from server (Forbidden): pods is forbidden: User \"aksdev@company.com\" cannot list resource \"pods\" in API group \"\" in the namespace \"default\"\n</code></pre> <p>This user does not have access to default namespace.</p> <p>Now try accessing the resources in its tenant namespaces which are under Tenant A:</p> <pre><code>$ kubectl get pods -n tenant-a-dev\n\nNo resources found in tenant-a-dev namespace.\n</code></pre> <p>Create an <code>nginx</code> pod in the same namespace</p> <pre><code>$ kubectl run nginx --image=nginx -n tenant-a-dev\n\npod/nginx created\n</code></pre> <p>Now try the same operation in other namespace of Tenant B:</p> <pre><code>$ kubectl run nginx --image=nginx -n tenant-b-dev\n\nError from server (Forbidden): pods is forbidden: User \"aksdev@company.com\" cannot create resource \"pods\" in API group \"\" in the namespace \"tenant-b-dev\"\n</code></pre> <p>This operation fails with an error showing strict controls in their Tenants.</p>"},{"location":"installation/azure-aks.html#opssre-group","title":"OpsSre group","text":"<p>OpsSre is the second group created at the start of this article, its scope is limited to Tenant B namespaces as we mentioned its group ID in Tenant B.</p> <p>Start by clearing token of <code>appdev</code> user:</p> <pre><code>kubelogin remove-tokens\n</code></pre> <p>Use the <code>opssre</code> user from <code>opssre</code> group to log in to the cluster:</p> <pre><code>kubectl get pods\n</code></pre> <p>This will take you to device login page. After entering the correct code, it will redirect you to Microsoft Login page, here you will enter the email and password of <code>opssre</code> user created at the start of the article.</p> <p>After successful log in, it will show you the output of your kubectl command:</p> <pre><code>Error from server (Forbidden): pods is forbidden: User \"opssre@company.com\" cannot list resource \"pods\" in API group \"\" in the namespace \"default\"\n</code></pre> <p>This user does not have access to default namespace</p> <p>Now try accessing the resources in its tenant namespaces which are under Tenant B:</p> <pre><code>$ kubectl get pods -n tenant-b-dev\n\nNo resources found in tenant-b-dev namespace.\n</code></pre> <p>Create an <code>nginx</code> pod in the same namespace:</p> <pre><code>$ kubectl run nginx --image=nginx -n tenant-b-dev\n\npod/nginx created\n</code></pre> <p>Now try the same operation in other namespace of Tenant A:</p> <pre><code>$ kubectl run nginx --image=nginx -n tenant-a-dev\n\nError from server (Forbidden): pods is forbidden: User \"opssre@company.com\" cannot create resource \"pods\" in API group \"\" in the namespace \"tenant-a-dev\"\n</code></pre> <p>This operation fails with an error showing strict controls in their Tenants.</p>"},{"location":"installation/azure-aks.html#cleanup-resources","title":"Cleanup Resources","text":"<p>Cleanup the users, groups, AKS Cluster and Resource Group created for this blog. Run the following set of commands to remove resources created in above sections:</p> <pre><code># Delete the Azure AD user accounts for aksdev and akssre.\n\n$ az ad user delete --id $AKSDEV_ID\n$ az ad user delete --id $AKSSRE_ID\n\n# Delete the Azure AD groups for `appdev`,`opssre` and `mto-admins`. This also deletes the Azure role assignments.\n\n$ az ad group delete --group appdev\n$ az ad group delete --group opssre\n$ az ad group delete --group mto-admins\n\n# Delete the Resource Group which will also delete the underlying AKS test cluster and related resources\n\n$ az group delete --name myResourceGroup\n</code></pre>"},{"location":"installation/helm.html","title":"Helm Chart","text":"<p>Following options are available in the Helm Chart for Multi Tenant Operator:</p> <pre><code>platform: Kubernetes\n\n# bypassedGroups are the comma-separated names of Groups which are bypassed in Namespace and Rolebinding webhooks\nbypassedGroups: \"system:cluster-admins,system:masters\"\n\nreplicaCount: 1\n\noperator:\n  image:\n    repository: ghcr.io/stakater/public/tenant-operator\n    tag: v0.12.64\n    pullPolicy: IfNotPresent\n  serviceAccount:\n    # Annotations to add to the service account\n    annotations: {}\n    # The name of the service account to use.\n    # If not set and create is true, a name is generated using the fullname template\n    name: \"controller-manager\"\n\nimagePullSecrets: []\n\nnameOverride: \"\"\nfullnameOverride: \"\"\n\nwatchNamespaces: []\n\n# Webhook Configuration\nwebhook:\n  enabled: true\n  certificates:\n    create: true\n\ndeployment:\n  annotations:\n    # reloader.stakater.com/auto: \"true\"\n\nservice:\n  type: ClusterIP\n  port: 443\n\npodSecurityContext:\n  runAsNonRoot: true\n\nsecurityContext:\n  {}\n  # capabilities:\n  #   drop:\n  #   - ALL\n  # readOnlyRootFilesystem: true\n  # runAsNonRoot: true\n  # runAsUser: 1000\n\nresources:\n\n  limits:\n    cpu: 100m\n    memory: 2Gi\n  requests:\n    cpu: 10m\n    memory: 200Mi\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n\nintegrationConfig:\n  # create: false\n  accessControl:\n    privileged:\n      namespaces:\n        - ^default$\n        - ^openshift-.*\n        - ^stakater-.*\n        - ^kube-.*\n        - ^redhat-.*\n        - ^hive-.*\n      serviceAccounts:\n        - ^system:serviceaccount:openshift-.*\n        - ^system:serviceaccount:stakater-.*\n        - ^system:serviceaccount:kube-.*\n        - ^system:serviceaccount:redhat-.*\n        - ^system:serviceaccount:hive-.*\n      groups:\n        # - saap-cluster-admins\n  components:\n    console: false\n    showback: false\n\nuserRoles:\n  create: true\n\n# Extend tenant cluster manager role\nmanagerRoleExtendedRules:\n  {}\n  # - apiGroups:\n  #   - user.openshift.io\n  #   resources:\n  #   - groups\n  #   verbs:\n  #   - create\n  #   - delete\n  #   - get\n  #   - list\n  #   - patch\n  #   - update\n  #   - watch\n</code></pre>"},{"location":"installation/kubernetes.html","title":"On Kubernetes","text":"<p>This document contains instructions on installing, uninstalling and configuring Multi Tenant Operator on Kubernetes.</p> <ol> <li> <p>Installing via Helm CLI</p> </li> <li> <p>Uninstall</p> </li> </ol>"},{"location":"installation/kubernetes.html#requirements","title":"Requirements","text":"<ul> <li>A Kubernetes cluster (v1.24 or higher)</li> <li>Helm CLI</li> <li>kubectl</li> <li> <p>To run on Kubernetes, two certificates are needed in the operator namespace for the operator to be up and running, named</p> <ol> <li><code>quota-template-intconfig-server-cert</code> pointing to <code>multi-tenant-operator-quota-template-intconfig-webhook-service.{{ .Release.Namespace }}.svc.cluster.local</code></li> <li><code>webhook-server-cert</code> pointing to <code>multi-tenant-operator-webhook-service.{{ .Release.Namespace }}.svc.cluster.local</code></li> </ol> <p>If you are using Cert Manager, these certificates will be handled by templates in Helm Chart</p> </li> </ul>"},{"location":"installation/kubernetes.html#installing-via-helm-cli","title":"Installing via Helm CLI","text":"<ul> <li> <p>Public Helm Chart of MTO is available at Stakater ghcr Packages and available Helm options can be seen at MTO Helm Chart Options</p> </li> <li> <p>Use <code>helm install</code> command to install MTO helm chart. Here, <code>bypassedGroups</code> has the names of groups which are designated as Cluster Admins in your cluster. For this example, we will use <code>system:masters</code></p> </li> </ul> <pre><code>helm install tenant-operator oci://ghcr.io/stakater/public/charts/multi-tenant-operator --version 0.12.62 --namespace multi-tenant-operator --create-namespace --set bypassedGroups=system:masters'\n</code></pre> <p>Note</p> <p>It is better to install MTO in its preferred namespace, <code>multi-tenant-operator</code></p> <p>Wait for the pods to be up</p> <pre><code>kubectl get pods -n multi-tenant-operator --watch\n</code></pre> <p>After all pods come in running state, you can follow Tutorials.</p>"},{"location":"installation/kubernetes.html#enterprise-license-configuration","title":"Enterprise License Configuration","text":"<p>For the Enterprise version, you need to have a configmap <code>license</code> created in MTO's namespace <code>multi-tenant-operator</code>. You will get this configmap when purchasing the Enterprise version. It would look like this:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: license\n  namespace: multi-tenant-operator\ndata:\n  payload.json: |\n    {\n        \"metaData\": {\n            \"tier\" : \"paid\",\n            \"company\": \"&lt;company name here&gt;\"\n        }\n    }\n  signature.base64.txt: &lt;base64 signature here&gt;\n</code></pre>"},{"location":"installation/kubernetes.html#uninstall-via-helm-cli","title":"Uninstall via Helm CLI","text":"<p>MTO can be uninstalled by Helm CLI if Helm was used to install it earlier.</p> <ul> <li>Use <code>helm uninstall</code> command to remove the earlier created <code>Helm Release</code> in <code>multi-tenant-operator</code> namespace</li> </ul> <pre><code>helm uninstall tenant-operator --namespace multi-tenant-operator\n</code></pre>"},{"location":"installation/kubernetes.html#notes","title":"Notes","text":"<ul> <li>For details on licensing of MTO please refer Pricing.</li> <li>For more details on how to use MTO please refer Tenant tutorial.</li> <li>For details on how to extend your MTO manager ClusterRole please refer extend-default-clusterroles.</li> </ul>"},{"location":"installation/openshift.html","title":"On OpenShift","text":"<p>MTO is RedHat Certified operator available on the Red Hat MarketPlace.</p> <p>This document contains instructions on installing, uninstalling and configuring Multi Tenant Operator on OpenShift.</p> <ol> <li> <p>OpenShift OperatorHub UI</p> </li> <li> <p>CLI/GitOps</p> </li> <li> <p>Enabling Console</p> </li> <li> <p>Uninstall</p> </li> </ol>"},{"location":"installation/openshift.html#requirements","title":"Requirements","text":"<ul> <li>An OpenShift cluster [v4.8 - v4.18]</li> </ul>"},{"location":"installation/openshift.html#installing-via-operatorhub-ui","title":"Installing via OperatorHub UI","text":"<ul> <li>After opening OpenShift console click on <code>Operators</code>, followed by <code>OperatorHub</code> from the side menu</li> </ul> <ul> <li>Now search for <code>Multi Tenant Operator</code> and then click on <code>Multi Tenant Operator</code> tile</li> </ul> <ul> <li>Click on the <code>install</code> button</li> </ul> <ul> <li>Select <code>Updated channel</code>. Select <code>multi-tenant-operator</code> to install the operator in <code>multi-tenant-operator</code> namespace from <code>Installed Namespace</code> dropdown menu. After configuring <code>Update approval</code> click on the <code>install</code> button.</li> </ul> <p>Note: Use <code>stable</code> channel for seamless upgrades. For <code>Production Environment</code> prefer <code>Manual</code> approval and use <code>Automatic</code> for <code>Development Environment</code></p> <p></p> <ul> <li>Wait for the operator to be installed</li> </ul> <p></p> <ul> <li>Once successfully installed, MTO will be ready to enforce multi-tenancy in your cluster</li> </ul> <p></p> <p>Note: MTO will be installed in <code>multi-tenant-operator</code> namespace.</p>"},{"location":"installation/openshift.html#installing-via-cli-or-gitops","title":"Installing via CLI OR GitOps","text":"<ul> <li>Create namespace <code>multi-tenant-operator</code></li> </ul> <pre><code>oc create namespace multi-tenant-operator\nnamespace/multi-tenant-operator created\n</code></pre> <ul> <li>Create an OperatorGroup YAML for MTO and apply it in <code>multi-tenant-operator</code> namespace.</li> </ul> <pre><code>oc create -f - &lt;&lt; EOF\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: tenant-operator\n  namespace: multi-tenant-operator\nEOF\noperatorgroup.operators.coreos.com/tenant-operator created\n</code></pre> <ul> <li>Create a subscription YAML for MTO and apply it in <code>multi-tenant-operator</code> namespace. To enable console set <code>.spec.config.env[].ENABLE_CONSOLE</code> to <code>true</code>. This will create a route resource, which can be used to access the Multi-Tenant-Operator console.</li> </ul> <pre><code>oc create -f - &lt;&lt; EOF\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: tenant-operator\n  namespace: multi-tenant-operator\nspec:\n  channel: stable\n  installPlanApproval: Automatic\n  name: tenant-operator\n  source: certified-operators\n  sourceNamespace: openshift-marketplace\n  startingCSV: tenant-operator.v0.10.0\nEOF\nsubscription.operators.coreos.com/tenant-operator created\n</code></pre> <p>Note: To bring MTO via GitOps, add the above files in GitOps repository.</p> <ul> <li>After creating the <code>subscription</code> custom resource open OpenShift console and click on <code>Operators</code>, followed by <code>Installed Operators</code> from the side menu</li> </ul> <p></p> <ul> <li>Wait for the installation to complete</li> </ul> <p></p> <ul> <li>Once the installation is complete click on <code>Workloads</code>, followed by <code>Pods</code> from the side menu and select <code>multi-tenant-operator</code> project</li> </ul> <p></p> <ul> <li>Once pods are up and running, MTO will be ready to enforce multi-tenancy in your cluster</li> </ul> <p></p> <p>For more details and configurations check out IntegrationConfig.</p>"},{"location":"installation/openshift.html#enabling-console","title":"Enabling Console","text":"<p>To enable console GUI for MTO, go to <code>Search</code> -&gt; <code>IntegrationConfig</code> -&gt; <code>tenant-operator-config</code> and make sure the following fields are set to <code>true</code>:</p> <pre><code>spec:\n  components:\n    console: true\n    showback: true\n</code></pre> <p>Note: If your <code>InstallPlan</code> approval is set to <code>Manual</code> then you will have to manually approve the <code>InstallPlan</code> for MTO console components to be installed.</p>"},{"location":"installation/openshift.html#manual-approval","title":"Manual Approval","text":"<ul> <li>Open OpenShift console and click on <code>Operators</code>, followed by <code>Installed Operators</code> from the side menu.</li> </ul> <ul> <li>Now click on <code>Upgrade available</code> in front of <code>mto-opencost</code> or <code>mto-prometheus</code>.</li> </ul> <ul> <li>Now click on <code>Preview InstallPlan</code> on top.</li> </ul> <ul> <li>Now click on <code>Approve</code> button.</li> </ul> <ul> <li>Now the <code>InstallPlan</code> will be approved, and MTO console components will be installed.</li> </ul>"},{"location":"installation/openshift.html#uninstall-via-operatorhub-ui","title":"Uninstall via OperatorHub UI","text":"<p>You can uninstall MTO by following these steps:</p> <ul> <li> <p>Decide on whether you want to retain tenant namespaces and ArgoCD AppProjects or not. If yes, please set <code>spec.onDelete.cleanNamespaces</code> to <code>false</code> for all those tenants whose namespaces you want to retain, and <code>spec.onDelete.cleanAppProject</code> to <code>false</code> for all those tenants whose AppProject you want to retain. For more details check out onDelete</p> </li> <li> <p>After making the required changes open OpenShift console and click on <code>Operators</code>, followed by <code>Installed Operators</code> from the side menu</p> </li> </ul> <p></p> <ul> <li>Now click on uninstall and confirm uninstall.</li> </ul> <p></p> <ul> <li> <p>Now the operator has been uninstalled.</p> </li> <li> <p><code>Optional:</code> you can also manually remove MTO's CRDs and its resources from the cluster.</p> </li> </ul>"},{"location":"installation/openshift.html#notes","title":"Notes","text":"<ul> <li>For details on licensing of MTO please refer Pricing.</li> <li>For more details on how to use MTO please refer Tenant tutorial.</li> <li>For details on how to extend your MTO manager ClusterRole please refer extend-default-clusterroles.</li> </ul>"},{"location":"installation/overview.html","title":"Overview","text":"<p>The Multi-Tenant Operator (MTO) supports two installation methods: Operator Lifecycle Manager (OLM) and Helm Chart. These methods ensure flexibility and compatibility across various Kubernetes environments, including OpenShift, Azure Kubernetes Service (AKS), Amazon Elastic Kubernetes Service (EKS), and others.</p>"},{"location":"installation/overview.html#installation-methods","title":"Installation Methods","text":""},{"location":"installation/overview.html#1-operator-lifecycle-manager-olm","title":"1. Operator Lifecycle Manager (OLM)","text":"<p>OLM is the recommended installation method for OpenShift. It leverages OpenShift's native operator management capabilities, ensuring seamless integration and lifecycle management.</p>"},{"location":"installation/overview.html#2-helm-chart","title":"2. Helm Chart","text":"<p>Helm is the preferred installation method for all other Kubernetes distributions, including AKS, EKS, GKE and generic Kubernetes environments. Helm simplifies the deployment process with its consistent and automated approach.</p>"},{"location":"installation/overview.html#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure the following:</p> <ul> <li>Access to an OpenShift cluster (for OLM installation) or a Kubernetes cluster (for Helm installation).</li> <li>Administrator-level permissions on the cluster.</li> <li>Familiarity with kubectl or platform-specific CLI tools.</li> <li>Helm installed locally for Helm-based installations.</li> </ul>"},{"location":"installation/overview.html#next-steps","title":"Next Steps","text":"<p>Choose the installation guide that matches your environment:</p> <ul> <li>Installing with OLM on OpenShift</li> <li>Installing with Helm on AKS</li> <li>Installing with Helm on EKS</li> <li>Installing with Helm on any Kubernetes</li> </ul> <p>By following the appropriate guide, you\u2019ll be able to deploy MTO efficiently and start managing multi-tenancy in your Kubernetes environment.</p>"},{"location":"installation/uninstalling.html","title":"Uninstall via OperatorHub UI on OpenShift","text":"<p>You can uninstall MTO by following these steps:</p> <ul> <li> <p>Decide on whether you want to retain tenant namespaces and ArgoCD AppProjects or not. For more details check out onDeletePurgeNamespaces onDeletePurgeAppProject</p> </li> <li> <p>In case you have enabled console and showback, you will have to disable it first by navigating to <code>Search</code> -&gt; <code>IntegrationConfig</code> -&gt; <code>tenant-operator-config</code> and set <code>spec.components.console</code> and <code>spec.components.showback</code> to <code>false</code>.</p> </li> <li> <p>Remove IntegrationConfig CR from the cluster by navigating to <code>Search</code> -&gt; <code>IntegrationConfig</code> -&gt; <code>tenant-operator-config</code> and select <code>Delete</code> from actions dropdown.</p> </li> <li> <p>After making the required changes open OpenShift console and click on <code>Operators</code>, followed by <code>Installed Operators</code> from the side menu</p> </li> </ul> <p></p> <ul> <li>Now click on uninstall and confirm uninstall.</li> </ul> <p></p> <ul> <li> <p>Now the operator has been uninstalled.</p> </li> <li> <p><code>Optional:</code> you can also manually remove MTO's CRDs and its resources from the cluster.</p> </li> </ul>"},{"location":"installation/uninstalling.html#notes","title":"Notes","text":"<ul> <li>For more details on how to use MTO please refer Tenant's tutorial.</li> <li>For more details on how to extend your MTO manager ClusterRole please refer extend-default-clusterroles.</li> </ul>"},{"location":"installation/aws-eks/installation.html","title":"MTO Installation Guide","text":"<p>Once the necessary preparations are complete, you can proceed with the installation section.</p> <p>The installation process consists of two steps:</p> <ol> <li>Install MTO Core</li> <li>Enable MTO Console</li> </ol>"},{"location":"installation/aws-eks/installation.html#install-mto-core","title":"Install MTO Core","text":"<p>We will be using helm to install the operator.</p> <pre><code>helm install tenant-operator oci://ghcr.io/stakater/public/charts/multi-tenant-operator --version 1.1.0 --namespace multi-tenant-operator --create-namespace\n</code></pre> <p>We will wait for the pods to come in running state.</p> <pre><code>NAME                                                              READY   STATUS    RESTARTS   AGE\ntenant-operator-namespace-controller-768f9459c4-758kb             2/2     Running   0          5m\ntenant-operator-pilot-controller-7c96f6589c-d979f                 2/2     Running   0          5m\ntenant-operator-resourcesupervisor-controller-566f59d57b-xbkws    2/2     Running   0          5m\ntenant-operator-template-quota-intconfig-controller-7fc99462dz6   2/2     Running   0          5m\ntenant-operator-templategroupinstance-controller-75cf68c872pljv   2/2     Running   0          5m\ntenant-operator-templateinstance-controller-d996b6fd-cx2dz        2/2     Running   0          5m\ntenant-operator-tenant-controller-57fb885c84-7ps92                2/2     Running   0          5m\ntenant-operator-webhook-5f8f675549-jv9n8                          2/2     Running   0          5m\n</code></pre>"},{"location":"installation/aws-eks/installation.html#enable-mto-console","title":"Enable MTO Console","text":"<p>Execute the following command to enable MTO console</p> <pre><code>kubectl patch integrationconfig tenant-operator-config \\\n  -n multi-tenant-operator --type merge --patch \"{\n  \\\"spec\\\": {\n    \\\"components\\\": {\n      \\\"console\\\": true,\n      \\\"ingress\\\": {\n        \\\"console\\\": {\n          \\\"host\\\": \\\"console.&lt;FULL_SUBDOMAIN&gt;\\\",\n          \\\"tlsSecretName\\\": \\\"&lt;SECRET_NAME&gt;\\\"\n        },\n        \\\"gateway\\\": {\n          \\\"host\\\": \\\"gateway.&lt;FULL_SUBDOMAIN&gt;\\\",\n          \\\"tlsSecretName\\\": \\\"&lt;SECRET_NAME&gt;\\\"\n        },\n        \\\"keycloak\\\": {\n          \\\"host\\\": \\\"keycloak.&lt;FULL_SUBDOMAIN&gt;\\\",\n          \\\"tlsSecretName\\\": \\\"&lt;SECRET_NAME&gt;\\\"\n        },\n        \\\"ingressClassName\\\": \\\"nginx\\\"\n      },\n      \\\"showback\\\": true\n    }\n  }\n}\"\n</code></pre> Placeholder Description <code>&lt;FULL_SUBDOMAIN&gt;</code> Full subdomain of the EKS cluster e.g. <code>iinhdnh6.demo.kubeapp.cloud</code> <code>&lt;SECRET_NAME&gt;</code> Name of the secret that should be used as TLS secret <p>Wait for the pods to be ready with the following command</p> <pre><code>kubectl wait --for=condition=ready pod -n multi-tenant-operator --all --timeout=300s\n</code></pre> <p>List the ingresses to access the URL of MTO Console</p> <pre><code>&gt; kubectl get ingress -n multi-tenant-operator\n\nNAME                       CLASS   HOSTS                                  ADDRESS                                                                          PORTS     AGE\ntenant-operator-console    nginx   console.iinhdnh6.demo.kubeapp.cloud    ae51c179026a94c90952fc50d5d91b52-a4446376b6415dcb.elb.eu-north-1.amazonaws.com   80, 443   23m\ntenant-operator-gateway    nginx   gateway.iinhdnh6.demo.kubeapp.cloud    ae51c179026a94c90952fc50d5d91b52-a4446376b6415dcb.elb.eu-north-1.amazonaws.com   80, 443   23m\ntenant-operator-keycloak   nginx   keycloak.iinhdnh6.demo.kubeapp.cloud   ae51c179026a94c90952fc50d5d91b52-a4446376b6415dcb.elb.eu-north-1.amazonaws.com   80, 443   24m\n</code></pre>"},{"location":"installation/aws-eks/installation.html#mto-console-admin-login","title":"MTO Console Admin Login","text":"<p>Patch the following integration config to give privileged access to MTO's default admin user</p> <pre><code>kubectl patch integrationconfigs.tenantoperator.stakater.com -n multi-tenant-operator tenant-operator-config --type=merge --patch \"{\n    \\\"spec\\\": {\n        \\\"accessControl\\\": {\n            \\\"privileged\\\": {\n                \\\"users\\\": [\n                    \\\"mto@stakater.com\\\"\n                ]\n            }\n        }\n    }\n}\"\n</code></pre> <p>Open the Console URL and Log In with the admin user. Default username and password is <code>mto</code></p> <p></p> <p>Dashboard will open after the successful login. Currently we don't have any tenants</p> <p></p>"},{"location":"installation/aws-eks/installation.html#whats-next","title":"What's Next","text":"<p>Now lets create our first tenant on EKS.</p>"},{"location":"installation/aws-eks/preparation.html","title":"MTO Preparation Guide","text":"<p>This document provides a detailed walk through of the preparation steps required for MTO installation.</p>"},{"location":"installation/aws-eks/preparation.html#cluster-requirements","title":"Cluster Requirements","text":"<p>An EKS 1.28+ cluster with the following components:</p> <ul> <li>A default storage class</li> <li>An Ingress controller</li> <li>Valid certificates for MTO Gateway, MTO Console, and MTO Keycloak</li> </ul> <p>DNS entries configured for MTO Gateway, MTO Console, and MTO Keycloak</p> <p>A user with cluster administrator privileges.</p>"},{"location":"installation/aws-eks/preparation.html#local-setup-requirements","title":"Local Setup Requirements","text":"<p>The installation machine must have:</p> <ul> <li>Helm installed</li> <li>kubectl installed</li> </ul> <p>Once these preparations are complete, you can proceed with the installation section.</p>"},{"location":"installation/aws-eks/preparation.html#optional-creating-and-configuring-an-eks-cluster","title":"Optional: Creating and Configuring an EKS Cluster","text":"<p>If you already have an EKS 1.28+ cluster that meets the Cluster Requirements you can skip this section and proceed directly to the Installation Guide.</p> <p>However, if you don\u2019t have an existing cluster, follow this step-by-step guide to create and configure one properly for MTO installation.</p>"},{"location":"installation/aws-eks/preparation.html#create-eks-cluster","title":"Create EKS Cluster","text":"<p>Execute the following snippet in your terminal with appropriate values. This snippet will configure the AWS Credentials. See Manage access keys for IAM users for more details about access keys</p> <pre><code>aws configure set region &lt;AWS_REGION&gt;\naws configure set aws_access_key_id &lt;AWS_ACCESS_KEY_ID&gt;\naws configure set aws_secret_access_key &lt;AWS_SECRET_ACCESS_KEY&gt;\n</code></pre> <p>Use the following command to create an EKS cluster if it doesn't exist</p> <pre><code>eksctl create cluster \\\n    --name &lt;CLUSTER_NAME&gt; \\\n    --region &lt;AWS_REGION&gt; \\\n    --nodegroup-name standard-workers \\\n    --node-type t3.xlarge \\\n    --nodes 1 \\\n    --nodes-min 1 \\\n    --nodes-max 3 \\\n    --managed\n</code></pre> <p>Set the kubernetes context to the specified cluster.</p> <pre><code># Update the current context\naws eks update-kubeconfig --name &lt;CLUSTER_NAME&gt; --region &lt;AWS_REGION&gt;\n</code></pre>"},{"location":"installation/aws-eks/preparation.html#install-ingress-controller","title":"Install Ingress Controller","text":"<p>To install nginx Ingress Controller, run the following command:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/aws/deploy.yaml\n</code></pre>"},{"location":"installation/aws-eks/preparation.html#install-certmanager","title":"Install Certmanager","text":"<p>To install Cert-Manager, execute the following command:</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.1/cert-manager.yaml\n</code></pre>"},{"location":"installation/aws-eks/preparation.html#create-lets-encrypt-access-key-secret","title":"Create Let's Encrypt Access Key Secret","text":"<p>To configure Let's Encrypt with AWS, create the Kubernetes Secret using the following command:</p> <pre><code>kubectl create secret generic letsencrypt-production-key \\\n  --namespace cert-manager \\\n  --from-literal=aws_secret_access_key=&lt;AWS_SECRET_ACCESS_KEY&gt;\n</code></pre> <p>Replace  with your actual AWS secret key."},{"location":"installation/aws-eks/preparation.html#create-clusterissuer-for-lets-encrypt","title":"Create <code>ClusterIssuer</code> for Let's Encrypt","text":"<p>To enable automatic SSL certificate issuance using Let\u2019s Encrypt, you need to create a ClusterIssuer resource in Kubernetes. This issuer will use Route 53 DNS-01 challenge to validate domain ownership and issue certificates.</p> <ol> <li> <p>Create a YAML file named <code>letsencrypt-clusterissuer.yaml</code> for the ClusterIssuer with the following content. Replace the placeholders with your actual values:</p> <ul> <li><code>&lt;AWS_ACCESS_KEY_ID&gt;</code>: Your AWS Access Key ID.</li> <li><code>&lt;REGION&gt;</code>: The AWS region where your Route 53 hosted zone is located (e.g., us-east-1).</li> <li><code>&lt;BASE_DOMAIN&gt;</code>: Your base domain (e.g., example.com).</li> <li><code>&lt;EMAIL&gt;</code>: Your email address for Let's Encrypt notifications.</li> </ul> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-production\nspec:\n  acme:\n    email: &lt;EMAIL&gt;  # Replace with your email address\n    preferredChain: ISRG Root X1\n    privateKeySecretRef:\n      name: issuer-account-key\n    server: 'https://acme-v02.api.letsencrypt.org/directory'\n    solvers:\n      - dns01:\n          route53:\n            accessKeyID: &lt;AWS_ACCESS_KEY_ID&gt;  # Replace with your AWS Access Key ID\n            region: &lt;REGION&gt;  # Replace with your AWS region\n            secretAccessKeySecretRef:\n              key: aws_secret_access_key\n              name: letsencrypt-production-key\n        selector:\n          dnsZones:\n            - &lt;BASE_DOMAIN&gt;  # Replace with your base domain\n</code></pre> </li> <li> <p>Apply the YAML file to your cluster:</p> <pre><code>kubectl apply -f letsencrypt-clusterissuer.yaml\n</code></pre> </li> <li> <p>Run the following command to ensure the ClusterIssuer is ready:</p> <pre><code>kubectl get clusterissuer letsencrypt-production\n</code></pre> </li> </ol>"},{"location":"installation/aws-eks/preparation.html#install-aws-ebs-csi-driver","title":"Install AWS EBS CSI Driver","text":"<p>The Amazon EBS CSI Driver enables Kubernetes to manage Amazon Elastic Block Store (EBS) volumes, handling their lifecycle as persistent storage for workloads.</p> <p>To install the AWS EBS CSI Driver, execute the following command:</p> <pre><code>kubectl apply -k \"github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.38\"\n</code></pre> <p>Once installed, verify that the driver is running by checking the pods in the kube-system namespace:</p> <pre><code>kubectl get pods -n kube-system | grep ebs\n</code></pre>"},{"location":"installation/aws-eks/preparation.html#create-storage-class-for-ebs","title":"Create Storage Class for EBS","text":"<ol> <li> <p>Create a file named <code>storage-class.yaml</code> and add the following content:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\nmetadata:\n  name: ebs-sc\nprovisioner: ebs.csi.aws.com\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  type: gp3\n</code></pre> </li> <li> <p>Apply the configuration using the following command:</p> <pre><code>kubectl apply -f storage-class.yaml\n</code></pre> </li> <li> <p>Set the storage class as default storage class using the following command</p> <pre><code>kubectl patch storageclass ebs-sc -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n</code></pre> </li> <li> <p>Verify that the StorageClass has been created:</p> <pre><code>kubectl get storageclass\n</code></pre> </li> </ol>"},{"location":"installation/aws-eks/preparation.html#create-wildcard-dns-record","title":"Create Wildcard DNS Record","text":"<p>To ensure proper routing for applications, you need to create a wildcard DNS record that points to the nginx Ingress Controller\u2019s external IP. Follow these steps:</p> <ol> <li> <p>Retrieve the Ingress Controller's External IP     Run the following command to get the external IP of the nginx Ingress Controller:</p> <pre><code>kubectl get svc ingress-nginx-controller -n ingress-nginx -o jsonpath=\"{.status.loadBalancer.ingress[0].hostname}\"\n</code></pre> <p>If the hostname is not available, wait for a few minutes and re-run the command.</p> </li> <li> <p>Retrieve Hosted Zone and Load Balancer IDs     Run the following commands to retrieve the Route 53 Hosted Zone ID and Load Balancer Hosted Zone ID:</p> <pre><code># Retrieve Hosted Zone ID\nHOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name &lt;BASE_DOMAIN&gt; --query \"HostedZones[0].Id\" --output text | cut -d '/' -f3)\n\n# Retrieve Hosted Zone Load Balancer ID\nHOSTED_ZONE_LB_ID=$(aws elbv2 describe-load-balancers --query \"LoadBalancers[0].CanonicalHostedZoneId\" --output text)\n</code></pre> </li> <li> <p>Create a JSON File for the DNS Update     Create a file named <code>change-batch.json</code> and update the placeholders:</p> <ul> <li><code>&lt;FULL_SUBDOMAIN&gt;</code> \u2013 Your subdomain (e.g., apps.example.com)</li> <li><code>&lt;HOSTED_ZONE_LB_ID&gt;</code> \u2013 Hosted Zone Load Balancer ID from Step 2</li> <li><code>&lt;EXTERNAL_IP&gt;</code> \u2013 External IP retrieved in Step 1</li> </ul> <pre><code>{\n  \"Comment\": \"Update wildcard DNS record to point to NGINX Ingress ELB\",\n  \"Changes\": [\n    {\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"*.&lt;FULL_SUBDOMAIN&gt;\",\n        \"Type\": \"A\",\n        \"AliasTarget\": {\n          \"HostedZoneId\": \"&lt;HOSTED_ZONE_LB_ID&gt;\",\n          \"DNSName\": \"&lt;EXTERNAL_IP&gt;\",\n          \"EvaluateTargetHealth\": false\n        }\n      }\n    }\n  ]\n}\n</code></pre> </li> <li> <p>Update the DNS Record in Route 53 by using the following command</p> <pre><code>aws route53 change-resource-record-sets --hosted-zone-id $HOSTED_ZONE_ID --change-batch file://change-batch.json\n</code></pre> </li> <li> <p>Verify DNS Configuration     Check if the DNS record has been propagated correctly:</p> <pre><code>nslookup &lt;YOUR_DOMAIN&gt;\n</code></pre> <p>Once the record is updated, traffic will be properly routed to the nginx Ingress Controller.</p> </li> </ol>"},{"location":"installation/aws-eks/preparation.html#create-wildcard-certificate","title":"Create Wildcard Certificate","text":"<p>A wildcard certificate allows all applications under a given subdomain to use a single SSL certificate. MTO will use this certificate to secure the MTO Console, MTO Gateway, and MTO Keycloak.</p> <ol> <li> <p>Apply the Wildcard Certificate Configuration</p> <p>Create a file named <code>wildcard-certificate.yaml</code> and replace the placeholders with the appropriate values:</p> <ul> <li><code>&lt;CERTIFICATE_NAME&gt;</code> - Name of the certificate to be generated.</li> <li><code>&lt;FULL_SUBDOMAIN&gt;</code> - DNS subdomain for which the wildcard certificate will be issued.</li> <li><code>&lt;NAMESPACE&gt;</code> - Namespace where the certificate and secret will be created. If MTO is installed in a different namespace, manually copy the generated secret.</li> <li><code>&lt;CERTIFICATE_SECRET_NAME&gt;</code> - Name of the secret that will store the generated certificate. This secret will be used in MTO\u2019s configuration to enable SSL for MTO components.</li> </ul> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: &lt;CERTIFICATE_NAME&gt;\n  namespace: &lt;NAMESPACE&gt;\nspec:\n  secretName: &lt;CERTIFICATE_SECRET_NAME&gt;\n  issuerRef:\n    name: letsencrypt-production\n    kind: ClusterIssuer\n  commonName: *.&lt;FULL_SUBDOMAIN&gt;\n  dnsNames:\n  - *.&lt;FULL_SUBDOMAIN&gt;\n</code></pre> </li> <li> <p>Apply the certificate configuration using the following command:</p> <pre><code>kubectl apply -f wildcard-certificate.yaml\n</code></pre> </li> <li> <p>Verify Certificate Creation</p> <p>After applying the configuration, check if the certificate has been issued successfully:</p> <pre><code>kubectl wait --for=condition=Ready certificate/&lt;CERTIFICATE_NAME&gt; -n &lt;NAMESPACE&gt; --timeout=300s\n</code></pre> </li> </ol>"},{"location":"installation/aws-eks/preparation.html#whats-next","title":"What's Next?","text":"<p>All the required components have been installed and configured. Now MTO can be installed on the EKS Cluster. See EKS MTO Installation Guide</p>"},{"location":"installation/aws-eks/validation.html","title":"MTO Validation Guide","text":"<p>In this guide, we will set up two tenants\u2014Logistics and Retail\u2014for an imaginary e-commerce company, each with one user.</p> <ul> <li>Falcon will be the user assigned to the Logistics tenant.  </li> <li>Bear will be the user assigned to the Retail tenant.</li> </ul>"},{"location":"installation/aws-eks/validation.html#1-create-configure-aws-iam-users-groups","title":"1. Create &amp; Configure AWS IAM Users &amp; Groups","text":""},{"location":"installation/aws-eks/validation.html#11-create-a-user","title":"1.1. Create a user","text":"<p>Create a user with username <code>falcon@nordmart.com</code></p> <pre><code>$ aws iam create-user --user-name falcon@nordmart.com\n\nOutput:\n{\n    \"User\": {\n        \"Path\": \"/\",\n        \"UserName\": \"falcon@nordmart.com\",\n        \"UserId\": \"AIDAZFWZTAEJ7ILHDKLLD\",\n        \"Arn\": \"arn:aws:iam::630742778131:user/falcon@nordmart.com\",\n        \"CreateDate\": \"2025-02-03T13:09:51Z\"\n    }\n}\n</code></pre>"},{"location":"installation/aws-eks/validation.html#12-attach-cluster-access-policy-to-user","title":"1.2. Attach cluster access policy to user","text":"<p>Create a AWS JSON policy file. This policy will allow the user to access the cluster.</p> <pre><code>{\n    \"Statement\": [\n        {\n            \"Action\": \"eks:DescribeCluster\",\n            \"Effect\": \"Allow\",\n            \"Resource\": \"*\"\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n</code></pre> <p>Attach a policy to user by running the following command</p> <pre><code>aws iam put-user-policy --user-name falcon@nordmart.com --policy-document file://policy.json --policy-name ClusterAccess\n</code></pre>"},{"location":"installation/aws-eks/validation.html#13-generate-access-key-for-the-user","title":"1.3. Generate access key for the user","text":"<p>Executing the following command will provide the Access Key Id and Access Secret Key Id that can be used to log in later</p> <pre><code>aws iam create-access-key --user-name \"falcon@nordmart.com\"\n</code></pre>"},{"location":"installation/aws-eks/validation.html#14-grant-user-access-to-kubernetes-via-configmap","title":"1.4. Grant user access to Kubernetes via <code>ConfigMap</code>","text":"<p>Use the following command to map this user in <code>aws-auth</code> configmap in <code>kube-system</code> namespace.</p> <pre><code>eksctl create iamidentitymapping --cluster \"&lt;CLUSTER_NAME&gt;\" \\\n                                 --region \"&lt;AWS_REGION&gt;\" \\\n                                 --arn \"&lt;USER_ARN&gt;\" \\\n                                 --username \"falcon@nordmart.com\" \\\n                                 --no-duplicate-arns\n</code></pre> <p>Repeat the same steps to create another user <code>bear@nordmart.com</code> for retail tenant.</p>"},{"location":"installation/aws-eks/validation.html#2-create-keycloak-user-for-mto-console","title":"2. Create Keycloak user for MTO Console","text":""},{"location":"installation/aws-eks/validation.html#21-create-keycloak-user","title":"2.1. Create Keycloak User","text":"<p>A Keycloak user with same username as IAM user needs to be created for MTO Console. In this section we will create a Keycloak user for Logistics tenant</p> <p>Ensure that MTO Console is enabled by executing the following command</p> <pre><code>$ kubectl get integrationconfig tenant-operator-config -o=jsonpath='{.spec.components}' -n multi-tenant-operator\n{\"console\":true,\"showback\":true}\n</code></pre> <p>List the ingresses to access the URL of MTO Console</p> <pre><code>kubectl get ingress -n multi-tenant-operator\n\nNAME                       CLASS   HOSTS                                  ADDRESS                                                                          PORTS     AGE\ntenant-operator-console    nginx   console.iinhdnh6.demo.kubeapp.cloud    ae51c179026a94c90952fc50d5d91b52-a4446376b6415dcb.elb.eu-north-1.amazonaws.com   80, 443   23m\ntenant-operator-gateway    nginx   gateway.iinhdnh6.demo.kubeapp.cloud    ae51c179026a94c90952fc50d5d91b52-a4446376b6415dcb.elb.eu-north-1.amazonaws.com   80, 443   23m\ntenant-operator-keycloak   nginx   keycloak.iinhdnh6.demo.kubeapp.cloud   ae51c179026a94c90952fc50d5d91b52-a4446376b6415dcb.elb.eu-north-1.amazonaws.com   80, 443   24m\n</code></pre> <ol> <li> <p>Navigate to Keycloak and Login using default credentials <code>admin/admin</code></p> </li> <li> <p>Change the Realm from <code>master</code> to <code>mto</code></p> </li> <li> <p>Navigate to Users and Click Add User</p> </li> <li> <p>Provide a username, this username must be same as IAM username <code>falcon@nordmart.com</code> in our case</p> <p></p> </li> <li> <p>Navigate to Credentials tab and set a password</p> <p></p> </li> </ol> <p>Repeat the same steps to create another user <code>bear@nordmart.com</code></p>"},{"location":"installation/aws-eks/validation.html#3-create-mto-quota","title":"3. Create MTO Quota","text":"<p>As cluster admin create a <code>Quota CR</code> with some resource limits:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: tenantoperator.stakater.com/v1beta1\nkind: Quota\nmetadata:\n  name: small\nspec:\n  limitrange:\n    limits:\n    - max:\n        cpu: 800m\n      min:\n        cpu: 200m\n      type: Container\n  resourcequota:\n    hard:\n      configmaps: \"10\"\n      memory: \"8Gi\"\nEOF\n</code></pre>"},{"location":"installation/aws-eks/validation.html#4-create-mto-tenants","title":"4. Create MTO Tenants","text":"<p>As cluster admin create 2 tenants <code>logistics</code> and <code>retail</code> with one user each:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: logistics\nspec:\n  namespaces:\n    withTenantPrefix:\n    - dev\n    - build\n  accessControl:\n    owners:\n      users:\n      - falcon@nordmart.com\n  quota: small\nEOF\n</code></pre> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: retail\nspec:\n  namespaces:\n    withTenantPrefix:\n    - dev\n    - build\n  accessControl:\n    owners:\n      users:\n      - bear@nordmart.com\n  quota: small\nEOF\n</code></pre> <p>Notice that the only difference in both tenant specs are the users.</p>"},{"location":"installation/aws-eks/validation.html#5-list-namespaces-as-cluster-admin","title":"5. List namespaces as cluster admin","text":"<p>Listing the namespaces as cluster admin will show following namespaces:</p> <pre><code>$ kubectl get namespaces\n\nNAME                    STATUS   AGE\ncert-manager            Active   8d\ndefault                 Active   9d\nkube-node-lease         Active   9d\nkube-public             Active   9d\nkube-system             Active   9d\nmulti-tenant-operator   Active   8d\nrandom                  Active   8d\nlogistics-dev           Active   5s\nlogistics-build         Active   5s\nretail-dev              Active   5s\nretail-build            Active   5s\n</code></pre>"},{"location":"installation/aws-eks/validation.html#6-validate-falcon-permissions","title":"6. Validate Falcon permissions","text":""},{"location":"installation/aws-eks/validation.html#61-switch-to-falcon","title":"6.1. Switch to falcon","text":"<p>Set the following environment variables from the access keys generated in previous steps</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>AWS_REGION</code> (optional)</li> </ul> <p>Execute the following command to update the kube context</p> <pre><code>aws configure set region $AWS_REGION\naws configure set aws_access_key_id $AWS_ACCESS_KEY_ID\naws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY\n\naws eks update-kubeconfig --name &lt;EKS_CLUSTER_NAME&gt; --region $AWS_REGION\n</code></pre>"},{"location":"installation/aws-eks/validation.html#62-check-cli-permissions","title":"6.2. Check CLI permissions","text":"<p>We will now try to deploy a pod from user <code>falcon@nordmart.com</code> in its tenant namespace <code>logistics-dev</code></p> <pre><code>$ kubectl run nginx --image nginx -n logistics-dev\n\npod/nginx created\n</code></pre> <p>And if we try the same operation in the other tenant with the same user, it will fail</p> <pre><code>$ kubectl run nginx --image nginx -n retail-dev\n\nError from server (Forbidden): pods is forbidden: User \"falcon@nordmart.com\" cannot create resource \"pods\" in API group \"\" in the namespace \"retail-dev\"\n</code></pre> <p>To be noted, <code>falcon@nordmart.com</code> can not list namespaces</p> <pre><code>$ kubectl get namespaces\n\nError from server (Forbidden): namespaces is forbidden: User \"falcon@nordmart.com\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope\n</code></pre>"},{"location":"installation/aws-eks/validation.html#63-validate-console-permissions","title":"6.3. Validate Console permissions","text":"<p>Navigate to MTO Console URL and Log In with the Keycloak user credentials.</p> <p></p> <p>Dashboard will open after the successful login. Now you can navigate different tenants and namespaces using MTO Console</p> <p></p>"},{"location":"installation/aws-eks/validation.html#7-validate-bear-permissions","title":"7. Validate Bear permissions","text":""},{"location":"installation/aws-eks/validation.html#71-switch-to-bear","title":"7.1. Switch to bear","text":"<p>Set the following environment variables from the access keys generated in previous steps</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>AWS_REGION</code> (optional)</li> </ul> <p>Execute the following command to update the kube context</p> <pre><code>aws configure set region $AWS_REGION\naws configure set aws_access_key_id $AWS_ACCESS_KEY_ID\naws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY\n\naws eks update-kubeconfig --name &lt;EKS_CLUSTER_NAME&gt; --region $AWS_REGION\n</code></pre>"},{"location":"installation/aws-eks/validation.html#72-check-cli-permissions","title":"7.2. Check CLI permissions","text":"<p>We will repeat the above operations for our retail user <code>bear@nordmart.com</code> as well</p> <pre><code>$ kubectl run nginx --image nginx -n retail-dev\n\npod/nginx created\n</code></pre> <p>Trying to do operations outside the scope of its own tenant will result in errors</p> <pre><code>$ kubectl run nginx --image nginx -n retail-dev\n\nError from server (Forbidden): pods is forbidden: User \"bear@nordmart.com\" cannot create resource \"pods\" in API group \"\" in the namespace \"retail-dev\"\n</code></pre> <p>To be noted, <code>bear@nordmart.com</code> can not list namespaces</p> <pre><code>$ kubectl get namespaces\n\nError from server (Forbidden): namespaces is forbidden: User \"bear@nordmart.com\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope\n</code></pre>"},{"location":"installation/aws-eks/validation.html#73-validate-console-permissions","title":"7.3. Validate Console permissions","text":"<p>Navigate to MTO Console URL and Log In with the Keycloak user credentials.</p> <p></p> <p>Dashboard will open after the successful login. Now you can navigate different tenants and namespaces using MTO Console</p> <p></p>"},{"location":"integrations/argocd.html","title":"ArgoCD","text":"<p>With the Multi-Tenant Operator (MTO), cluster administrators can configure multi-tenancy within their cluster. The integration of ArgoCD with MTO allows for the configuration of multi-tenancy in ArgoCD applications and AppProjects.</p> <p>MTO can be configured to create AppProjects for each tenant. These AppProjects enable tenants to create ArgoCD Applications that can be synced to namespaces owned by them. Cluster admins can blacklist certain namespace resources and allow specific cluster-scoped resources as needed (see the NamespaceResourceBlacklist and ClusterResourceWhitelist sections in Integration Config docs and Tenant Custom Resource docs).</p> <p>Note that ArgoCD integration in MTO is optional.</p>"},{"location":"integrations/argocd.html#default-argocd-configuration","title":"Default ArgoCD configuration","text":"<p>We have set a default ArgoCD configuration in Multi Tenant Operator that fulfils the following use cases:</p> <ul> <li>Tenants can only see their ArgoCD applications in the ArgoCD frontend.</li> <li>Tenant 'Owners' and 'Editors' have full access to their ArgoCD applications.</li> <li>Tenants in the 'Viewers' group have read-only access to their ArgoCD applications.</li> <li>Tenants can sync all namespace-scoped resources, except those that are blacklisted.</li> <li>Tenants can sync only cluster-scoped resources that are allow-listed.</li> <li>Tenant 'Owners' can configure their own GitOps source repositories at the tenant level.</li> <li>Cluster admins can prevent specific resources from syncing via ArgoCD.</li> <li>Cluster admins have full access to all ArgoCD applications and AppProjects.</li> <li>ArgoCD integration is on a per-tenant level; namespace-scoped applications are synced only to tenant namespaces.</li> </ul>"},{"location":"integrations/argocd.html#creating-argocd-appprojects-for-your-tenant","title":"Creating ArgoCD AppProjects for your tenant","text":"<p>To ensure each tenant has their own ArgoCD AppProjects, administrators must first specify the ArgoCD namespace in the IntegrationConfig:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: IntegrationConfig\nmetadata:\n  name: tenant-operator-config\n  namespace: multi-tenant-operator\nspec:\n  ...\n  argocd:\n    namespace: openshift-operators\n  ...\n</code></pre> <p>Administrators then create an Extension CR associated with the tenant:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Extensions\nmetadata:\n  name: extensions-sample\nspec:\n  tenantName: tenant-sample\n  argoCD:\n    onDeletePurgeAppProject: true\n    appProject:\n      sourceRepos:\n        - \"github.com/stakater/repo\"\n      clusterResourceWhitelist:\n        - group: \"\"\n          kind: \"Pod\"\n      namespaceResourceBlacklist:\n        - group: \"v1\"\n          kind: \"ConfigMap\"\n</code></pre> <p>This creates an AppProject for the tenant:</p> <pre><code>oc get AppProject -A\nNAMESPACE             NAME           AGE\nopenshift-operators   tenant-sample  5d15h\n</code></pre> <p>Example of the created AppProject:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: tenant-sample\n  namespace: openshift-operators\nspec:\n  destinations:\n    - namespace: tenant-sample-build\n      server: \"https://kubernetes.default.svc\"\n    - namespace: tenant-sample-dev\n      server: \"https://kubernetes.default.svc\"\n    - namespace: tenant-sample-stage\n      server: \"https://kubernetes.default.svc\"\n  roles:\n    - description: &gt;-\n        Role that gives full access to all resources inside the tenant's\n        namespace to the tenant owner groups\n      groups:\n        - saap-cluster-admins\n        - stakater-team\n        - tenant-sample-owner-group\n      name: tenant-sample-owner\n      policies:\n        - \"p, proj:tenant-sample:tenant-sample-owner, *, *, tenant-sample/*, allow\"\n    - description: &gt;-\n        Role that gives edit access to all resources inside the tenant's\n        namespace to the tenant owner group\n      groups:\n        - saap-cluster-admins\n        - stakater-team\n        - tenant-sample-edit-group\n      name: tenant-sample-edit\n      policies:\n        - \"p, proj:tenant-sample:tenant-sample-edit, *, *, tenant-sample/*, allow\"\n    - description: &gt;-\n        Role that gives view access to all resources inside the tenant's\n        namespace to the tenant owner group\n      groups:\n        - saap-cluster-admins\n        - stakater-team\n        - tenant-sample-view-group\n      name: tenant-sample-view\n      policies:\n        - \"p, proj:tenant-sample:tenant-sample-view, *, get, tenant-sample/*, allow\"\n  sourceRepos:\n    - \"https://github.com/stakater/gitops-config\"\n</code></pre> <p>Users belonging to the tenant group will now see only applications created by them in the ArgoCD frontend:</p> <p></p> <p>Note</p> <p>For ArgoCD Multi Tenancy to work properly, any default roles or policies attached to all users must be removed.</p>"},{"location":"integrations/argocd.html#preventing-argocd-from-syncing-certain-name-spaced-resources","title":"Preventing ArgoCD from Syncing Certain Name-spaced Resources","text":"<p>To prevent tenants from syncing ResourceQuota and LimitRange resources to their namespaces, administrators can specify these resources in the blacklist section of the ArgoCD configuration in the IntegrationConfig:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: IntegrationConfig\nmetadata:\n  name: tenant-operator-config\n  namespace: multi-tenant-operator\nspec:\n  ...\n  integrations:\n    argocd:\n      namespace: openshift-operators\n      namespaceResourceBlacklist:\n        - group: \"\"\n          kind: ResourceQuota\n        - group: \"\"\n          kind: LimitRange\n  ...\n</code></pre> <p>This configuration ensures these resources are not synced by ArgoCD if added to any tenant's project directory in GitOps. The AppProject will include the blacklisted resources:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: tenant-sample\n  namespace: openshift-operators\nspec:\n  ...\n  namespaceResourceBlacklist:\n    - group: ''\n      kind: ResourceQuota\n    - group: ''\n      kind: LimitRange\n  ...\n</code></pre>"},{"location":"integrations/argocd.html#allowing-argocd-to-sync-certain-cluster-wide-resources","title":"Allowing ArgoCD to Sync Certain Cluster-Wide Resources","text":"<p>To allow tenants to sync the Environment cluster-scoped resource, administrators can specify this resource in the allow-list section of the ArgoCD configuration in the IntegrationConfig's spec:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: IntegrationConfig\nmetadata:\n  name: tenant-operator-config\n  namespace: multi-tenant-operator\nspec:\n  ...\n  integrations:\n    argocd:\n      namespace: openshift-operators\n      clusterResourceWhitelist:\n        - group: \"\"\n          kind: Environment\n  ...\n</code></pre> <p>This configuration ensures these resources are synced by ArgoCD if added to any tenant's project directory in GitOps. The AppProject will include the allow-listed resources:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: tenant-sample\n  namespace: openshift-operators\nspec:\n  ...\n  clusterResourceWhitelist:\n  - group: \"\"\n    kind: Environment\n  ...\n</code></pre>"},{"location":"integrations/argocd.html#overriding-namespaceresourceblacklist-andor-clusterresourcewhitelist-per-tenant","title":"Overriding NamespaceResourceBlacklist and/or ClusterResourceWhitelist Per Tenant","text":"<p>To override the <code>namespaceResourceBlacklist</code> and/or <code>clusterResourceWhitelist</code> set via Integration Config for a specific tenant, administrators can specify these in the <code>argoCD</code> section of the Extension CR:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Extensions\nmetadata:\n  name: extensions-blue-sky\nspec:\n  tenantName: blue-sky\n  argoCD:\n    onDeletePurgeAppProject: true\n    appProject:\n      sourceRepos:\n        - \"github.com/stakater/repo\"\n      clusterResourceWhitelist:\n        - group: \"\"\n          kind: \"Pod\"\n      namespaceResourceBlacklist:\n        - group: \"v1\"\n          kind: \"ConfigMap\"\n</code></pre> <p>This configuration allows for tailored settings for each tenant, ensuring flexibility and control over ArgoCD resources.</p>"},{"location":"integrations/aws-pricing.html","title":"AWS Pricing","text":"<p>MTO supports AWS pricing model via the <code>integrationConfig.components.showbackOpts.cloudIntegrationSecretRef</code> field. Following 2 types of pricing are supported:</p> <ul> <li><code>AWS Standard Pricing</code></li> <li><code>AWS Spot Instance Datafeed</code></li> </ul>"},{"location":"integrations/aws-pricing.html#aws-standard-pricing","title":"AWS Standard Pricing","text":"<p>OpenCost will automatically read the node information <code>node.spec.providerID</code> to determine the cloud service provider (CSP) in use. If it detects the CSP is AWS, it will attempt to pull the AWS on-demand pricing from the configured public API URL with no further configuration required.</p> <p>OpenCost will request pricing data from the <code>us-east-1</code> region for your <code>node_region</code> using the template:</p> <pre><code>https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/${node_region}/index.json\n</code></pre>"},{"location":"integrations/aws-pricing.html#aws-spot-instance-pricing","title":"AWS Spot Instance Pricing","text":"<p>To enable the AWS Spot Instance pricing, subscribe to AWS Spot Instance Data Feed with the following command</p> <pre><code>aws ec2 create-spot-datafeed-subscription \\\n    --bucket &lt;BUCKET_NAME&gt; \\\n    [--prefix &lt;BUCKET_PREFIX&gt;]\n</code></pre> <ul> <li><code>&lt;BUCKET_NAME&gt;</code> \u2014 Name of the bucket Amazon S3 bucket to store the data feed files</li> <li><code>&lt;BUCKET_PREFIX&gt;</code> \u2014 Optional prefix directory in which data feed files will be stored</li> </ul>"},{"location":"integrations/aws-pricing.html#amazon-s3-bucket-requirements","title":"Amazon S3 bucket requirements","text":"<p>When you subscribe to the data feed, you must specify an Amazon S3 bucket to store the data feed files.</p> <p>Before you choose an Amazon S3 bucket for the data feed, consider the following:</p> <ul> <li> <p>You must have <code>FULL_CONTROL</code> permission to the bucket. If you're the bucket owner, you have this permission by default. Otherwise, the bucket owner must grant your AWS account this permission.</p> </li> <li> <p>When you subscribe to a data feed, these permissions are used to update the bucket ACL to give the AWS data feed account <code>FULL_CONTROL</code> permission. The AWS data feed account writes data feed files to the bucket. If your account doesn't have the required permissions, the data feed files cannot be written to the bucket. For more information, see Logs sent to Amazon S3 in the Amazon CloudWatch Logs User Guide.</p> </li> <li> <p>If you update the ACL and remove the permissions for the AWS data feed account, the data feed files cannot be written to the bucket. You must resubscribe to the data feed to receive the data feed files.</p> </li> <li> <p>Each data feed file has its own ACL (separate from the ACL for the bucket). The bucket owner has <code>FULL_CONTROL</code> permission to the data files. The AWS data feed account has read and write permissions.</p> </li> <li> <p>If you delete your data feed subscription, Amazon EC2 doesn't remove the read and write permissions for the AWS data feed account on either the bucket or the data files. You must remove these permissions yourself.</p> </li> <li> <p>If you encrypt your Amazon S3 bucket using server-side encryption with an AWS KMS key stored in AWS Key Management Service (SSE-KMS), you must use a customer managed key. For more information, see Amazon S3 bucket server-side encryption in the Amazon CloudWatch Logs User Guide.</p> </li> </ul>"},{"location":"integrations/aws-pricing.html#configure-an-iam-policy","title":"Configure an IAM Policy","text":"<ul> <li>Create a user or role for OpenCost with access to the spot feed bucket. Attach the policy below to the role/user and replace <code>&lt;BUCKET_NAME&gt;</code> with your spot bucket name</li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:ListAllMyBuckets\",\n                \"s3:ListBucket\",\n                \"s3:HeadBucket\",\n                \"s3:HeadObject\",\n                \"s3:List*\",\n                \"s3:Get*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;BUCKET_NAME&gt;\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Sid\": \"SpotDataAccess\"\n        }\n    ]\n}\n</code></pre>"},{"location":"integrations/aws-pricing.html#option-1-service-keys","title":"Option 1: Service Keys","text":"<p>Generate Access Keys with following steps</p> <ol> <li>Navigate to the AWS IAM Console.</li> <li>Select Access Management &gt; Users from the left navigation.</li> <li>Find the OpenCost User and select Security credentials &gt; Create access key.</li> <li>Follow along to receive the Access Key ID and Secret Access Key (AWS will not provide you the Secret Access Key in the future, so make sure you save this value).</li> </ol> <p>Create a <code>service-key.json</code> and replace the values with the user access keys</p> <pre><code>{\n  \"aws_access_key_id\": \"&lt;ACCESS_KEY_ID&gt;\",\n  \"aws_secret_access_key\": \"&lt;SECRET_ACCESS_KEY&gt;\"\n}\n</code></pre> <p>Create a Kubernetes secret with the following command</p> <pre><code>kubectl create secret generic &lt;SECRET_NAME&gt; --from-file=service-key.json --namespace multi-tenant-operator\n</code></pre> <p>Note</p> <p>When managing the service account key as a Kubernetes secret, the secret must reference the service account key JSON file, and that file must be named <code>service-key.json</code>.</p> <p>The data feed will provide specific pricing information about any Spot instances in your account on an hourly basis. After setting this up, the bucket information can be provided through options in the IntegrationConfig via <code>integrationConfig.components.showbackOpts.custom</code> object.</p> <ul> <li><code>awsSpotDataBucket</code> - The name of the S3 bucket Spot Instance Data Feed is publishing to.</li> <li><code>awsSpotDataRegion</code> - The region configured for Spot Instance Data Feed</li> <li><code>awsSpotDataPrefix</code> - The prefix (if any) configured for Spot Instance Data Feed</li> <li><code>projectID</code> - The AWS Account ID</li> <li><code>&lt;SECRET_NAME&gt;</code> - Name of the service key secret created in previous step</li> </ul> <p>Example:</p> <pre><code>components:\n  showback: true # should be enabled\n  showbackOpts:\n    custom:\n      provider: aws\n      description: \"AWS Provider Configuration. Provides default values used if instance type or spot information is not found.\"\n      CPU: \"0.031611\"\n      spotCPU: \"0.006655\"\n      RAM: \"0.004237\"\n      GPU: \"0.95\"\n      spotRAM: \"0.000892\"\n      storage: \"0.000138888889\"\n      zoneNetworkEgress: \"0.01\"\n      regionNetworkEgress: \"0.01\"\n      internetNetworkEgress: \"0.143\"\n      spotLabel: \"kops.k8s.io/instancegroup\"\n      spotLabelValue: \"spotinstance-nodes\"\n      awsSpotDataRegion: \"us-east-2\"\n      awsSpotDataBucket: \"my-spot-bucket\"\n      awsSpotDataPrefix: \"spot_pricing_prefix\"\n      projectID: \"012345678901\"\n\n    cloudPricingSecretRef:\n      name: &lt;SECRET_NAME&gt;\n      namespace: multi-tenant-operator\n</code></pre>"},{"location":"integrations/aws-pricing.html#option-2-irsa-iam-roles-for-service-accounts","title":"Option 2: IRSA (IAM Roles for Service Accounts)","text":"<p>After creating the role and policy, attach the role ARN as an annotation to the opencost gateway account via following command</p> <pre><code>kubectl annotate serviceaccounts -n multi-tenant-operator tenant-operator-opencost-gateway eks.amazonaws.com/role-arn=&lt;ROLE_ARN&gt;\n</code></pre> <p>Apply the IntegrationConfig with current spot bucket data feed information</p> <p>Example:</p> <pre><code>components:\n  showback: true # should be enabled\n  showbackOpts:\n    custom:\n      provider: aws\n      description: \"AWS Provider Configuration. Provides default values used if instance type or spot information is not found.\"\n      CPU: \"0.031611\"\n      spotCPU: \"0.006655\"\n      RAM: \"0.004237\"\n      GPU: \"0.95\"\n      spotRAM: \"0.000892\"\n      storage: \"0.000138888889\"\n      zoneNetworkEgress: \"0.01\"\n      regionNetworkEgress: \"0.01\"\n      internetNetworkEgress: \"0.143\"\n      spotLabel: \"kops.k8s.io/instancegroup\"\n      spotLabelValue: \"spotinstance-nodes\"\n      awsSpotDataRegion: \"us-east-2\"\n      awsSpotDataBucket: \"my-spot-bucket\"\n      awsSpotDataPrefix: \"spot_pricing_prefix\"\n      projectID: \"012345678901\"\n</code></pre>"},{"location":"integrations/aws-pricing.html#references","title":"References","text":"<p>For more information, refer to the following resources</p> <ul> <li><code>OpenCost documentation</code></li> <li><code>Kubecost documentation</code></li> <li><code>Assign IAM roles to Kubernetes service accounts</code></li> <li><code>Track your Spot Instance costs using the Spot Instance data feed</code></li> <li><code>Manage access keys from IAM users</code></li> </ul>"},{"location":"integrations/azure-pricing.html","title":"Azure Pricing","text":"<p>MTO supports Azure pricing model via the <code>integrationConfig.components.showbackOpts.cloudPricingSecretRef</code> field. Following 2 types of pricing are supported:</p> <ul> <li><code>Azure Standard Pricing</code></li> <li><code>Customer-specific pricing</code></li> </ul>"},{"location":"integrations/azure-pricing.html#azure-standard-pricing","title":"Azure Standard Pricing","text":"<p>For Azure pricing configuration, OpenCost needs access to the Microsoft Azure Billing Rate Card API to access accurate pricing data for your Kubernetes resources.</p> <p>Follow the steps below to create a custom Azure role and secret to access the Azure Rate Card API:</p>"},{"location":"integrations/azure-pricing.html#create-a-custom-azure-role","title":"Create a Custom Azure role","text":"<p>Start by creating an Azure role definition. Below is an example definition, replace <code>YOUR_SUBSCRIPTION_ID</code> with the ID of the subscription containing your Kubernetes cluster. (How to find your subscription ID.)</p> <pre><code>{\n    \"Name\": \"OpenCostRole\",\n    \"IsCustom\": true,\n    \"Description\": \"Rate Card query role\",\n    \"Actions\": [\n        \"Microsoft.Compute/virtualMachines/vmSizes/read\",\n        \"Microsoft.Resources/subscriptions/locations/read\",\n        \"Microsoft.Resources/providers/read\",\n        \"Microsoft.ContainerService/containerServices/read\",\n        \"Microsoft.Commerce/RateCard/read\"\n    ],\n    \"AssignableScopes\": [\n        \"/subscriptions/YOUR_SUBSCRIPTION_ID\"\n    ]\n}\n</code></pre> <p>Save this into a file called <code>myrole.json</code></p> <p>Next, you'll want to register that role with Azure:</p> <pre><code>az role definition create --verbose --role-definition @myrole.json\n</code></pre>"},{"location":"integrations/azure-pricing.html#create-an-azure-service-principal","title":"Create an Azure Service Principal","text":"<p>Next, create an Azure Service Principal.</p> <pre><code>az ad sp create-for-rbac --name \"OpenCostAccess\" --role \"OpenCostRole\" --scope \"/subscriptions/YOUR_SUBSCRIPTION_ID\" --output json\n</code></pre> <p>Keep this information which is used in the <code>service-key.json</code> below.</p>"},{"location":"integrations/azure-pricing.html#supply-azure-service-principal-details-to-opencost","title":"Supply Azure Service Principal details to OpenCost","text":"<p>Create a file called <code>service-key.json</code> and update it with the Service Principal details from the above steps:</p> <pre><code>{\n    \"subscriptionId\": \"&lt;Azure Subscription ID&gt;\",\n    \"serviceKey\": {\n        \"appId\": \"&lt;Azure AD App ID&gt;\",\n        \"displayName\": \"OpenCostAccess\",\n        \"password\": \"&lt;Azure AD Client Secret&gt;\",\n        \"tenant\": \"&lt;Azure AD Tenant ID&gt;\"\n    }\n}\n</code></pre> <p>Next, create a secret for the Azure Service Principal</p> <p>Note</p> <p>When managing the service account key as a Kubernetes secret, the secret must reference the service account key JSON file, and that file must be named <code>service-key.json</code>.</p> <pre><code>kubectl create secret generic azure-service-key -n multi-tenant-operator --from-file=service-key.json\n</code></pre>"},{"location":"integrations/azure-pricing.html#update-the-integrationconfig","title":"Update the IntegrationConfig","text":"<p>Finally, update the IntegrationConfig with the Azure pricing model:</p> <pre><code>components:\n    console: true # should be enabled\n    showback: true # should be enabled\n    showbackOpts:\n      cloudPricingSecretRef:\n        name: azure-service-key\n        namespace: multi-tenant-operator\n</code></pre>"},{"location":"integrations/azure-pricing.html#customer-specific-pricing","title":"Customer-specific pricing","text":"<p>The Rate Card prices retrieved with the setup above are the standard prices for Azure resources offered to all customers. If your organisation has an Enterprise Agreement or Partner Agreement with Azure you may have discounts for some of the resources used by your clusters. In that case you can configure OpenCost to use the Consumption Price Sheet API to request prices specifically for your billing account.</p> <p>Note</p> <p>Calling the Price Sheet API uses the service principal secret created above - those steps are prerequisites for this section.</p>"},{"location":"integrations/azure-pricing.html#find-your-billing-account-id","title":"Find your billing account ID","text":"<p>You can find your billing account ID in the Azure portal, or using the <code>az</code> CLI:</p> <pre><code>az billing account list --query \"[].{name:name, displayName:displayName}\"\n</code></pre>"},{"location":"integrations/azure-pricing.html#grant-billing-access-to-your-service-principal","title":"Grant billing access to your Service Principal","text":"<p>To call the Price Sheet API the service principal you created above needs to be granted the EnrollmentReader billing role. You can do this by following this Azure guide and using the Role Assignments API reference page.</p> <p>Assigning a billing role isn't directly supported in the <code>az</code> CLI yet, so the process is quite involved. To simplify this, you can use the <code>Bash</code> script below to collect the details of your service principal, construct the PUT request and send it with curl.</p> <p>Save the script to a file named <code>assign-billing-role.bash</code> and run it:</p> <pre><code>export SP_NAME=OpenCostAccess\nexport BILLING_ACCOUNT_ID=&lt;your billing account ID&gt;\nchmod u+x assign-billing-role.bash\n./assign-billing-role.bash\n</code></pre>"},{"location":"integrations/azure-pricing.html#find-the-offer-id-for-your-subscription","title":"Find the offer ID for your subscription","text":"<p>As well as the billing account ID, OpenCost also needs the offer ID for your subscription to query the price sheet. You can find this on the subscription page in the Azure portal.</p>"},{"location":"integrations/azure-pricing.html#configure-opencost-to-use-the-price-sheet-api","title":"Configure OpenCost to use the Price Sheet API","text":"<p>The billing account and offer ID need to be passed to OpenCost in environment variables. To do this, create a secret with the following values:</p> <pre><code>kubectl create secret generic customer-specific-pricing -n multi-tenant-operator --from-literal=azure-billing-account=&lt;your billing account ID&gt; --from-literal=azure-offer-id=&lt;your offer ID&gt;\n</code></pre> <p>Finally, update the IntegrationConfig with the Azure pricing model:</p> <pre><code>components:\n    console: true # should be enabled\n    showback: true # should be enabled\n    showbackOpts:\n      cloudPricingSecretRef:\n        name: customer-specific-pricing\n        namespace: multi-tenant-operator\n</code></pre>"},{"location":"integrations/azure-pricing.html#script-to-assign-billing-role","title":"Script to assign billing role","text":"<pre><code>#!/bin/bash\n\n# Helper to assign the billing EnrollmentReader role to a service principal\n# Needs SP name and billing account name variables set\n\nset -euo pipefail\n\nif [[ -z \"${SP_NAME}\" ]]; then\n  echo \"SP_NAME is not set\"\n  exit 1\nfi\n\nif [[ -z \"${BILLING_ACCOUNT_ID}\" ]]; then\n  echo \"BILLING_ACCOUNT_ID is not set\"\n  exit 1\nfi\n\n# Generate a unique name for the assignment.\nROLE_ASSIGNMENT_NAME=\"$(uuidgen)\"\n\n# Work out the SP ID and tenant ID from the name.\nread -r SP_ID TENANT_ID &lt; &lt;(az ad sp list --display-name \"${SP_NAME}\" --query '[0].{id:id,tenantId:appOwnerOrganizationId}' -o tsv)\n\n# Get bearer token for talking to API.\nACCESS_TOKEN=\"$(az account get-access-token --query accessToken -o tsv)\"\n\nURL=\"https://management.azure.com/providers/Microsoft.Billing/billingAccounts/${BILLING_ACCOUNT_ID}/billingRoleAssignments/${ROLE_ASSIGNMENT_NAME}?api-version=2019-10-01-preview\"\n\necho \"Creating EnrollmentReader role assignment for SP ${SP_NAME} (${SP_ID}) in billing account ${BILLING_ACCOUNT_ID}\"\necho \"Role assignment name: ${ROLE_ASSIGNMENT_NAME}\"\n\n# This is the role definition ID for EnrollmentReader\nENROLLMENT_READER_ROLE=\"24f8edb6-1668-4659-b5e2-40bb5f3a7d7e\"\nRESPONSE=\"$(curl --silent --show-error -X PUT \"${URL}\" \\\n-H \"Authorization: Bearer ${ACCESS_TOKEN}\" \\\n-H \"Content-type: application/json\" \\\n-d \"{\n  \\\"properties\\\": {\n    \\\"principalId\\\": \\\"${SP_ID}\\\",\n    \\\"principalTenantId\\\": \\\"${TENANT_ID}\\\",\n    \\\"roleDefinitionId\\\": \\\"/providers/Microsoft.Billing/billingAccounts/${BILLING_ACCOUNT_ID}/billingRoleDefinitions/${ENROLLMENT_READER_ROLE}\\\"\n  }\n}\")\"\n\necho \"Response: ${RESPONSE}\"\n</code></pre>"},{"location":"integrations/azure-pricing.html#conclusion","title":"Conclusion","text":"<p>In this guide, we have seen how to configure OpenCost to use Azure pricing model. We have seen how to configure Azure Pricing Configuration, Customer-specific pricing, and Azure Cloud Costs. To enable all three pricing models, you can create a single secret with all the required values and update the IntegrationConfig to use the secret.</p> <p>for example:</p> <pre><code>kubectl create secret generic azure-pricing -n multi-tenant-operator --from-file=service-key.json --from-literal=azure-billing-account=&lt;your billing account ID&gt; --from-literal=azure-offer-id=&lt;your offer ID&gt; --from-file=./cloud-integration.json\n</code></pre> <p>Update the IntegrationConfig to use the secret:</p> <pre><code>components:\n    console: true # should be enabled\n    showback: true # should be enabled\n    showbackOpts:\n      cloudPricingSecretRef:\n        name: azure-pricing\n        namespace: multi-tenant-operator\n</code></pre> <p>For more information, refer to the OpenCost documentation.</p>"},{"location":"integrations/devworkspace.html","title":"DevWorkspace","text":""},{"location":"integrations/devworkspace.html#devworkspaces-metadata-via-multi-tenant-operator","title":"DevWorkspaces metadata via Multi Tenant Operator","text":"<p>DevWorkspaces require specific metadata on a namespace for it to work in it. With Multi Tenant Operator (MTO), you can create sandbox namespaces for users of a Tenant, and then add the required metadata automatically on all sandboxes.</p>"},{"location":"integrations/devworkspace.html#required-metadata-for-enabling-devworkspace-on-sandbox","title":"Required metadata for enabling DevWorkspace on sandbox","text":"<pre><code>  labels:\n    app.kubernetes.io/part-of: che.eclipse.org\n    app.kubernetes.io/component: workspaces-namespace\n  annotations:\n    che.eclipse.org/username: &lt;username&gt;\n</code></pre>"},{"location":"integrations/devworkspace.html#automate-sandbox-metadata-for-all-tenant-users-via-tenant-cr","title":"Automate sandbox metadata for all Tenant users via Tenant CR","text":"<p>With Multi Tenant Operator (MTO), you can set <code>sandboxMetadata</code> like below to automate metadata for all sandboxes:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@acme.org\n    editors:\n      users:\n        - erik@acme.org\n    viewers:\n      users:\n        - john@acme.org\n  namespaces:\n    sandboxes:\n      enabled: true\n      private: false\n    metadata:\n      sandbox:\n        labels:\n          app.kubernetes.io/part-of: che.eclipse.org\n          app.kubernetes.io/component: workspaces-namespace\n        annotations:\n          che.eclipse.org/username: \"{{ TENANT.USERNAME }}\"\n</code></pre> <p>It will create sandbox namespaces and also apply the <code>sandboxMetadata</code> for owners and editors. Notice the template <code>{{ TENANT.USERNAME }}</code>, it will resolve the username as value of the corresponding annotation. For more info on templated value, see here</p>"},{"location":"integrations/devworkspace.html#automate-sandbox-metadata-for-all-tenant-users-via-integrationconfig-cr","title":"Automate sandbox metadata for all Tenant users via IntegrationConfig CR","text":"<p>You can also automate the metadata on all sandbox namespaces by using IntegrationConfig, notice <code>metadata.sandboxes</code>:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: IntegrationConfig\nmetadata:\n  name: tenant-operator-config\n  namespace: multi-tenant-operator\nspec:\n  accessControl:\n    namespaceAccessPolicy:\n      deny:\n        privilegedNamespaces: {}\n    privileged:\n      namespaces:\n      - ^default$\n      - ^openshift.*\n      - ^kube.*\n      serviceAccounts:\n      - ^system:serviceaccount:openshift.*\n      - ^system:serviceaccount:kube.*\n      - ^system:serviceaccount:stakater-actions-runner-controller:actions-runner-controller-runner-deployment$\n    rbac:\n      tenantRoles:\n        default:\n          editor:\n            clusterRoles:\n            - edit\n          owner:\n            clusterRoles:\n            - admin\n          viewer:\n            clusterRoles:\n            - view\n  components:\n    console: false\n    ingress:\n      console: {}\n      gateway: {}\n      keycloak: {}\n    showback: false\n  integrations:\n    vault:\n      accessInfo:\n        accessorPath: \"\"\n        address: \"\"\n        roleName: \"\"\n        secretRef:\n          name: \"\"\n          namespace: \"\"\n      authMethod: kubernetes\n      config:\n        ssoClient: \"\"\n      enabled: false\n  metadata:\n    groups: {}\n    namespaces: {}\n    sandboxes:\n      labels:\n        app.kubernetes.io/part-of: che.eclipse.org\n        app.kubernetes.io/component: workspaces-namespace\n      annotations:\n        che.eclipse.org/username: \"{{ TENANT.USERNAME }}\"\n</code></pre> <p>For more info on templated value <code>\"{{ TENANT.USERNAME }}\"</code>, see here</p>"},{"location":"integrations/keycloak.html","title":"Keycloak","text":"<p>MTO Console uses Keycloak for authentication and authorization. By default, the MTO Console uses an internal Keycloak instance that is provisioned by the Multi Tenant Operator in its own namespace. However, you can also integrate an external Keycloak instance with the MTO Console.</p> <p>This guide will help you integrate an external Keycloak instance with the MTO Console.</p>"},{"location":"integrations/keycloak.html#prerequisites","title":"Prerequisites","text":"<ul> <li>An OpenShift cluster with Multi Tenant Operator installed.</li> <li>An external Keycloak instance.</li> </ul>"},{"location":"integrations/keycloak.html#steps","title":"Steps","text":"<p>Navigate to the Keycloak console.</p> <ul> <li>Go to your realm.</li> <li>Click on the <code>Clients</code>.</li> <li>Click on the <code>Create</code> button to create a new client.</li> </ul> <p></p> <p>Create a new client.</p> <ul> <li>Fill in the <code>Client ID</code>, <code>Client Name</code> and <code>Client Protocol</code> fields.</li> </ul> <p></p> <ul> <li>Add <code>Valid Redirect URIs</code> and <code>Web Origins</code> for the client.</li> </ul> <p></p> <p>Note: The <code>Valid Redirect URIs</code> and <code>Web Origins</code> should be the URL of the MTO Console.</p> <ul> <li>Click on the <code>Save</code> button.</li> </ul>"},{"location":"integrations/keycloak.html#update-integration-config","title":"Update Integration Config","text":"<ul> <li>Update the <code>IntegrationConfig</code> CR with the following configuration.</li> </ul> <pre><code>integrations: \n  keycloak:\n    realm: &lt;realm&gt;\n    address: &lt;keycloak-address&gt;\n    clientName: &lt;client-name&gt;\n</code></pre> <ul> <li>Now, the MTO Console will be integrated with the external Keycloak instance.</li> </ul>"},{"location":"integrations/mattermost.html","title":"Mattermost","text":""},{"location":"integrations/mattermost.html#requirements","title":"Requirements","text":"<p><code>MTO-Mattermost-Integration-Operator</code></p> <p>Please contact stakater to install the Mattermost integration operator before following the below-mentioned steps.</p>"},{"location":"integrations/mattermost.html#steps-to-enable-integration","title":"Steps to enable integration","text":"<p>Bill wants some tenants to also have their own Mattermost Teams. To make sure this happens correctly, Bill will first add the <code>stakater.com/mattermost: true</code> label to the tenants. The label will enable the <code>mto-mattermost-integration-operator</code> to create and manage Mattermost Teams based on Tenants.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: sigma\n  labels:\n    stakater.com/mattermost: 'true'\nspec:\n  quota: medium\n  accessControl:\n    owners:\n      users:\n        - user\n    editors:\n      users:\n        - user1\n  namespaces:\n    sandboxes:\n      enabled: false\n    withTenantPrefix:\n      - dev\n      - build\n      - prod\n</code></pre> <p>Now user can log In to Mattermost to see their Team and relevant channels associated with it.</p> <p></p> <p>The name of the Team is similar to the Tenant name. Notification channels are pre-configured for every team, and can be modified.</p>"},{"location":"integrations/vault/vault-ic.html","title":"Vault","text":"<p>Vault is used to secure, store and tightly control access to tokens, passwords, certificates, and encryption keys for protecting secrets and other sensitive data using a UI, CLI, or http API.</p> <p>To enable Vault multi-tenancy, a role has to be created in Vault under Kubernetes authentication with the following permissions:</p> <pre><code>path \"secret/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\npath \"sys/mounts\" {\n  capabilities = [\"read\", \"list\"]\n}\npath \"sys/mounts/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\npath \"managed-addons/*\" {\n  capabilities = [\"read\", \"list\"]\n}\npath \"auth/kubernetes/role/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\npath \"sys/auth\" {\n  capabilities = [\"read\", \"list\"]\n}\npath \"sys/policies/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\npath \"identity/group\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\npath \"identity/group-alias\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\npath \"identity/group/name/*\" {\n  capabilities = [\"read\", \"list\"]\n}\npath \"identity/group/id/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\n</code></pre> <p>If Bill (the cluster admin) has Vault configured in his cluster, then he can take benefit from MTO's integration with Vault.</p> <p>MTO automatically creates Vault secret paths for tenants, where tenant members can securely save their secrets. It also authorizes tenant members to access these secrets via OIDC.</p> <p>Bill would first have to integrate Vault with MTO by adding the details in IntegrationConfig. For more details</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: IntegrationConfig\nmetadata:\n  name: tenant-operator-config\n  namespace: multi-tenant-operator\nspec:\n  integrations:\n    vault:\n      enabled: true\n      authMethod: kubernetes\n      accessInfo: \n        accessorPath: oidc/\n        address: https://vault.apps.prod.abcdefghi.kubeapp.cloud/\n        roleName: mto\n        secretRef:       \n          name: ''\n          namespace: ''\n      config: \n        ssoClient: vault\n</code></pre> <p>Bill then creates a tenant for Anna and John:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  accessControl:\n    owners:\n      users:\n      - anna@acme.org\n    viewers:\n      users:\n      - john@acme.org\n  quota: small\n  namespaces:\n    sandboxes:\n      enabled: false\n</code></pre> <p>Now Bill goes to <code>Vault</code> and sees that a path for <code>tenant</code> has been made under the name <code>bluesky/kv</code>, confirming that Tenant members with the Owner or Edit roles now have access to the tenant's Vault path.</p> <p>Now if Anna sign's in to the Vault via OIDC, she can see her tenants path and secrets. Whereas if John sign's in to the Vault via OIDC, he can't see his tenants path or secrets as he doesn't have the access required to view them.</p> <p>For more details around enabling Kubernetes auth in Vault, visit here</p>"},{"location":"integrations/vault/vault-kc-entraid.html","title":"Integrating Vault and Keycloak with Microsoft Entra ID","text":"<p>This guide provides step-by-step instructions for integrating Vault with Keycloak using Microsoft Entra ID for OIDC-based authentication. The setup focuses on using group IDs for access control, as Microsoft Entra ID only provides group IDs in its tokens, not group names.</p>"},{"location":"integrations/vault/vault-kc-entraid.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Microsoft Entra ID configured for OIDC.</li> <li>Keycloak setup with an Identity Provider (IDP) pointing to Microsoft Entra ID.</li> <li>Hashicorp Vault installed and configured.</li> <li>Vault Configured in IntegrationConfig</li> </ul>"},{"location":"integrations/vault/vault-kc-entraid.html#steps-to-implement-group-based-access-control-with-group-ids","title":"Steps to Implement Group-Based Access Control with Group IDs","text":""},{"location":"integrations/vault/vault-kc-entraid.html#step-1-configure-microsoft-entra-id-to-include-group-ids-in-tokens","title":"Step 1: Configure Microsoft Entra ID to Include Group IDs in Tokens","text":"<ol> <li>Navigate to Microsoft Entra ID:</li> <li> <p>Go to Microsoft Entra ID \u2192 App Registrations.</p> </li> <li> <p>Set up Optional Claims:</p> </li> <li>In the Microsoft Entra ID App Registration for your Keycloak, configure an optional claim for the app configured with keycloak to include group IDs in the tokens.</li> </ol> <p></p>"},{"location":"integrations/vault/vault-kc-entraid.html#step-2-create-an-attribute-importer-mapper-in-keycloak","title":"Step 2: Create an Attribute Importer Mapper in Keycloak","text":"<ol> <li>Create Attribute Importer Mapper:</li> </ol> <p>To configure the groups claim from Microsoft Entra ID in Keycloak, create a new mapper with the following settings as shown in the image:</p> <pre><code>- Mapper Type: User Attribute\n- User Attribute: groups\n- Claim: groups\n- Sync Mode: FORCE\n</code></pre> <p></p>"},{"location":"integrations/vault/vault-kc-entraid.html#step-3-set-up-a-mapper-for-vault-client-in-keycloak","title":"Step 3: Set Up a Mapper for Vault Client in Keycloak","text":"<ol> <li> <p>Create ProtocolMapper:</p> <p>To configure the Protocol Mapper that forwards the groups attribute (containing group IDs) from the user's profile into the token, create a new mapper with the following settings as shown in the image:</p> </li> </ol> <p></p>"},{"location":"integrations/vault/vault-kc-entraid.html#step-4-patch-tenant-spec-with-microsoft-entra-id-group-ids-for-rbac","title":"Step 4: Patch Tenant Spec with Microsoft Entra ID Group IDs for RBAC","text":"<ol> <li>Add group IDs:</li> <li> <p>Modify the existing Tenant resource to include the Microsoft Entra ID group IDs under <code>accessControl</code>. This will ensure the correct group-based RBAC is applied for Vault.</p> </li> <li> <p>Example Tenant Spec:    Here\u2019s an example of how to patch the Tenant with the group ID from Microsoft Entra ID:</p> </li> </ol> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: arsenal\nspec:\n  accessControl:\n    owners:\n      groups:\n        - &lt;object-id&gt;\n</code></pre> <ul> <li><code>owners.groups</code> should be updated with the relevant Microsoft Entra ID group IDs to enforce access control based on the users\u2019 group memberships.</li> </ul> <p></p>"},{"location":"integrations/vault/vault-kc-entraid.html#conclusion","title":"Conclusion","text":"<p>By following these steps, you can successfully integrate Vault with Keycloak for OIDC authentication, using Microsoft Entra ID group IDs for access control. This configuration allows for granular, group-based permissions while working with the limitations of Microsoft Entra ID\u2019s token output.</p>"},{"location":"integrations/vault/vault.html","title":"Multi-Tenancy in Vault","text":""},{"location":"integrations/vault/vault.html#vault-multitenancy","title":"Vault Multitenancy","text":"<p>Hashicorp Vault is an identity-based secret and encryption management system. Vault validates and authorizes a system's clients (users, machines, apps) before providing them access to secrets or stored sensitive data.</p>"},{"location":"integrations/vault/vault.html#vault-integration-in-multi-tenant-operator","title":"Vault integration in Multi Tenant Operator","text":""},{"location":"integrations/vault/vault.html#service-account-auth-in-vault","title":"Service Account Auth in Vault","text":"<p>MTO enables the Kubernetes auth method which can be used to authenticate with Vault using a Kubernetes Service Account Token. When enabled, for every tenant namespace, MTO automatically creates policies and roles that allow the service accounts present in those namespaces to read secrets at tenant's path in Vault. The name of the role is the same as namespace name.</p> <p>These service accounts are required to have <code>stakater.com/vault-access: true</code> label, so they can be authenticated with Vault via MTO.</p> <p>The Diagram shows how MTO enables ServiceAccounts to read secrets from Vault.</p> <p></p>"},{"location":"integrations/vault/vault.html#user-oidc-auth-in-vault","title":"User OIDC Auth in Vault","text":"<p>This requires a running <code>RHSSO(RedHat Single Sign On)</code> instance integrated with Vault over OIDC login method.</p> <p>MTO creates specific policies in Vault for its tenant users.</p> <p>Mapping of tenant roles to Vault is shown below</p> Tenant Role Vault Path Vault Capabilities Owner, Editor (tenantName)/* Create, Read, Update, Delete, List Owner, Editor sys/mounts/(tenantName)/* Create, Read, Update, Delete, List Owner, Editor managed-addons/* Read, List Viewer (tenantName)/* Read <p>A simple user login workflow is shown in the diagram below.</p> <p></p>"},{"location":"kubernetes-resources/extensions.html","title":"Extensions","text":"<p>Extensions in MTO enhance its functionality by allowing integration with external services. Currently, MTO supports integration with ArgoCD, enabling you to synchronize your repositories and configure AppProjects directly through MTO. Future updates will include support for additional integrations.</p>"},{"location":"kubernetes-resources/extensions.html#configuring-argocd-integration","title":"Configuring ArgoCD Integration","text":"<p>Let us take a look at how you can create an Extension CR and integrate ArgoCD with MTO.</p> <p>Before you create an Extension CR, you need to modify the Integration Config resource and add the ArgoCD configuration.</p> <pre><code>  integrations:\n    argocd:\n      clusterResourceWhitelist:\n        - group: tronador.stakater.com\n          kind: EnvironmentProvisioner\n      namespaceResourceBlacklist:\n        - group: ''\n          kind: ResourceQuota\n      namespace: openshift-operators\n</code></pre> <p>The above configuration will allow the <code>EnvironmentProvisioner</code> CRD and blacklist the <code>ResourceQuota</code> resource. Also note that the <code>namespace</code> field is mandatory and should be set to the namespace where the ArgoCD is deployed.</p> <p>Every Extension CR is associated with a specific Tenant. Here's an example of an Extension CR that is associated with a Tenant named <code>tenant-sample</code>:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Extensions\nmetadata:\n  name: extensions-sample\nspec:\n  tenantName: tenant-sample\n  argoCD:\n    onDeletePurgeAppProject: true\n    appProject:\n      sourceRepos:\n        - \"github.com/stakater/repo\"\n      clusterResourceWhitelist:\n        - group: \"\"\n          kind: \"Pod\"\n      namespaceResourceBlacklist:\n        - group: \"v1\"\n          kind: \"ConfigMap\"\n</code></pre> <p>The above CR creates an Extension for the Tenant named <code>tenant-sample</code> with the following configurations:</p> <ul> <li><code>onDeletePurgeAppProject</code>: If set to <code>true</code>, the AppProject will be deleted when the Extension is deleted.</li> <li><code>sourceRepos</code>: List of repositories to sync with ArgoCD.</li> <li><code>appProject</code>: Configuration for the AppProject.<ul> <li><code>clusterResourceWhitelist</code>: List of cluster-scoped resources to sync.</li> <li><code>namespaceResourceBlacklist</code>: List of namespace-scoped resources to ignore.</li> </ul> </li> </ul> <p>In the backend, MTO will create an ArgoCD AppProject with the specified configurations.</p>"},{"location":"kubernetes-resources/integration-config.html","title":"Integration Config","text":"<p>IntegrationConfig is used to configure settings of multi-tenancy for Multi Tenant Operator.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: IntegrationConfig\nmetadata:\n  name: tenant-operator-config\n  namespace: multi-tenant-operator\nspec:\n  components:\n    console: true\n    showback: true\n    showbackOpts:\n      custom:\n        CPU: \"0.031611\"\n        spotCPU: \"0.006655\"\n        RAM: \"0.004237\"\n        spotRAM: \"0.000892\"\n        GPU: \"0.95\"\n        storage: \"0.00005479452\"\n        zoneNetworkEgress: \"0.01\"\n        regionNetworkEgress: \"0.01\"\n        internetNetworkEgress: \"0.12\"\n        spotLabel: \"kops.k8s.io/instancegroup\"\n        spotLabelValue: \"spotinstance-nodes\"\n        awsSpotDataRegion: \"us-east-2\"\n        awsSpotDataBucket: \"my-spot-bucket\"\n        awsSpotDataPrefix: \"spot_pricing_prefix\"\n        projectID: \"012345678901\"\n\n      cloudPricingSecretRef:\n        name: secret-name\n        namespace: multi-tenant-operator\n\n      opencostServiceRoleArn: arn:aws:iam::123456789012:role/S3Access\n\n    ingress:\n      ingressClassName: 'nginx'\n      keycloak:\n        host: tenant-operator-keycloak.apps.mycluster-ams.abcdef.cloud\n        tlsSecretName: tenant-operator-tls\n      console:\n        host: tenant-operator-console.apps.mycluster-ams.abcdef.cloud\n        tlsSecretName: tenant-operator-tls\n      gateway:\n        host: tenant-operator-gateway.apps.mycluster-ams.abcdef.cloud\n        tlsSecretName: tenant-operator-tls\n  accessControl:\n    rbac:\n      tenantRoles:\n        default:\n          owner:\n            clusterRoles:\n              - admin\n          editor:\n            clusterRoles:\n              - edit\n          viewer:\n            clusterRoles:\n              - view\n        custom:\n        - labelSelector:\n            matchExpressions:\n            - key: stakater.com/kind\n              operator: In\n              values:\n                - build\n            matchLabels:\n              stakater.com/kind: dev\n          owner:\n            clusterRoles:\n              - custom-owner\n          editor:\n            clusterRoles:\n              - custom-editor\n          viewer:\n            clusterRoles:\n              - custom-view\n    namespaceAccessPolicy:\n      deny:\n        privilegedNamespaces:\n          users:\n            - system:serviceaccount:openshift-argocd:argocd-application-controller\n            - adam@stakater.com\n          groups:\n            - cluster-admins\n    privileged:\n      namespaces:\n        - ^default$\n        - ^openshift.*\n        - ^kube.*\n      serviceAccounts:\n        - ^system:serviceaccount:openshift.*\n        - ^system:serviceaccount:kube.*\n      users:\n        - ''\n      groups:\n        - cluster-admins\n  metadata:\n    groups:\n      labels:\n        role: customer-reader\n      annotations:\n        openshift.io/node-selector: node-role.kubernetes.io/worker=\n    namespaces:\n      labels:\n        stakater.com/workload-monitoring: \"true\"\n      annotations:\n        openshift.io/node-selector: node-role.kubernetes.io/worker=\n    sandboxes:\n      labels:\n        stakater.com/kind: sandbox\n      annotations:\n        openshift.io/node-selector: node-role.kubernetes.io/worker=\n  integrations:\n    keycloak:\n      realm: mto\n      address: https://keycloak.apps.prod.abcdefghi.kubeapp.cloud\n      clientName: mto-console\n    argocd:\n      clusterResourceWhitelist:\n        - group: tronador.stakater.com\n          kind: EnvironmentProvisioner\n      namespaceResourceBlacklist:\n        - group: '' # all groups\n          kind: ResourceQuota\n      namespace: openshift-operators\n    vault:\n      enabled: true\n      authMethod: kubernetes      #enum: {kubernetes:default, token}\n      accessInfo:\n        accessorPath: oidc/\n        address: https://vault.apps.prod.abcdefghi.kubeapp.cloud/\n        roleName: mto\n        secretRef:\n          name: ''\n          namespace: ''\n      config:\n        ssoClient: vault\n  tenantPolicies:\n    network:\n      disableIntraTenantNetworking: true\n</code></pre> <p>Following are the different components that can be used to configure multi-tenancy in a cluster via Multi Tenant Operator.</p>"},{"location":"kubernetes-resources/integration-config.html#components","title":"Components","text":"<pre><code>  components:\n    console: true\n    showback: true\n    showbackOpts:\n      custom:\n        CPU: \"0.031611\"\n        spotCPU: \"0.006655\"\n        RAM: \"0.004237\"\n        spotRAM: \"0.000892\"\n        GPU: \"0.95\"\n        storage: \"0.00005479452\"\n        zoneNetworkEgress: \"0.01\"\n        regionNetworkEgress: \"0.01\"\n        internetNetworkEgress: \"0.12\"\n      cloudPricingSecretRef:\n        name: azure-pricing\n        namespace: multi-tenant-operator\n    ingress:\n      ingressClassName: nginx\n      keycloak:\n        host: tenant-operator-keycloak.apps.mycluster-ams.abcdef.cloud\n        tlsSecretName: tenant-operator-tls\n      console:\n        host: tenant-operator-console.apps.mycluster-ams.abcdef.cloud\n        tlsSecretName: tenant-operator-tls\n      gateway:\n        host: tenant-operator-gateway.apps.mycluster-ams.abcdef.cloud\n        tlsSecretName: tenant-operator-tls\n</code></pre> <ul> <li><code>components.console:</code> Enables or disables the console GUI for MTO.</li> <li><code>components.showback:</code> Enables or disables the showback feature on the console.</li> <li><code>components.ingress:</code> Configures the ingress settings for various components:<ul> <li><code>ingressClassName:</code> Ingress class to be used for the ingress.</li> <li><code>console:</code> Settings for the console's ingress.<ul> <li><code>host:</code> hostname for the console's ingress.</li> <li><code>tlsSecretName:</code> Name of the secret containing the TLS certificate and key for the console's ingress.</li> </ul> </li> <li><code>gateway:</code> Settings for the gateway's ingress.<ul> <li><code>host:</code> hostname for the gateway's ingress.</li> <li><code>tlsSecretName:</code> Name of the secret containing the TLS certificate and key for the gateway's ingress.</li> </ul> </li> <li><code>keycloak:</code> Settings for the Keycloak's ingress.<ul> <li><code>host:</code> hostname for the Keycloak's ingress.</li> <li><code>tlsSecretName:</code> Name of the secret containing the TLS certificate and key for the Keycloak's ingress.</li> </ul> </li> </ul> </li> <li><code>components.showbackOpts:</code> Configures the showback feature with the following options:<ul> <li><code>custom:</code> Custom pricing model for showback.</li> <li><code>cloudPricingSecretRef:</code> Secret reference for AWS / Azure Pricing model.</li> <li><code>opencostServiceRoleArn</code>: Role ARN to be used by OpenCost gateway's service account</li> </ul> </li> </ul> <p>Here's an example of how to generate the secrets required to configure MTO:</p> <p>TLS Secret for Ingress:</p> <p>Create a TLS secret containing your SSL/TLS certificate and key for secure communication. This secret will be used for the Console, Gateway, and Keycloak ingresses.</p> <pre><code>kubectl -n multi-tenant-operator create secret tls &lt;tls-secret-name&gt; --key=&lt;path-to-key.pem&gt; --cert=&lt;path-to-cert.pem&gt;\n</code></pre> <p>Integration config will be managing the following resources required for console GUI:</p> <ul> <li><code>Postgresql</code> resources</li> <li><code>Prometheus</code> resources</li> <li><code>Opencost</code> resources</li> <li><code>MTO Console, Gateway, Keycloak</code> resources</li> <li><code>Showback</code> cron-job</li> </ul> <p>Details on console GUI and showback can be found here</p>"},{"location":"kubernetes-resources/integration-config.html#custom-pricing","title":"Custom Pricing","text":"<p>You can modify IntegrationConfig to customise the default pricing model. Here is what you need at <code>IntegrationConfig.spec.components</code>:</p> <pre><code>components:\n    console: true # should be enabled\n    showback: true # should be enabled\n    showbackOpts:\n      # add below and override any default value\n      # you can also remove the ones you do not need\n      custom:\n        CPU: \"0.031611\"\n        spotCPU: \"0.006655\"\n        RAM: \"0.004237\"\n        spotRAM: \"0.000892\"\n        GPU: \"0.95\"\n        storage: \"0.00005479452\"\n        zoneNetworkEgress: \"0.01\"\n        regionNetworkEgress: \"0.01\"\n        internetNetworkEgress: \"0.12\"\n</code></pre> <p>After modifying your default IntegrationConfig in <code>multi-tenant-operator</code> namespace, a configmap named <code>opencost-custom-pricing</code> will be updated. You will be able to see updated pricing info in <code>mto-console</code>.</p>"},{"location":"kubernetes-resources/integration-config.html#azure-pricing","title":"Azure Pricing","text":"<p>MTO supports Azure pricing model via the <code>showbackOpts.cloudIntegrationSecretRef</code> field. Following 2 types of pricing are supported:</p> <ul> <li><code>Azure Standard Pricing</code></li> <li><code>Customer-specific pricing</code></li> </ul> <p>More details on Azure pricing can be found here.</p>"},{"location":"kubernetes-resources/integration-config.html#aws-pricing","title":"AWS Pricing","text":"<p>MTO supports AWS pricing model via the <code>integrationConfig.components.showbackOpts.cloudIntegrationSecretRef</code> field. Following 2 types of pricing are supported:</p> <ul> <li><code>AWS Standard Pricing</code></li> <li><code>AWS Spot Instance Pricing</code></li> </ul> <p>More details on AWS pricing can be found here.</p>"},{"location":"kubernetes-resources/integration-config.html#access-control","title":"Access Control","text":"<pre><code>accessControl:\n  rbac:\n    tenantRoles:\n      default:\n        owner:\n          clusterRoles:\n            - admin\n        editor:\n          clusterRoles:\n            - edit\n        viewer:\n          clusterRoles:\n            - view\n      custom:\n      - labelSelector:\n          matchExpressions:\n          - key: stakater.com/kind\n            operator: In\n            values:\n              - build\n          matchLabels:\n            stakater.com/kind: dev\n        owner:\n          clusterRoles:\n            - custom-owner\n        editor:\n          clusterRoles:\n            - custom-editor\n        viewer:\n          clusterRoles:\n            - custom-view\n  namespaceAccessPolicy:\n    deny:\n      privilegedNamespaces:\n        users:\n          - system:serviceaccount:openshift-argocd:argocd-application-controller\n          - adam@stakater.com\n          groups:\n            - cluster-admins\n  privileged:\n    namespaces:\n      - ^default$\n      - ^openshift.*\n      - ^kube.*\n    serviceAccounts:\n      - ^system:serviceaccount:openshift.*\n      - ^system:serviceaccount:kube.*\n    users:\n      - ''\n    groups:\n      - cluster-admins\n</code></pre>"},{"location":"kubernetes-resources/integration-config.html#rbac","title":"RBAC","text":"<p>RBAC is used to configure the roles that will be applied to each Tenant namespace. The field allows optional custom roles, that are then used to create RoleBindings for namespaces that match a labelSelector.</p>"},{"location":"kubernetes-resources/integration-config.html#tenantroles","title":"TenantRoles","text":"<p>TenantRoles are required within the IntegrationConfig, as they are used for defining what roles will be applied to each Tenant namespace. The field allows optional custom roles, that are then used to create RoleBindings for namespaces that match a labelSelector.</p> <p>\u26a0\ufe0f If you do not configure roles in any way, then the default OpenShift roles of <code>owner</code>, <code>edit</code>, and <code>view</code> will apply to Tenant members. Their details can be found here</p> <pre><code>rbac:\n  tenantRoles:\n    default:\n      owner:\n        clusterRoles:\n          - admin\n      editor:\n        clusterRoles:\n          - edit\n      viewer:\n        clusterRoles:\n          - view\n    custom:\n    - labelSelector:\n        matchExpressions:\n        - key: stakater.com/kind\n          operator: In\n          values:\n            - build\n        matchLabels:\n          stakater.com/kind: dev\n      owner:\n        clusterRoles:\n          - custom-owner\n      editor:\n        clusterRoles:\n          - custom-editor\n      viewer:\n        clusterRoles:\n          - custom-view\n</code></pre>"},{"location":"kubernetes-resources/integration-config.html#default","title":"Default","text":"<p>This field contains roles that will be used to create default <code>roleBindings</code> for each namespace that belongs to tenants. These <code>roleBindings</code> are only created for a namespace if that namespace isn't already matched by the <code>custom</code> field below it. Therefore, it is required to have at least one role mentioned within each of its three subfields: <code>owner</code>, <code>editor</code>, and <code>viewer</code>. These 3 subfields also correspond to the member fields of the Tenant CR</p>"},{"location":"kubernetes-resources/integration-config.html#custom","title":"Custom","text":"<p>An array of custom roles. Similar to the <code>default</code> field, you can mention roles within this field as well. However, the custom roles also require the use of a <code>labelSelector</code> for each iteration within the array. The roles mentioned here will only apply to the namespaces that are matched by the <code>labelSelector</code>. If a namespace is matched by 2 different <code>labelSelectors</code>, then both roles will apply to it. Additionally, roles can be skipped within the <code>labelSelector</code>. These missing roles are then inherited from the <code>default</code> roles field . For example, if the following custom roles arrangement is used:</p> <pre><code>custom:\n- labelSelector:\n    matchExpressions:\n    - key: stakater.com/kind\n      operator: In\n      values:\n        - build\n    matchLabels:\n      stakater.com/kind: dev\n  owner:\n    clusterRoles:\n      - custom-owner\n</code></pre> <p>Then the <code>editor</code> and <code>viewer</code> roles will be taken from the <code>default</code> roles field, as that is required to have at least one role mentioned.</p>"},{"location":"kubernetes-resources/integration-config.html#namespace-access-policy","title":"Namespace Access Policy","text":"<p>Namespace Access Policy is used to configure the namespaces that are allowed to be created by tenants. It also allows the configuration of namespaces that are ignored by MTO.</p> <pre><code>namespaceAccessPolicy:\n  deny:\n    privilegedNamespaces:\n      groups:\n        - cluster-admins\n      users:\n        - system:serviceaccount:openshift-argocd:argocd-application-controller\n        - adam@stakater.com\n  privileged:\n    namespaces:\n      - ^default$\n      - ^openshift.*\n      - ^kube.*\n    serviceAccounts:\n      - ^system:serviceaccount:openshift.*\n      - ^system:serviceaccount:kube.*\n    users:\n      - ''\n    groups:\n      - cluster-admins\n</code></pre>"},{"location":"kubernetes-resources/integration-config.html#deny","title":"Deny","text":"<p><code>namespaceAccessPolicy.Deny:</code> Can be used to restrict privileged users/groups CRUD operation over managed namespaces.</p>"},{"location":"kubernetes-resources/integration-config.html#privileged","title":"Privileged","text":""},{"location":"kubernetes-resources/integration-config.html#namespaces","title":"Namespaces","text":"<p><code>privileged.namespaces:</code> Contains the list of <code>namespaces</code> ignored by MTO. MTO will not manage the <code>namespaces</code> in this list. Treatment for privileged namespaces does not involve further integrations or finalizers processing as with normal namespaces. Values in this list are regex patterns.</p> <p>For example:</p> <ul> <li>To ignore the <code>default</code> namespace, we can specify <code>^default$</code></li> <li>To ignore all namespaces starting with the <code>openshift-</code> prefix, we can specify <code>^openshift-.*</code>.</li> <li>To ignore any namespace containing <code>stakater</code> in its name, we can specify <code>^stakater.</code>. (A constant word given as a regex pattern will match any namespace containing that word.)</li> </ul>"},{"location":"kubernetes-resources/integration-config.html#serviceaccounts","title":"ServiceAccounts","text":"<p><code>privileged.serviceAccounts:</code> Contains the list of <code>ServiceAccounts</code> ignored by MTO. MTO will not manage the <code>ServiceAccounts</code> in this list. Values in this list are regex patterns. For example, to ignore all <code>ServiceAccounts</code> starting with the <code>system:serviceaccount:openshift-</code> prefix, we can use <code>^system:serviceaccount:openshift-.*</code>; and to ignore a specific service account like <code>system:serviceaccount:builder</code> service account we can use <code>^system:serviceaccount:builder$.</code></p> <p>Note</p> <p><code>stakater</code>, <code>stakater.</code> and <code>stakater.*</code> will have the same effect. To check out the combinations, go to Regex101, select Golang, and type your expected regex and test string.</p>"},{"location":"kubernetes-resources/integration-config.html#users","title":"Users","text":"<p><code>privileged.users:</code> Contains the list of <code>users</code> ignored by MTO. MTO will not manage the <code>users</code> in this list. Values in this list are regex patterns.</p>"},{"location":"kubernetes-resources/integration-config.html#groups","title":"Groups","text":"<p><code>privileged.groups:</code> Contains names of the groups that are allowed to perform CRUD operations on namespaces present on the cluster. Users in the specified group(s) will be able to perform these operations without MTO getting in their way. MTO does not interfere even with the deletion of <code>privilegedNamespaces</code>.</p> <p>Note</p> <p>User <code>kube:admin</code> is bypassed by default to perform operations as a cluster admin, this includes operations on all the namespaces.</p> <p>\u26a0\ufe0f If you want to use a more complex regex pattern (for the <code>privileged.namespaces</code> or <code>privileged.serviceAccounts</code> field), it is recommended that you test the regex pattern first -  either locally or using a platform such as https://regex101.com/.</p>"},{"location":"kubernetes-resources/integration-config.html#metadata","title":"Metadata","text":"<pre><code>metadata:\n  groups:\n    labels:\n      role: customer-reader\n    annotations: {}\n  namespaces:\n    labels:\n      stakater.com/workload-monitoring: \"true\"\n    annotations:\n      openshift.io/node-selector: node-role.kubernetes.io/worker=\n  sandboxes:\n    labels:\n      stakater.com/kind: sandbox\n    annotations: {}\n</code></pre>"},{"location":"kubernetes-resources/integration-config.html#namespaces-group-and-sandbox","title":"Namespaces, group and sandbox","text":"<p>We can use the <code>metadata.namespaces</code>, <code>metadata.group</code> and <code>metadata.sandbox</code> fields to automatically add <code>labels</code> and <code>annotations</code> to the Namespaces and Groups managed via MTO.</p> <p>If we want to add default labels/annotations to sandbox namespaces of tenants than we just simply add them in <code>metadata.namespaces.labels</code>/<code>metadata.namespaces.annotations</code> respectively.</p> <p>Whenever a project is made it will have the labels and annotations as mentioned above.</p> <pre><code>kind: Project\napiVersion: project.openshift.io/v1\nmetadata:\n  name: bluesky-build\n  annotations:\n    openshift.io/node-selector: node-role.kubernetes.io/worker=\n  labels:\n    workload-monitoring: 'true'\n    stakater.com/tenant: bluesky\nspec:\n  finalizers:\n    - kubernetes\nstatus:\n  phase: Active\n</code></pre> <pre><code>kind: Group\napiVersion: user.openshift.io/v1\nmetadata:\n  name: bluesky-owner-group\n  labels:\n    role: customer-reader\nusers:\n  - andrew@stakater.com\n</code></pre>"},{"location":"kubernetes-resources/integration-config.html#integrations","title":"Integrations","text":"<p>Integrations are used to configure the integrations that MTO has with other tools. Currently, MTO supports the following integrations:</p> <pre><code>integrations:\n  keycloak:\n    realm: mto\n    address: https://keycloak.apps.prod.abcdefghi.kubeapp.cloud\n    clientName: mto-console\n  argocd:\n    clusterResourceWhitelist:\n      - group: tronador.stakater.com\n        kind: EnvironmentProvisioner\n    namespaceResourceBlacklist:\n      - group: '' # all groups\n        kind: ResourceQuota\n    namespace: openshift-operators\n  vault:\n    enabled: true\n    authMethod: kubernetes      #enum: {kubernetes:default, Token}\n    accessInfo:\n      accessorPath: oidc/\n      address: https://vault.apps.prod.abcdefghi.kubeapp.cloud/\n      roleName: mto\n      secretRef:\n        name: ''\n        namespace: ''\n    config:\n      ssoClient: vault\n</code></pre>"},{"location":"kubernetes-resources/integration-config.html#keycloak","title":"Keycloak","text":"<p>Keycloak is an open-source Identity and Access Management solution aimed at modern applications and services. It makes it easy to secure applications and services with little to no code.</p> <p>If a <code>Keycloak</code> instance is already set up within your cluster, configure it for MTO by enabling the following configuration:</p> <pre><code>keycloak:\n  realm: mto\n  address: https://keycloak.apps.prod.abcdefghi.kubeapp.cloud/\n  clientName: mto-console\n</code></pre> <ul> <li><code>keycloak.realm:</code> The realm in Keycloak where the client is configured.</li> <li><code>keycloak.address:</code> The address of the Keycloak instance.</li> <li><code>keycloak.clientName:</code> The name of the client in Keycloak.</li> </ul> <p>For more details around enabling Keycloak in MTO, visit here</p>"},{"location":"kubernetes-resources/integration-config.html#argocd","title":"ArgoCD","text":"<p>ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It follows the GitOps pattern of using Git repositories as the source of truth for defining the desired application state. ArgoCD uses Kubernetes manifests and configures the applications on the cluster.</p> <p>If <code>argocd</code> is configured on a cluster, then ArgoCD configuration can be enabled.</p> <pre><code>argocd:\n  enabled: bool\n  clusterResourceWhitelist:\n    - group: tronador.stakater.com\n      kind: EnvironmentProvisioner\n  namespaceResourceBlacklist:\n    - group: '' # all groups\n      kind: ResourceQuota\n  namespace: openshift-operators\n</code></pre> <ul> <li><code>argocd.clusterResourceWhitelist</code> allows ArgoCD to sync the listed cluster scoped resources from your GitOps repo.</li> <li><code>argocd.namespaceResourceBlacklist</code> prevents ArgoCD from syncing the listed resources from your GitOps repo.</li> <li><code>argocd.namespace</code> is an optional field used to specify the namespace where ArgoCD Applications and AppProjects are deployed. The field should be populated when you want to create an ArgoCD AppProject for each tenant.</li> </ul>"},{"location":"kubernetes-resources/integration-config.html#vault","title":"Vault","text":"<p>Vault is used to secure, store and tightly control access to tokens, passwords, certificates, encryption keys for protecting secrets and other sensitive data using a UI, CLI, or http API.</p> <p>If <code>vault</code> is configured on a cluster, then Vault configuration can be enabled.</p> <pre><code>vault:\n  enabled: true\n  authMethod: kubernetes      #enum: {kubernetes:default, token}\n  accessInfo:\n    accessorPath: oidc/\n    address: https://vault.apps.prod.abcdefghi.kubeapp.cloud/\n    roleName: mto\n    secretRef:\n      name: ''\n      namespace: ''\n  config:\n    ssoClient: vault\n</code></pre> <p>If enabled, then admins have to specify the <code>authMethod</code> to be used for authentication. MTO supports two authentication methods:</p> <ul> <li><code>kubernetes</code>: This is the default authentication method. It uses the Kubernetes authentication method to authenticate with Vault.</li> <li><code>token</code>: This method uses a Vault token to authenticate with Vault.</li> </ul>"},{"location":"kubernetes-resources/integration-config.html#authmethod-kubernetes","title":"AuthMethod - Kubernetes","text":"<p>If <code>authMethod</code> is set to <code>kubernetes</code>, then admins have to specify the following fields:</p> <ul> <li><code>accessorPath:</code> Accessor Path within Vault to fetch SSO accessorID</li> <li><code>address:</code> Valid Vault address reachable within cluster.</li> <li><code>roleName:</code> Vault's Kubernetes authentication role</li> <li><code>sso.clientName:</code> SSO client name.</li> </ul>"},{"location":"kubernetes-resources/integration-config.html#authmethod-token","title":"AuthMethod - Token","text":"<p>If <code>authMethod</code> is set to <code>token</code>, then admins have to specify the following fields:</p> <ul> <li><code>accessorPath:</code> Accessor Path within Vault to fetch SSO accessorID</li> <li><code>address:</code> Valid Vault address reachable within cluster.</li> <li><code>secretRef:</code> Secret containing Vault token.<ul> <li><code>name:</code> Name of the secret containing Vault token.</li> <li><code>namespace:</code> Namespace of the secret containing Vault token.</li> </ul> </li> </ul> <p>For more details around enabling Kubernetes auth in Vault, visit here</p> <p>The role created within Vault for Kubernetes authentication should have the following permissions:</p> <pre><code>path \"secret/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\npath \"sys/mounts\" {\n  capabilities = [\"read\", \"list\"]\n}\npath \"sys/mounts/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\npath \"managed-addons/*\" {\n  capabilities = [\"read\", \"list\"]\n}\npath \"auth/kubernetes/role/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\npath \"sys/auth\" {\n  capabilities = [\"read\", \"list\"]\n}\npath \"sys/policies/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\npath \"identity/group\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\npath \"identity/group-alias\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\npath \"identity/group/name/*\" {\n  capabilities = [\"read\", \"list\"]\n}\npath \"identity/group/id/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"patch\", \"delete\", \"list\"]\n}\n</code></pre>"},{"location":"kubernetes-resources/integration-config.html#tenantpolicies","title":"TenantPolicies","text":"<p><code>tenantPolicies</code> contains settings to configure additional tenant isolation.</p> <pre><code>tenantPolicies:\n  network:\n    disableIntraTenantNetworking: true\n</code></pre>"},{"location":"kubernetes-resources/integration-config.html#network","title":"Network","text":"<p>Configure how tenants are allowed to communicate</p> <pre><code>network:\n  disableIntraTenantNetworking: true\n</code></pre> <ul> <li><code>disableIntraTenantNetworking</code>: (Default false) Disallow tenants communicating with other tenants by deploying NetworkPolicies.</li> </ul> <p>\u26a0\ufe0f This will disable only intra-tenant networking. In cases requiring stricter filtering, this setting should be turned off and the stricter NetworkPolicies deployed through Templates or manually. If you need help with your enterprise environment, do not hesitate to contact us.</p>"},{"location":"kubernetes-resources/quota.html","title":"Quota","text":"<p>Using Multi Tenant Operator, the cluster-admin can set and enforce cluster resource quotas and limit ranges for tenants.</p>"},{"location":"kubernetes-resources/quota.html#assigning-resource-quotas","title":"Assigning Resource Quotas","text":"<p>Bill is a cluster admin who will first create <code>Quota</code> CR where he sets the maximum resource limits that Anna's tenant will have. Here <code>limitrange</code> is an optional field, cluster admin can skip it if not needed.</p> <pre><code>kubectl create -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta1\nkind: Quota\nmetadata:\n  name: small\nspec:\n  resourcequota:\n    hard:\n      requests.cpu: '5'\n      requests.memory: '5Gi'\n      configmaps: \"10\"\n      secrets: \"10\"\n      services: \"10\"\n      services.loadbalancers: \"2\"\n  limitrange:\n    limits:\n      - type: \"Pod\"\n        max:\n          cpu: \"2\"\n          memory: \"1Gi\"\n        min:\n          cpu: \"200m\"\n          memory: \"100Mi\"\nEOF\n</code></pre> <p>For more details please refer to Quotas.</p> <pre><code>kubectl get quota small\nNAME       STATE    AGE\nsmall      Active   3m\n</code></pre> <p>Bill then proceeds to create a tenant for Anna, while also linking the newly created <code>Quota</code>.</p> <pre><code>kubectl create -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n        - anthony@aurora.org\n  namespaces:\n    sandboxes:\n      enabled: true\nEOF\n</code></pre> <p>Now that the quota is linked with Anna's tenant, Anna can create any resource within the values of resource quota and limit range.</p> <pre><code>kubectl -n bluesky-production create deployment nginx --image nginx:latest --replicas 4\n</code></pre> <p>Once the resource quota assigned to the tenant has been reached, Anna cannot create further resources.</p> <pre><code>kubectl create pods bluesky-training\nError from server (Cannot exceed Namespace quota: please, reach out to the system administrators)\n</code></pre>"},{"location":"kubernetes-resources/quota.html#limiting-persistentvolume-for-tenant","title":"Limiting PersistentVolume for Tenant","text":"<p>Bill, as a cluster admin, wants to restrict the amount of storage a Tenant can use. For that he'll add the <code>requests.storage</code> field to <code>quota.spec.resourcequota.hard</code>. If Bill wants to restrict tenant <code>bluesky</code> to use only <code>50Gi</code> of storage, he'll first create a quota with <code>requests.storage</code> field set to <code>50Gi</code>.</p> <pre><code>kubectl create -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta1\nkind: Quota\nmetadata:\n  name: medium\nspec:\n  resourcequota:\n    hard:\n      requests.cpu: '5'\n      requests.memory: '10Gi'\n      requests.storage: '50Gi'\n</code></pre> <p>Once the quota is created, Bill will create the tenant and set the quota field to the one he created.</p> <pre><code>kubectl create -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n        - anthony@aurora.org\n  namespaces:\n    sandboxes:\n      enabled: true\nEOF\n</code></pre> <p>Now, the combined storage used by all tenant namespaces will not exceed <code>50Gi</code>.</p>"},{"location":"kubernetes-resources/quota.html#adding-storageclass-restrictions-for-tenant","title":"Adding StorageClass Restrictions for Tenant","text":"<p>Now, Bill, as a cluster admin, wants to make sure that no Tenant can provision more than a fixed amount of storage from a StorageClass. Bill can restrict that using <code>&lt;storage-class-name&gt;.storageclass.storage.k8s.io/requests.storage</code> field in <code>quota.spec.resourcequota.hard</code> field. If Bill wants to restrict tenant <code>sigma</code> to use only <code>20Gi</code> of storage from storage class <code>stakater</code>, he'll first create a StorageClass <code>stakater</code> and then create the relevant Quota with <code>stakater.storageclass.storage.k8s.io/requests.storage</code> field set to <code>20Gi</code>.</p> <pre><code>kubectl create -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta1\nkind: Quota\nmetadata:\n  name: small\nspec:\n  resourcequota:\n    hard:\n      requests.cpu: '2'\n      requests.memory: '4Gi'\n      stakater.storageclass.storage.k8s.io/requests.storage: '20Gi'\n</code></pre> <p>Once the quota is created, Bill will create the tenant and set the quota field to the one he created.</p> <pre><code>kubectl create -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n        - anthony@aurora.org\n  namespaces:\n    sandboxes:\n      enabled: true\nEOF\n</code></pre> <p>Now, the combined storage provisioned from StorageClass <code>stakater</code> used by all tenant namespaces will not exceed <code>20Gi</code>.</p> <p>The <code>20Gi</code> limit will only be applied to StorageClass <code>stakater</code>. If a tenant member creates a PVC with some other StorageClass, he will not be restricted.</p> <p>Tip</p> <p>More details about <code>Resource Quota</code> can be found here</p>"},{"location":"kubernetes-resources/resource-supervisor.html","title":"Resource Supervisor","text":"<p>Supports hibernation of <code>deployments</code> and <code>statefulsets</code> for given namespaces via names and/or label selectors. If provided, it will update AppProject instance with SyncWindow to deny sync to selected namespaces.</p> <p>Following namespaces will be ignored:</p> <ul> <li>with annotation <code>\"hibernation.stakater.com/exclude\": \"true\"</code></li> <li>whose name match with privileged namespaces' regex specified in IntegrationConfig</li> <li>namespace where MTO is installed</li> </ul>"},{"location":"kubernetes-resources/resource-supervisor.html#supported-modes","title":"Supported modes","text":""},{"location":"kubernetes-resources/resource-supervisor.html#hibernation-with-cron-schedule","title":"Hibernation with cron schedule","text":"<p>Applications will sleep and wake up at provided cron schedule</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: ResourceSupervisor\nmetadata:\n  name: rs-example1\nspec:\n  argocd:\n    appProjects: []\n    namespace: \"\"\n  namespaces:\n    labelSelector:\n      matchLabels: {}\n      matchExpressions: {}\n    names:\n    - bluesky-dev\n  schedule:\n    sleepSchedule: \"10 * * * *\" # sleep each hour at min 10\n    wakeSchedule: \"50 * * * *\" # wake up each hour at min 50\n</code></pre>"},{"location":"kubernetes-resources/resource-supervisor.html#sleep","title":"Sleep","text":"<p>Applications will sleep instantly, and will wake up when resource supervisor is deleted</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: ResourceSupervisor\nmetadata:\n  name: rs-example2\nspec:\n  argocd:\n    appProjects: []\n    namespace: \"\"\n  namespaces:\n    labelSelector:\n      matchLabels: {}\n      matchExpressions: {}\n    names:\n    - bluesky-dev\n  schedule: {}\n</code></pre>"},{"location":"kubernetes-resources/resource-supervisor.html#sleep-at-given-cron-schedule","title":"Sleep at given cron schedule","text":"<p>Applications will sleep at provided cron schedule, and will wake up when resource supervisor is deleted</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: ResourceSupervisor\nmetadata:\n  name: rs-example3\nspec:\n  argocd:\n    appProjects: []\n    namespace: \"\"\n  namespaces:\n    labelSelector:\n      matchLabels: {}\n      matchExpressions: {}\n    names:\n    - bluesky-dev\n  schedule:\n    sleepSchedule: \"0 0 1 2 2025\" # sleep on 1st February 2025\n</code></pre>"},{"location":"kubernetes-resources/resource-supervisor.html#more-examples","title":"More examples","text":""},{"location":"kubernetes-resources/resource-supervisor.html#example-1","title":"Example 1","text":"<p>labelSelector's <code>matchLabels</code> and <code>matchExpressions</code> is <code>AND</code> operation. Here is an example with it:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: ResourceSupervisor\nmetadata:\n  name: rs-example4\nspec:\n  argocd:\n    appProjects: []\n    namespace: \"\"\n  namespaces:\n    labelSelector:\n      matchLabels:\n        stakater.com/current-tenant: bluesky\n        stakater.com/kind: dev\n      matchExpressions:\n       - { key: \"private-sandbox\", operator: In , values: [\"true\"] }\n    names:\n    - bluesky-staging\n  schedule:\n    sleepSchedule: \"\"\n</code></pre> <p>It will sleep <code>bluesky-staging</code> namespace, and all those which have the specified labels.</p>"},{"location":"kubernetes-resources/resource-supervisor.html#example-2","title":"Example 2","text":"<p>If you provide Argo CD AppProject in spec, it will create <code>syncWindow</code> with <code>deny</code> policy</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: ResourceSupervisor\nmetadata:\n  name: rs-example4\nspec:\n  argocd:\n    appProjects:\n      - dev-apps\n      - dev-apps2\n    namespace: \"customer-argocd-projects\"\n  namespaces:\n    labelSelector:\n      matchLabels: {}\n      matchExpressions: {}\n    names:\n    - bluesky-staging\n    - bluesky-dev\n  schedule:\n    sleepSchedule: \"\"\n</code></pre> <p>It will sleep given namespaces, and create <code>deny</code> <code>syncWindow</code> on provided AppProjects</p>"},{"location":"kubernetes-resources/template/template-group-instance.html","title":"TemplateGroupInstance","text":"<p>Cluster scoped resource:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateGroupInstance\nmetadata:\n  name: namespace-parameterized-restrictions-tgi\nspec:\n  template: namespace-parameterized-restrictions\n  sync: true\n  selector:\n    matchExpressions:\n    - key: stakater.com/tenant\n      operator: In\n      values:\n        - alpha\n        - beta\nparameters:\n  - name: CIDR_IP\n    value: \"172.17.0.0/16\"\n</code></pre> <p>TemplateGroupInstance distributes a template across multiple namespaces which are selected by labelSelector.</p>"},{"location":"kubernetes-resources/template/template-instance.html","title":"TemplateInstance","text":"<p>Namespace scoped resource:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateInstance\nmetadata:\n  name: networkpolicy\n  namespace: build\nspec:\n  template: networkpolicy\n  sync: true\nparameters:\n  - name: CIDR_IP\n    value: \"172.17.0.0/16\"\n</code></pre> <p>TemplateInstance are used to keep track of resources created from Templates, which are being instantiated inside a Namespace. Generally, a TemplateInstance is created from a Template and then the TemplateInstances will not be updated when the Template changes later on. To change this behavior, it is possible to set <code>spec.sync: true</code> in a TemplateInstance. Setting this option, means to keep this TemplateInstance in sync with the underlying template (similar to Helm upgrade).</p>"},{"location":"kubernetes-resources/template/template.html","title":"Template","text":"<p>Templates are used to initialize Namespaces, share common resources across namespaces, and map secrets/configmaps from one namespace to other namespaces.</p> <p>They can contain pre-defined parameters such as <code>${namespace}</code>/<code>${tenant}</code>.</p> <p>Also, you can define custom variables in <code>Template</code>, <code>TemplateInstance</code> and <code>TemplateGroupInstance</code>. The parameters defined in <code>Templates</code> are overwritten the values defined in <code>TemplateInstance</code> and <code>TemplateGroupInstance</code>.</p>"},{"location":"kubernetes-resources/template/template.html#specification","title":"Specification","text":"<p><code>Template</code> Custom Resource (CR) supports three key methods for defining and managing resources: <code>manifests</code>, <code>helm</code>, and <code>resource mapping</code>. Let\u2019s dive into each method, their differences, and their use cases:</p>"},{"location":"kubernetes-resources/template/template.html#1-manifests","title":"1. Manifests","text":"<p>This approach uses raw Kubernetes manifests (YAML files) that specify resources directly in the template.</p>"},{"location":"kubernetes-resources/template/template.html#how-it-works","title":"How It Works","text":"<ul> <li>The template includes the actual YAML specifications of resources like <code>Deployment</code>, <code>Service</code>, <code>ConfigMap</code>, etc.</li> <li>These manifests are applied as-is or with minor parameter substitutions (e.g., dynamically populated <code>{tenant}</code> and <code>{namespace}</code> variables wherever added or user defined parameters).</li> </ul>"},{"location":"kubernetes-resources/template/template.html#use-cases","title":"Use Cases","text":"<ul> <li>Best for straightforward and simple resources where you don't need advanced templating logic or dependency management.</li> <li>Ideal when the resource definitions are static or have minimal customization needs.</li> </ul>"},{"location":"kubernetes-resources/template/template.html#example","title":"Example","text":"<pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: networkpolicy\nparameters:\n  - name: CIDR_IP\n    value: \"172.17.0.0/16\"\nresources:\n  manifests:\n    - kind: NetworkPolicy\n      apiVersion: networking.k8s.io/v1\n      metadata:\n        name: deny-cross-ns-traffic\n      spec:\n        podSelector:\n          matchLabels:\n            role: db\n        policyTypes:\n        - Ingress\n        - Egress\n        ingress:\n        - from:\n          - ipBlock:\n              cidr: \"${{CIDR_IP}}\"\n              except:\n              - 172.17.1.0/24\n          - namespaceSelector:\n              matchLabels:\n                project: myproject\n          - podSelector:\n              matchLabels:\n                role: frontend\n          ports:\n          - protocol: TCP\n            port: 6379\n        egress:\n        - to:\n          - ipBlock:\n              cidr: 10.0.0.0/24\n          ports:\n          - protocol: TCP\n            port: 5978\n</code></pre>"},{"location":"kubernetes-resources/template/template.html#2-helm","title":"2. Helm","text":"<p>This method integrates Helm charts into the template, allowing you to leverage Helm's templating capabilities and package management.</p>"},{"location":"kubernetes-resources/template/template.html#how-it-works_1","title":"How It Works","text":"<ul> <li>The <code>Template</code> references a Helm chart.</li> <li>Values for the Helm chart can be passed by the <code>values</code> field.</li> <li>The Helm chart generates the necessary Kubernetes resources dynamically at runtime.</li> </ul>"},{"location":"kubernetes-resources/template/template.html#use-cases_1","title":"Use Cases","text":"<ul> <li>Best for complex resource setups with interdependencies (e.g., a microservice with a Deployment, Service, Ingress, and Configmap).</li> <li>Useful for resources requiring advanced templating logic or modular packaging.</li> <li>Great for managing third-party tools or applications (e.g., deploying Prometheus, Keycloak, or databases).</li> </ul>"},{"location":"kubernetes-resources/template/template.html#example_1","title":"Example","text":"<pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: redis\nresources:\n  helm:\n    releaseName: redis\n    chart:\n      repository:\n        name: redis\n        version: 14.6.0\n        repoUrl: https://charts.bitnami.com/bitnami\n        username:\n          key: username\n          name: redis-creds\n          namespace: namespace-n1\n        password:\n          key: password\n          name: redis-creds\n          namespace: namespace-n1\n    setValues:\n      - name: port\n        value: '6379'\n        forceString: false\n    values: |\n      redisPort: 6379\n</code></pre> <p>A brief explanation of the fields in the Helm section:</p> <ul> <li><code>releaseName</code>: The name of the Helm release.</li> <li><code>chart</code>: The Helm chart details.<ul> <li><code>repository</code>: The Helm repository details.<ul> <li><code>name</code>: The name of the Helm repository.</li> <li><code>version</code>: The version of the Helm chart.</li> <li><code>repoUrl</code>: The URL of the Helm repository.</li> </ul> </li> <li><code>username</code>: A reference to the secret containing the username for the Helm repository in case the chart is in a private repository.<ul> <li><code>key</code>: The key in the secret containing the username.</li> <li><code>name</code>: The name of the secret containing the username.</li> <li><code>namespace</code>: The namespace of the secret containing the username.</li> </ul> </li> <li><code>password</code>: A reference to the secret containing the password for the Helm repository in case the chart is in a private repository.<ul> <li><code>key</code>: The key in the secret containing the password.</li> <li><code>name</code>: The name of the secret containing the password.</li> <li><code>namespace</code>: The namespace of the secret containing the password.</li> </ul> </li> </ul> </li> <li><code>setValues</code>: The values to set in the Helm chart.<ul> <li><code>name</code>: The name of the value.</li> <li><code>value</code>: The value to set.</li> <li><code>forceString</code>: Whether to use <code>--set</code> or <code>--set-string</code> when setting the value. Default is <code>false</code> (use <code>--set</code>).</li> </ul> </li> <li><code>values</code>: The values file for the Helm chart.</li> </ul>"},{"location":"kubernetes-resources/template/template.html#3-resource-mapping","title":"3. Resource Mapping","text":"<p>This approach maps secrets and configmaps from one tenant's namespace to another tenant's namespace, or within a tenant's namespace.</p>"},{"location":"kubernetes-resources/template/template.html#how-it-works_2","title":"How It Works","text":"<ul> <li>The template contains mappings to pre-existing resources (secrets and configmaps only).</li> </ul>"},{"location":"kubernetes-resources/template/template.html#use-cases_2","title":"Use Cases","text":"<ul> <li>Ideal for maintaining consistency across shared resources without duplicating definitions.</li> <li>Best when resources already exist.</li> </ul>"},{"location":"kubernetes-resources/template/template.html#example_2","title":"Example","text":"<pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: resource-mapping\nresources:\n  resourceMappings:\n    secrets:\n      - name: secret-s1\n        namespace: namespace-n1\n    configMaps:\n      - name: configmap-c1\n        namespace: namespace-n2\n</code></pre>"},{"location":"kubernetes-resources/template/how-to-guides/configuring-multitenant-network-isolation.html","title":"Configuring Multi-Tenant Isolation with Network Policy Template","text":"<p>Bill is a cluster admin who wants to configure network policies to provide multi-tenant network isolation.</p> <p>First, Bill creates a template for network policies:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: tenant-network-policy\nresources:\n  manifests:\n  - apiVersion: networking.k8s.io/v1\n    kind: NetworkPolicy\n    metadata:\n      name: allow-same-namespace\n    spec:\n      podSelector: {}\n      ingress:\n      - from:\n        - podSelector: {}\n  - apiVersion: networking.k8s.io/v1\n    kind: NetworkPolicy\n    metadata:\n      name: allow-from-openshift-monitoring\n    spec:\n      ingress:\n      - from:\n        - namespaceSelector:\n            matchLabels:\n              network.openshift.io/policy-group: monitoring\n      podSelector: {}\n      policyTypes:\n      - Ingress\n  - apiVersion: networking.k8s.io/v1\n    kind: NetworkPolicy\n    metadata:\n      name: allow-from-openshift-ingress\n    spec:\n      ingress:\n      - from:\n        - namespaceSelector:\n            matchLabels:\n              network.openshift.io/policy-group: ingress\n      podSelector: {}\n      policyTypes:\n      - Ingress\n</code></pre> <p>Once the template has been created, Bill edits the IntegrationConfig to add unique label to all tenant projects:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: IntegrationConfig\nmetadata:\n  name: tenant-operator-config\n  namespace: multi-tenant-operator\nspec:\n  metadata:\n    namespaces:\n      labels:\n        stakater.com/workload-monitoring: \"true\"\n        tenant-network-policy: \"true\"\n      annotations:\n        openshift.io/node-selector: node-role.kubernetes.io/worker=\n    sandbox:\n      labels:\n        stakater.com/kind: sandbox\n  privileged:\n    namespaces:\n      - default\n      - ^openshift.*\n      - ^kube.*\n    serviceAccounts:\n      - ^system:serviceaccount:openshift.*\n      - ^system:serviceaccount:kube.*\n</code></pre> <p>Bill has added a new label <code>tenant-network-policy: \"true\"</code> in project section of IntegrationConfig, now MTO will add that label in all tenant projects.</p> <p>Finally, Bill creates a <code>TemplateGroupInstance</code> which will distribute the network policies using the newly added project label and template.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateGroupInstance\nmetadata:\n  name: tenant-network-policy-group\nspec:\n  template: tenant-network-policy\n  selector:\n    matchLabels:\n      tenant-network-policy: \"true\"\n  sync: true\n</code></pre> <p>MTO will now deploy the network policies mentioned in <code>Template</code> to all projects matching the label selector mentioned in the TemplateGroupInstance.</p>"},{"location":"kubernetes-resources/template/how-to-guides/copying-resources-2.html","title":"Copying Secrets and Configmaps across Tenant Namespaces via TGI","text":"<p>Bill is a cluster admin who wants to map a <code>docker-pull-secret</code>, present in a <code>build</code> namespace, in tenant namespaces where certain labels exists.</p> <p>First, Bill creates a template:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: docker-pull-secret\nresources:\n  resourceMappings:\n    secrets:\n      - name: docker-pull-secret\n        namespace: build\n</code></pre> <p>Once the template has been created, Bill makes a <code>TemplateGroupInstance</code> referring to the <code>Template</code> he wants to deploy with <code>MatchLabels</code>:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateGroupInstance\nmetadata:\n  name: docker-secret-group-instance\nspec:\n  template: docker-pull-secret\n  selector:\n    matchLabels:\n      kind: build\n  sync: true\n</code></pre> <p>Afterward, Bill can see that secrets has been successfully mapped in all matching namespaces.</p> <pre><code>kubectl get secret docker-pull-secret -n bluesky-anna-aurora-sandbox\nNAME             STATE    AGE\ndocker-pull-secret    Active   3m\n\nkubectl get secret docker-pull-secret -n alpha-dave-aurora-sandbox\nNAME             STATE    AGE\ndocker-pull-secret    Active   3m\n</code></pre>"},{"location":"kubernetes-resources/template/how-to-guides/copying-resources-2.html#mapping-resources-within-tenant-namespaces-via-ti","title":"Mapping Resources within Tenant Namespaces via TI","text":"<p>Anna is a tenant owner who wants to map a <code>docker-pull-secret</code>, present in <code>bluseky-build</code> namespace, to <code>bluesky-anna-aurora-sandbox</code> namespace.</p> <p>First, Bill creates a template:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: docker-pull-secret\nresources:\n  resourceMappings:\n    secrets:\n      - name: docker-pull-secret\n        namespace: bluesky-build\n</code></pre> <p>Once the template has been created, Anna creates a <code>TemplateInstance</code> in <code>bluesky-anna-aurora-sandbox</code> namespace, referring to the <code>Template</code>.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateInstance\nmetadata:\n  name: docker-secret-instance\n  namespace: bluesky-anna-aurora-sandbox\nspec:\n  template: docker-pull-secret\n  sync: true\n</code></pre> <p>Afterward, Bill can see that secrets has been successfully mapped in all matching namespaces.</p> <pre><code>kubectl get secret docker-pull-secret -n bluesky-anna-aurora-sandbox\nNAME             STATE    AGE\ndocker-pull-secret    Active   3m\n</code></pre>"},{"location":"kubernetes-resources/template/how-to-guides/copying-resources.html","title":"Propagate Secrets from Parent to Descendant namespaces","text":"<p>Secrets like <code>registry</code> credentials often need to exist in multiple Namespaces, so that Pods within different namespaces can have access to those credentials in form of secrets.</p> <p>Manually creating secrets within different namespaces could lead to challenges, such as:</p> <ul> <li>Someone will have to create secret either manually or via GitOps each time there is a new descendant namespace that needs the secret</li> <li>If we update the parent secret, they will have to update the secret in all descendant namespaces</li> <li>This could be time-consuming, and a small mistake while creating or updating the secret could lead to unnecessary debugging</li> </ul> <p>With the help of Multi-Tenant Operator's Template feature we can make this secret distribution experience easy.</p> <p>For example, to copy a Secret called <code>registry</code> which exists in the <code>example</code> to new Namespaces whenever they are created, we will first create a Template which will have reference of the registry secret.</p> <p>It will also push updates to the copied Secrets and keep the propagated secrets always sync and updated with parent namespaces.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: registry-secret\nresources:\n  resourceMappings:\n    secrets:\n      - name: registry\n        namespace: example\n</code></pre> <p>Now using this Template we can propagate registry secret to different namespaces that have some common set of labels.</p> <p>For example, will just add one label <code>kind: registry</code> and all namespaces with this label will get this secret.</p> <p>For propagating it on different namespaces dynamically will have to create another resource called <code>TemplateGroupInstance</code>. <code>TemplateGroupInstance</code> will have <code>Template</code> and <code>matchLabel</code> mapping as shown below:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateGroupInstance\nmetadata:\n  name: registry-secret-group-instance\nspec:\n  template: registry-secret\n  selector:\n    matchLabels:\n      kind: registry\n  sync: true\n</code></pre> <p>After reconciliation, you will be able to see those secrets in namespaces having mentioned label.</p> <p>MTO will keep injecting this secret to the new namespaces created with that label.</p> <pre><code>kubectl get secret registry-secret -n example-ns-1\nNAME             STATE    AGE\nregistry-secret    Active   3m\n\nkubectl get secret registry-secret -n example-ns-2\nNAME             STATE    AGE\nregistry-secret    Active   3m\n</code></pre>"},{"location":"kubernetes-resources/template/how-to-guides/deploying-private-helm-charts.html","title":"Deploying Private Helm Chart to Multiple Namespaces","text":"<p>Multi Tenant Operator uses its <code>helm</code> functionality from <code>Template</code> and <code>TemplateGroupInstance</code> to deploy private and public charts to multiple namespaces.</p>"},{"location":"kubernetes-resources/template/how-to-guides/deploying-private-helm-charts.html#deploying-helm-chart-to-namespaces-via-templategroupinstances-from-oci-registry","title":"Deploying Helm Chart to Namespaces via TemplateGroupInstances from OCI Registry","text":"<p>Bill, the cluster admin, wants to deploy a helm chart from <code>OCI</code> registry in namespaces where certain labels exists.</p> <p>First, Bill creates a template:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: chart-deploy\nresources:\n  helm:\n    releaseName: random-release\n    chart:\n      repository:\n        name: random-chart\n        repoUrl: 'oci://ghcr.io/stakater/charts/random-chart'\n        version: 0.0.15\n        password:\n          key: password\n          name: repo-user\n          namespace: shared-ns\n        username:\n          key: username\n          name: repo-user\n          namespace: shared-ns\n</code></pre> <p>Once the template has been created, Bill makes a <code>TemplateGroupInstance</code> referring to the <code>Template</code> he wants to deploy with <code>MatchLabels</code>:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateGroupInstance\nmetadata:\n  name: chart-deploy\nspec:\n  selector:\n    matchExpressions:\n      - key: stakater.com/kind\n        operator: In\n        values:\n          - system\n  sync: true\n  template: chart-deploy\n</code></pre> <p>Multi Tenant Operator will pick up the credentials from the mentioned namespace to pull the chart and apply it.</p> <p>Afterward, Bill can see that manifests in the chart have been successfully created in all label matching namespaces.</p>"},{"location":"kubernetes-resources/template/how-to-guides/deploying-private-helm-charts.html#deploying-helm-chart-to-namespaces-via-templategroupinstances-from-https-registry","title":"Deploying Helm Chart to Namespaces via TemplateGroupInstances from <code>HTTPS</code> Registry","text":"<p>Bill, the cluster admin, wants to deploy a helm chart from <code>HTTPS</code> registry in namespaces where certain labels exists.</p> <p>First, Bill creates a template:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: chart-deploy\nresources:\n  helm:\n    releaseName: random-release\n    chart:\n      repository:\n        name: random-chart\n        repoUrl: 'nexus-helm-url/registry'\n        version: 0.0.15\n        password:\n          key: password\n          name: repo-user\n          namespace: shared-ns\n        username:\n          key: username\n          name: repo-user\n          namespace: shared-ns\n</code></pre> <p>Once the template has been created, Bill makes a <code>TemplateGroupInstance</code> referring to the <code>Template</code> he wants to deploy with <code>MatchLabels</code>:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateGroupInstance\nmetadata:\n  name: chart-deploy\nspec:\n  selector:\n    matchExpressions:\n      - key: stakater.com/kind\n        operator: In\n        values:\n          - system\n  sync: true\n  template: chart-deploy\n</code></pre> <p>Multi Tenant Operator will pick up the credentials from the mentioned namespace to pull the chart and apply it.</p> <p>Afterward, Bill can see that manifests in the chart have been successfully created in all label matching namespaces.</p>"},{"location":"kubernetes-resources/template/how-to-guides/deploying-templates.html","title":"Distributing Resources in Namespaces","text":"<p>Multi Tenant Operator has two Custom Resources which can cover this need using the <code>Template</code> CR, depending upon the conditions and preference.</p> <ol> <li>TemplateGroupInstance</li> <li>TemplateInstance</li> </ol>"},{"location":"kubernetes-resources/template/how-to-guides/deploying-templates.html#deploying-template-to-namespaces-via-templategroupinstances","title":"Deploying Template to Namespaces via TemplateGroupInstances","text":"<p>Bill, the cluster admin, wants to deploy a docker pull secret in namespaces where certain labels exists.</p> <p>First, Bill creates a template:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: docker-secret\nresources:\n  manifests:\n    - kind: Secret\n      apiVersion: v1\n      metadata:\n        name: docker-pull-secret\n      data:\n        .dockercfg: eyAKICAiaHR0cHM6IC8vaW5kZXguZG9ja2VyLmlvL3YxLyI6IHsgImF1dGgiOiAiYzNSaGEyRjBaWEk2VjI5M1YyaGhkRUZIY21WaGRGQmhjM04zYjNKayJ9Cn0K\n      type: kubernetes.io/dockercfg\n</code></pre> <p>Once the template has been created, Bill makes a <code>TemplateGroupInstance</code> referring to the <code>Template</code> he wants to deploy with <code>MatchLabels</code>:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateGroupInstance\nmetadata:\n  name: docker-secret-group-instance\nspec:\n  template: docker-secret\n  selector:\n    matchLabels:\n      kind: build\n  sync: true\n</code></pre> <p>Afterward, Bill can see that secrets have been successfully created in all label matching namespaces.</p> <pre><code>kubectl get secret docker-secret -n bluesky-anna-aurora-sandbox\nNAME             STATE    AGE\ndocker-secret    Active   3m\n\nkubectl get secret docker-secret -n alpha-dave-aurora-sandbox\nNAME             STATE    AGE\ndocker-secret    Active   2m\n</code></pre> <p><code>TemplateGroupInstance</code> can also target specific tenants or all tenant namespaces under a single YAML definition.</p>"},{"location":"kubernetes-resources/template/how-to-guides/deploying-templates.html#templategroupinstance-for-multiple-tenants","title":"TemplateGroupInstance for multiple Tenants","text":"<p>It can be done by using the <code>matchExpressions</code> field, dividing the tenant label in key and values.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateGroupInstance\nmetadata:\n  name: docker-secret-group-instance\nspec:\n  template: docker-secret\n  selector:\n    matchExpressions:\n    - key: stakater.com/tenant\n      operator: In\n      values:\n        - alpha\n        - beta\n  sync: true\n</code></pre>"},{"location":"kubernetes-resources/template/how-to-guides/deploying-templates.html#templategroupinstance-for-all-tenants","title":"TemplateGroupInstance for all Tenants","text":"<p>This can also be done by using the <code>matchExpressions</code> field, using just the tenant label key <code>stakater.com/tenant</code>.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateGroupInstance\nmetadata:\n  name: docker-secret-group-instance\nspec:\n  template: docker-secret\n  selector:\n    matchExpressions:\n    - key: stakater.com/tenant\n      operator: Exists\n  sync: true\n</code></pre>"},{"location":"kubernetes-resources/template/how-to-guides/deploying-templates.html#deploying-template-to-a-namespace-via-templateinstance","title":"Deploying Template to a Namespace via TemplateInstance","text":"<p>Anna wants to deploy a docker pull secret in her namespace.</p> <p>First Anna asks Bill, the cluster admin, to create a template of the secret for her:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: docker-pull-secret\nresources:\n  manifests:\n    - kind: Secret\n      apiVersion: v1\n      metadata:\n        name: docker-pull-secret\n      data:\n        .dockercfg: eyAKICAiaHR0cHM6IC8vaW5kZXguZG9ja2VyLmlvL3YxLyI6IHsgImF1dGgiOiAiYzNSaGEyRjBaWEk2VjI5M1YyaGhkRUZIY21WaGRGQmhjM04zYjNKayJ9Cn0K\n      type: kubernetes.io/dockercfg\n</code></pre> <p>Once the template has been created, Anna creates a <code>TemplateInstance</code> in her namespace referring to the <code>Template</code> she wants to deploy:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateInstance\nmetadata:\n  name: docker-pull-secret-instance\n  namespace: bluesky-anna-aurora-sandbox\nspec:\n  template: docker-pull-secret\n  sync: true\n</code></pre> <p>Once this is created, Anna can see that the secret has been successfully applied.</p> <pre><code>kubectl get secret docker-secret -n bluesky-anna-aurora-sandbox\nNAME                  STATE    AGE\ndocker-pull-secret    Active   3m\n</code></pre>"},{"location":"kubernetes-resources/template/how-to-guides/deploying-templates.html#passing-parameters-to-template-via-templateinstance-templategroupinstance","title":"Passing Parameters to Template via TemplateInstance, TemplateGroupInstance","text":"<p>Anna wants to deploy a LimitRange resource to certain namespaces.</p> <p>First Anna asks Bill, the cluster admin, to create template with parameters for LimitRange for her:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: namespace-parameterized-restrictions\nparameters:\n  # Name of the parameter\n  - name: DEFAULT_CPU_LIMIT\n    # The default value of the parameter\n    value: \"1\"\n  - name: DEFAULT_CPU_REQUESTS\n    value: \"0.5\"\n    # If a parameter is required the template instance will need to set it\n    # required: true\n    # Make sure only values are entered for this parameter\n    validation: \"^[0-9]*\\\\.?[0-9]+$\"\nresources:\n  manifests:\n    - apiVersion: v1\n      kind: LimitRange\n      metadata:\n        name: namespace-limit-range-${namespace}\n      spec:\n        limits:\n          - default:\n              cpu: \"${{DEFAULT_CPU_LIMIT}}\"\n            defaultRequest:\n              cpu: \"${{DEFAULT_CPU_REQUESTS}}\"\n            type: Container\n</code></pre> <p>Afterward, Anna creates a <code>TemplateInstance</code> in her namespace referring to the <code>Template</code> she wants to deploy:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateInstance\nmetadata:\n  name: namespace-parameterized-restrictions-instance\n  namespace: bluesky-anna-aurora-sandbox\nspec:\n  template: namespace-parameterized-restrictions\n  sync: true\nparameters:\n  - name: DEFAULT_CPU_LIMIT\n    value: \"1.5\"\n  - name: DEFAULT_CPU_REQUESTS\n    value: \"1\"\n</code></pre> <p>If she wants to distribute the same Template over multiple namespaces, she can use <code>TemplateGroupInstance</code>.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateGroupInstance\nmetadata:\n  name: namespace-parameterized-restrictions-tgi\nspec:\n  template: namespace-parameterized-restrictions\n  sync: true\n  selector:\n    matchExpressions:\n    - key: stakater.com/tenant\n      operator: In\n      values:\n        - alpha\n        - beta\nparameters:\n  - name: DEFAULT_CPU_LIMIT\n    value: \"1.5\"\n  - name: DEFAULT_CPU_REQUESTS\n    value: \"1\"\n</code></pre>"},{"location":"kubernetes-resources/template/how-to-guides/distributing-manifests.html","title":"Distributing Resources in Namespaces","text":"<p>Multi Tenant Operator has two Custom Resources which can cover this need using the <code>Template</code> CR, depending upon the conditions and preference.</p> <ol> <li>TemplateGroupInstance</li> <li>TemplateInstance</li> </ol>"},{"location":"kubernetes-resources/template/how-to-guides/distributing-manifests.html#deploying-template-to-namespaces-via-templategroupinstances","title":"Deploying Template to Namespaces via TemplateGroupInstances","text":"<p>Bill, the cluster admin, wants to deploy a docker pull secret in namespaces where certain labels exists.</p> <p>First, Bill creates a template:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: docker-secret\nresources:\n  manifests:\n    - kind: Secret\n      apiVersion: v1\n      metadata:\n        name: docker-pull-secret\n      data:\n        .dockercfg: eyAKICAiaHR0cHM6IC8vaW5kZXguZG9ja2VyLmlvL3YxLyI6IHsgImF1dGgiOiAiYzNSaGEyRjBaWEk2VjI5M1YyaGhkRUZIY21WaGRGQmhjM04zYjNKayJ9Cn0K\n      type: kubernetes.io/dockercfg\n</code></pre> <p>Once the template has been created, Bill makes a <code>TemplateGroupInstance</code> referring to the <code>Template</code> he wants to deploy with <code>template</code> field, and the namespaces where resources are needed, using <code>selector</code> field:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateGroupInstance\nmetadata:\n  name: docker-secret-group-instance\nspec:\n  template: docker-pull-secret\n  selector:\n    matchExpressions:\n    - key: kind\n      operator: In\n      values:\n        - build\n  sync: true\n</code></pre> <p>Afterward, Bill can see that secrets have been successfully created in all label matching namespaces.</p> <pre><code>kubectl get secret docker-secret -n bluesky-anna-aurora-sandbox\nNAME             STATE    AGE\ndocker-secret    Active   3m\n\nkubectl get secret docker-secret -n alpha-dave-aurora-sandbox\nNAME             STATE    AGE\ndocker-secret    Active   2m\n</code></pre> <p><code>TemplateGroupInstance</code> can also target specific tenants or all tenant namespaces under a single YAML definition.</p>"},{"location":"kubernetes-resources/template/how-to-guides/distributing-secrets-using-sealed-secret-template.html","title":"Distributing Secrets Using Sealed Secrets Template","text":"<p>Bill is a cluster admin who wants to provide a mechanism for distributing secrets in multiple namespaces. For this, he wants to use Sealed Secrets as the solution by adding them to MTO Template CR</p> <p>First, Bill creates a Template in which Sealed Secret is mentioned:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: tenant-sealed-secret\nresources:\n  manifests:\n  - kind: SealedSecret\n    apiVersion: bitnami.com/v1alpha1\n    metadata:\n      name: mysecret\n    spec:\n      encryptedData:\n        .dockerconfigjson: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEq.....\n      template:\n        type: kubernetes.io/dockerconfigjson\n        # this is an example of labels and annotations that will be added to the output secret\n        metadata:\n          labels:\n            \"jenkins.io/credentials-type\": usernamePassword\n          annotations:\n            \"jenkins.io/credentials-description\": credentials from Kubernetes\n</code></pre> <p>Once the template has been created, Bill has to edit the <code>Tenant</code> to add unique label to namespaces in which the secret has to be deployed. For this, he can use the support for common and specific labels across namespaces.</p> <p>Bill has to specify a label on namespaces in which he needs the secret. He can add it to all namespaces inside a tenant or some specific namespaces depending on the use case.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n        - anthony@aurora.org\n    editors:\n      users:\n        - john@aurora.org\n      groups:\n        - alpha\n  namespaces:\n    sandboxes:\n      enabled: false\n    withTenantPrefix:\n      - dev\n      - build\n      - prod\n    withoutTenantPrefix: []\n    metadata:\n      specific:\n        - namespaces:\n            - bluesky-test-namespace\n          labels:\n            distribute-image-pull-secret: true\n      common:\n        labels:\n          distribute-image-pull-secret: true\n</code></pre> <p>Bill has added support for a new label <code>distribute-image-pull-secret: true\"</code> for tenant projects/namespaces, now MTO will add that label depending on the used field.</p> <p>Finally, Bill creates a <code>TemplateGroupInstance</code> which will deploy the Sealed Secrets using the newly created project label and template:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateGroupInstance\nmetadata:\n  name: tenant-sealed-secret\nspec:\n  template: tenant-sealed-secret\n  selector:\n    matchLabels:\n      distribute-image-pull-secret: true\n  sync: true\n</code></pre> <p>MTO will now deploy the Sealed Secrets mentioned in <code>Template</code> to namespaces which have the mentioned label. The rest of the work to deploy secret from a Sealed Secret has to be done by Sealed Secrets Controller.</p>"},{"location":"kubernetes-resources/template/how-to-guides/resource-sync-by-tgi.html","title":"Sync Resources Deployed by TemplateGroupInstance","text":"<p>The TemplateGroupInstance CR provides two types of resource sync for the resources mentioned in Template</p> <p>For the given example, let's consider we want to apply the following template</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: docker-secret\nresources:\n  manifests:\n\n    - kind: Secret\n      apiVersion: v1\n      metadata:\n        name: docker-pull-secret\n      data:\n        .dockercfg: eyAKICAiaHR0cHM6IC8vaW5kZXguZG9ja2VyLmlvL3YxLyI6IHsgImF1dGgiOiAiYzNSaGEyRjBaWEk2VjI5M1YyaGhkRUZIY21WaGRGQmhjM04zYjNKayJ9Cn0K\n      type: kubernetes.io/dockercfg\n\n    - apiVersion: v1\n      kind: ServiceAccount\n      metadata:\n        name: example-automated-thing\n      secrets:\n        - name: example-automated-thing-token-zyxwv\n</code></pre> <p>And the following TemplateGroupInstance is used to deploy these resources to namespaces having label <code>kind: build</code></p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: TemplateGroupInstance\nmetadata:\n  name: docker-secret-group-instance\nspec:\n  template: docker-secret\n  selector:\n    matchLabels:\n      kind: build\n  sync: true\n</code></pre> <p>As we can see, in our TGI, we have a field <code>spec.sync</code> which is set to <code>true</code>. This will update the resources on two conditions:</p> <ul> <li>The Template CR is updated</li> <li> <p>The TemplateGroupInstance CR is reconciled/updated</p> </li> <li> <p>If, for any reason, the underlying resource gets updated or deleted, <code>TemplateGroupInstance</code> CR will try to revert it back to the state mentioned in the <code>Template</code> CR.</p> </li> </ul> <p>Note</p> <p>Updates to ServiceAccounts are ignored by both, reconciler and informers, in an attempt to avoid conflict between the TGI controller and Kube Controller Manager. ServiceAccounts are only reverted in case of unexpected deletions when sync is true.</p>"},{"location":"kubernetes-resources/template/how-to-guides/resource-sync-by-tgi.html#ignore-resources-updates-on-resources","title":"Ignore Resources Updates on Resources","text":"<p>If the resources mentioned in <code>Template</code> CR conflict with another controller/operator, and you want TemplateGroupInstance to not actively revert the resource updates, you can add the following label to the conflicting resource <code>multi-tenant-operator/ignore-resource-updates: \"\"</code>.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: docker-secret\nresources:\n  manifests:\n\n    - kind: Secret\n      apiVersion: v1\n      metadata:\n        name: docker-pull-secret\n      data:\n        .dockercfg: eyAKICAiaHR0cHM6IC8vaW5kZXguZG9ja2VyLmlvL3YxLyI6IHsgImF1dGgiOiAiYzNSaGEyRjBaWEk2VjI5M1YyaGhkRUZIY21WaGRGQmhjM04zYjNKayJ9Cn0K\n      type: kubernetes.io/dockercfg\n\n    - apiVersion: v1\n      kind: ServiceAccount\n      metadata:\n        name: example-automated-thing\n        labels:\n          multi-tenant-operator/ignore-resource-updates: \"\"\n      secrets:\n        - name: example-automated-thing-token-zyxwv\n</code></pre> <p>Note</p> <p>However, this label will not stop Multi Tenant Operator from updating the resource on following conditions: - Template gets updated - TemplateGroupInstance gets updated - Resource gets deleted</p> <p>If you don't want to sync the resources in any case, you can disable sync via <code>sync: false</code> in <code>TemplateGroupInstance</code> spec.</p>"},{"location":"kubernetes-resources/template/how-to-guides/template-default-params.html","title":"Using Templates with Default Parameters","text":"<pre><code>apiVersion: tenantoperator.stakater.com/v1alpha1\nkind: Template\nmetadata:\n  name: namespace-parameterized-restrictions\nparameters:\n  # Name of the parameter\n  - name: DEFAULT_CPU_LIMIT\n    # The default value of the parameter\n    value: \"1\"\n  - name: DEFAULT_CPU_REQUESTS\n    value: \"0.5\"\n    # If a parameter is required the template instance will need to set it\n    # required: true\n    # Make sure only values are entered for this parameter\n    validation: \"^[0-9]*\\\\.?[0-9]+$\"\nresources:\n  manifests:\n    - apiVersion: v1\n      kind: LimitRange\n      metadata:\n        name: namespace-limit-range-${namespace}\n      spec:\n        limits:\n          - default:\n              cpu: \"${{DEFAULT_CPU_LIMIT}}\"\n            defaultRequest:\n              cpu: \"${{DEFAULT_CPU_REQUESTS}}\"\n            type: Container\n</code></pre> <p>Parameters can be used with both <code>manifests</code> and <code>helm charts</code></p>"},{"location":"kubernetes-resources/template/how-to-guides/templated-metadata-values.html","title":"Templated values in Labels and Annotations","text":"<p>Templated values are placeholders in your configuration that get replaced with actual data when the CR is processed. Below is a list of currently supported templated values, their descriptions, and where they can be used.</p>"},{"location":"kubernetes-resources/template/how-to-guides/templated-metadata-values.html#supported-templated-values","title":"Supported templated values","text":"<ul> <li> <p><code>\"{{ TENANT.USERNAME }}\"</code></p> <ul> <li>Description: The username associated with users specified in Tenant under <code>Owners</code> and <code>Editors</code>.</li> <li>Supported in CRs:<ul> <li><code>Tenant</code>: Under <code>sandboxMetadata.labels</code> and <code>sandboxMetadata.annotations</code>.</li> <li><code>IntegrationConfig</code>: Under <code>metadata.sandboxs.labels</code> and <code>metadata.sandboxs.annotations</code>.</li> </ul> </li> <li>Example:</li> </ul> <pre><code>  annotation:\n    che.eclipse.org/username: \"{{ TENANT.USERNAME }}\" # double quotes are required\n</code></pre> </li> </ul>"},{"location":"kubernetes-resources/tenant/tenant-overview.html","title":"Tenant","text":"<p>A minimal Tenant definition requires only a quota field, essential for limiting resource consumption:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: alpha\nspec:\n  quota: small\n</code></pre> <p>For a more comprehensive setup, a detailed Tenant definition includes various configurations:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: tenant-sample\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - kubeadmin\n      groups:\n        - admin-group\n    editors:\n      users:\n        - devuser1\n        - devuser2\n      groups:\n        - dev-group\n    viewers:\n      users:\n        - viewuser\n      groups:\n        - view-group\n  hibernation:\n  # UTC time\n    sleepSchedule: \"20 * * * *\"\n    wakeSchedule: \"40 * * * *\"        \n  namespaces:\n    sandboxes:\n      enabled: true\n      private: true\n    withoutTenantPrefix:\n      - analytics\n      - marketing\n    withTenantPrefix:\n      - dev\n      - staging\n    onDeletePurgeNamespaces: true\n    metadata:\n      common:\n        labels:\n          common-label: common-value\n        annotations:\n          common-annotation: common-value\n      sandbox:\n        labels:\n          sandbox-label: sandbox-value\n        annotations:\n          sandbox-annotation: sandbox-value\n      specific:\n        - namespaces:\n            - tenant-sample-dev\n          labels:\n            specific-label: specific-dev-value\n          annotations:\n            specific-annotation: specific-dev-value\n  desc: \"This is a sample tenant setup for the v1beta3 version.\"\n  storageClasses:\n    allowed:\n      - staging\n      - dev\n  ingressClasses:\n    allowed:\n      - nginx\n      - trafeik\n\n  podPriorityClasses:\n    allowed:\n      - high-priority\n</code></pre>"},{"location":"kubernetes-resources/tenant/tenant-overview.html#access-control","title":"Access Control","text":"<p>Structured access control is critical for managing roles and permissions within a tenant effectively. It divides users into three categories, each with customizable privileges. This design enables precise role-based access management.  </p> <p>These roles are obtained from IntegrationConfig's TenantRoles field.</p> <ul> <li><code>Owners</code>: Have full administrative rights, including resource management and namespace creation. Their roles are crucial for high-level management tasks.</li> <li><code>Editors</code>: Granted permissions to modify resources, enabling them to support day-to-day operations without full administrative access.</li> <li><code>Viewers</code>: Provide read-only access, suitable for oversight and auditing without the ability to alter resources.</li> </ul> <p>Users and groups are linked to these roles by specifying their usernames or group names in the respective fields under <code>owners</code>, <code>editors</code>, and <code>viewers</code>.</p>"},{"location":"kubernetes-resources/tenant/tenant-overview.html#quota","title":"Quota","text":"<p>The <code>quota</code> field sets the resource limits for the tenant, such as CPU and memory usage, to prevent any single tenant from consuming a disproportionate amount of resources. This mechanism ensures efficient resource allocation and fosters fair usage practices across all tenants.  </p> <p>For more information on quotas, please refer here.</p>"},{"location":"kubernetes-resources/tenant/tenant-overview.html#namespaces","title":"Namespaces","text":"<p>Controls the creation and management of namespaces within the tenant:</p> <ul> <li> <p><code>sandboxes</code>:</p> <ul> <li>When enabled, sandbox namespaces are created with the following naming convention - {TenantName}-{UserName}-sandbox.</li> <li>In case of groups, the sandbox namespaces will be created for each member of the group.</li> <li>Setting <code>private</code> to true will make the sandboxes visible only to the user they belong to. By default, sandbox namespaces are visible to all tenant members.</li> </ul> </li> <li> <p><code>withoutTenantPrefix</code>: Lists the namespaces to be created without automatically prefixing them with the tenant name, useful for shared or common resources.</p> </li> <li><code>withTenantPrefix</code>: Namespaces listed here will be prefixed with the tenant name, ensuring easy identification and isolation.</li> <li><code>onDeletePurgeNamespaces</code>: Determines whether namespaces associated with the tenant should be deleted upon the tenant's deletion, enabling clean up and resource freeing.</li> <li><code>metadata</code>: Configures metadata like labels and annotations that are applied to namespaces managed by the tenant:<ul> <li><code>common</code>: Applies specified labels and annotations across all namespaces within the tenant, ensuring consistent metadata for resources and workloads.</li> <li><code>sandbox</code>: Special metadata for sandbox namespaces, which can include templated annotations or labels for dynamic information.<ul> <li>We also support the use of a templating mechanism within annotations, specifically allowing the inclusion of the tenant's username through the placeholder <code>{{ TENANT.USERNAME }}</code>. This template can be utilized to dynamically insert the tenant's username value into annotations, for example, as <code>username: {{ TENANT.USERNAME }}</code>.</li> </ul> </li> <li><code>specific</code>: Allows applying unique labels and annotations to specified tenant namespaces, enabling custom configurations for particular workloads or environments.</li> </ul> </li> </ul>"},{"location":"kubernetes-resources/tenant/tenant-overview.html#hibernation","title":"Hibernation","text":"<p><code>hibernation</code> allows for the scheduling of inactive periods for namespaces associated with the tenant, effectively putting them into a \"sleep\" mode. This capability is designed to conserve resources during known periods of inactivity.</p> <ul> <li>Configuration for this feature involves two key fields, <code>sleepSchedule</code> and <code>wakeSchedule</code>, both of which accept strings formatted according to cron syntax.</li> <li>These schedules dictate when the namespaces will automatically transition into and out of hibernation, aligning resource usage with actual operational needs.</li> </ul>"},{"location":"kubernetes-resources/tenant/tenant-overview.html#description","title":"Description","text":"<p><code>desc</code> provides a human-readable description of the tenant, aiding in documentation and at-a-glance understanding of the tenant's purpose and configuration.</p> <p>\u26a0\ufe0f If same label or annotation key is being applied using different methods provided, then the highest precedence will be given to <code>namespaces.metadata.specific</code> followed by <code>namespaces.metadata.common</code> and in the end would be the ones applied from <code>openshift.project.labels</code>/<code>openshift.project.annotations</code> in <code>IntegrationConfig</code></p>"},{"location":"kubernetes-resources/tenant/tenant-overview.html#storage","title":"Storage","text":"<pre><code>storageClasses:\n  allowed:\n    - staging-fast\n    - shared\n</code></pre> <ul> <li><code>allowed</code> can be used to limit a tenant to only being able to create PersistentVolumeClaims for StorageClasses in the list. If <code>storageClass</code> is not specified for a PersistentVolumeClaim, the default StorageClass (if set) will be evaluated as any other class name. If the default StorageClass is not set, the evaluation will be deferred until a default StorageClass is set. <code>\"\"</code> is evaluated as any other class name, so if you are using it to manually bind to PersistentVolumes while using StorageClass filtering you need to add  an empty string <code>\"\"</code> to the tenants allow-list or it will get filtered.</li> </ul>"},{"location":"kubernetes-resources/tenant/tenant-overview.html#ingress","title":"Ingress","text":"<p>Note</p> <p>This field is applicable only for Kubernetes. For more information, refer to the Ingress Sharding Guide.</p> <ul> <li><code>allowed</code> restricts a tenant to creating Ingress resources only with the specified IngressClasses. The empty string <code>\"\"</code> is treated like any other IngressClass name. If you use it while filtering IngressClasses, you must include <code>\"\"</code> in the tenant's allow-list, or it will be filtered out. If no IngressClass is specified for an Ingress resource, it will be treated as <code>\"\"</code>.</li> </ul> <pre><code>ingressClasses:\n  allowed:\n  - nginx\n  - traefik\n</code></pre>"},{"location":"kubernetes-resources/tenant/tenant-overview.html#pod-priority-classes","title":"Pod Priority Classes","text":"<ul> <li><code>allowed</code> restricts a tenant to creating pods only with the specified <code>priorityClasse</code>. The empty string <code>\"\"</code> is treated like any other <code>priorityClass</code> name. If you use it while filtering PodPriorityClasses, you must include <code>\"\"</code> in the tenant's allow-list, or it will be filtered out. If no PodPriorityClass is specified for a resource, it will be treated as <code>\"\"</code>.</li> </ul> <p>The following resources will be watched for PodPriorityClasses:</p> <ul> <li>Pods</li> <li>Deployments</li> <li>StatefulSets</li> <li>ReplicaSets</li> <li>Jobs</li> <li>CronJobs</li> <li>Daemonsets</li> </ul> <pre><code>podPriorityClasses:\n  allowed:\n    - high-priority\n</code></pre>"},{"location":"kubernetes-resources/tenant/how-to-guides/assign-metadata.html","title":"Assign Metadata","text":"<p>In the v1beta3 version of the Tenant Custom Resource (CR), metadata assignment has been refined to offer granular control over labels and annotations across different namespaces associated with a tenant. This functionality enables precise and flexible management of metadata, catering to both general and specific needs.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/assign-metadata.html#distributing-common-labels-and-annotations","title":"Distributing Common Labels and Annotations","text":"<p>To apply common labels and annotations across all namespaces within a tenant, the <code>namespaces.metadata.common</code> field in the Tenant CR is utilized. This approach ensures that essential metadata is uniformly present across all namespaces, supporting consistent identification, management, and policy enforcement.</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n        - anthony@aurora.org\n    editors:\n      users:\n        - john@aurora.org\n      groups:\n        - alpha\n  namespaces:\n    withTenantPrefix:\n      - dev\n      - build\n      - prod\n    metadata:\n      common:\n        labels:\n          app.kubernetes.io/managed-by: tenant-operator\n          app.kubernetes.io/part-of: tenant-alpha\n        annotations:\n          openshift.io/node-selector: node-role.kubernetes.io/infra=\nEOF\n</code></pre> <p>By configuring the <code>namespaces.metadata.common</code> field as shown, all namespaces within the tenant will inherit the specified labels and annotations.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/assign-metadata.html#distributing-specific-labels-and-annotations","title":"Distributing Specific Labels and Annotations","text":"<p>For scenarios requiring targeted application of labels and annotations to specific namespaces, the Tenant CR's <code>namespaces.metadata.specific</code> field is designed. This feature enables the assignment of unique metadata to designated namespaces, accommodating specialized configurations and requirements.</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n        - anthony@aurora.org\n    editors:\n      users:\n        - john@aurora.org\n      groups:\n        - alpha\n  namespaces:\n    withTenantPrefix:\n      - dev\n      - build\n      - prod\n    metadata:\n      specific:\n        - namespaces:\n            - bluesky-dev\n          labels:\n            app.kubernetes.io/is-sandbox: \"true\"\n          annotations:\n            openshift.io/node-selector: node-role.kubernetes.io/worker=\nEOF\n</code></pre> <p>This configuration directs the specific labels and annotations solely to the enumerated namespaces, enabling distinct settings for particular environments.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/assign-metadata.html#assigning-metadata-to-sandbox-namespaces","title":"Assigning Metadata to Sandbox Namespaces","text":"<p>To specifically address sandbox namespaces within the tenant, the <code>namespaces.metadata.sandbox</code> property of the Tenant CR is employed. This section allows for the distinct management of sandbox namespaces, enhancing security and differentiation in development or testing environments.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n        - anthony@aurora.org\n    editors:\n      users:\n        - john@aurora.org\n      groups:\n        - alpha\n  namespaces:\n    sandboxes:\n      enabled: true\n      private: true\n    metadata:\n      sandbox:\n        labels:\n          app.kubernetes.io/part-of: che.eclipse.org\n        annotations:\n          che.eclipse.org/username: \"{{ TENANT.USERNAME }}\" # templated placeholder\n</code></pre> <p>This setup ensures that all sandbox namespaces receive the designated metadata, with support for templated values, such as {{ TENANT.USERNAME }}, allowing dynamic customization based on the tenant or user context.</p> <p>These enhancements in metadata management within the <code>v1beta3</code> version of the Tenant CR provide comprehensive and flexible tools for labeling and annotating namespaces, supporting a wide range of organizational, security, and operational objectives.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-namespaces.html","title":"Create Namespaces","text":"<p>Bill, tasked with structuring namespaces for different environments within a tenant, utilizes the Tenant Custom Resource (CR) to streamline this process efficiently. Here's how Bill can orchestrate the creation of <code>dev</code>, <code>build</code>, and <code>production</code> environments for the tenant members directly through the Tenant CR.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-namespaces.html#strategy-for-namespace-creation","title":"Strategy for Namespace Creation","text":"<p>To facilitate the environment setup, Bill decides to categorize the namespaces based on their association with the tenant's name. He opts to use the <code>namespaces.withTenantPrefix</code> field for namespaces that should carry the tenant name as a prefix, enhancing clarity and organization. For namespaces that do not require a tenant name prefix, Bill employs the <code>namespaces.withoutTenantPrefix</code> field.</p> <p>Here's how Bill configures the Tenant CR to create these namespaces:</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n        - anthony@aurora.org\n    editors:\n      users:\n        - john@aurora.org\n      groups:\n        - alpha\n  namespaces:\n    withTenantPrefix:\n      - dev\n      - build\n    withoutTenantPrefix:\n      - prod\nEOF\n</code></pre> <p>This configuration ensures the creation of the desired namespaces, directly correlating them with the bluesky tenant.</p> <p>Upon applying the above configuration, Bill and the tenant members observe the creation of the following namespaces:</p> <pre><code>kubectl get namespaces\nNAME             STATUS   AGE\nbluesky-dev      Active   5m\nbluesky-build    Active   5m\nprod             Active   5m\n</code></pre> <p>Anna, as a tenant owner, gains the capability to further customize or create new namespaces within her tenant's scope. For example, creating a bluesky-production namespace with the necessary tenant label:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: bluesky-production\n  labels:\n    stakater.com/tenant: bluesky\n</code></pre> <p>\u26a0\ufe0f It's crucial for Anna to include the tenant label <code>tenantoperator.stakater.com/tenant: bluesky</code> to ensure the namespace is recognized as part of the bluesky tenant. Failure to do so, or if Anna is not associated with the bluesky tenant, will result in Multi Tenant Operator (MTO) denying the namespace creation.</p> <p>Following the creation, the MTO dynamically assigns roles to Anna and other tenant members according to their designated user types, ensuring proper access control and operational capabilities within these namespaces.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-namespaces.html#incorporating-existing-namespaces-into-the-tenant-via-argocd","title":"Incorporating Existing Namespaces into the Tenant via ArgoCD","text":"<p>For teams practicing GitOps, existing namespaces can be seamlessly integrated into the Tenant structure by appending the tenant label to the namespace's manifest within the GitOps repository. This approach allows for efficient, automated management of namespace affiliations and access controls, ensuring a cohesive tenant ecosystem.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-namespaces.html#add-existing-namespaces-to-tenant-via-gitops","title":"Add Existing Namespaces to Tenant via GitOps","text":"<p>Using GitOps as your preferred development workflow, you can add existing namespaces for your tenants by including the tenant label.</p> <p>To add an existing namespace to your tenant via GitOps:</p> <ol> <li>Migrate the namespace resource to the GitOps-monitored repository</li> <li>Amend the namespace manifest to include the tenant label: tenantoperator.stakater.com/tenant: . <li>Synchronize the GitOps repository with the cluster to propagate the changes</li> <li>Validate that the tenant users now have appropriate access to the integrated namespace</li>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-namespaces.html#removing-namespaces-via-gitops","title":"Removing Namespaces via GitOps","text":"<p>To disassociate or remove namespaces from the cluster through GitOps, the namespace configuration should be eliminated from the GitOps repository. Additionally, detaching the namespace from any ArgoCD-managed applications by removing the <code>app.kubernetes.io/instance</code> label ensures a clean removal without residual dependencies.</p> <p>Synchronizing the repository post-removal finalizes the deletion process, effectively managing the lifecycle of namespaces within a tenant-operated Kubernetes environment.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-sandbox.html","title":"Create Sandbox Namespaces","text":"<p>Sandbox namespaces offer a personal development and testing space for users within a tenant. This guide covers how to enable and configure sandbox namespaces for tenant users, along with setting privacy and applying metadata specifically for these sandboxes.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-sandbox.html#enabling-sandbox-namespaces","title":"Enabling Sandbox Namespaces","text":"<p>Bill has assigned the ownership of the tenant bluesky to Anna and Anthony. To provide them with their sandbox namespaces, he must enable the sandbox functionality in the tenant's configuration.</p> <p>To enable sandbox namespaces, Bill updates the Tenant Custom Resource (CR) with sandboxes.enabled: true:</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n        - anthony@aurora.org\n    editors:\n      users:\n        - john@aurora.org\n      groups:\n        - alpha\n  namespaces:\n    sandboxes:\n      enabled: true\nEOF\n</code></pre> <p>This configuration automatically generates sandbox namespaces for Anna, Anthony, and even John (as an editor) with the naming convention <code>&lt;tenantName&gt;-&lt;userName&gt;-sandbox</code>.</p> <pre><code>kubectl get namespaces\nNAME                             STATUS   AGE\nbluesky-anna-aurora-sandbox      Active   5d5h\nbluesky-anthony-aurora-sandbox   Active   5d5h\nbluesky-john-aurora-sandbox      Active   5d5h\n</code></pre>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-sandbox.html#creating-private-sandboxes","title":"Creating Private Sandboxes","text":"<p>To address privacy concerns where users require their sandbox namespaces to be visible only to themselves, Bill can set the <code>sandboxes.private: true</code> in the Tenant CR:</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n        - anthony@aurora.org\n    editors:\n      users:\n        - john@aurora.org\n      groups:\n        - alpha\n  namespaces:\n    sandboxes:\n      enabled: true\n      private: true\nEOF\n</code></pre> <p>With <code>private: true</code>, each sandbox namespace is accessible and visible only to its designated user, enhancing privacy and security.</p> <p>With the above configuration <code>Anna</code> and <code>Anthony</code> will now have new sandboxes created</p> <pre><code>kubectl get namespaces\nNAME                             STATUS   AGE\nbluesky-anna-aurora-sandbox      Active   5d5h\nbluesky-anthony-aurora-sandbox   Active   5d5h\nbluesky-john-aurora-sandbox      Active   5d5h\n</code></pre> <p>However, from the perspective of <code>Anna</code>, only their sandbox will be visible</p> <pre><code>kubectl get namespaces\nNAME                             STATUS   AGE\nbluesky-anna-aurora-sandbox      Active   5d5h\n</code></pre>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-sandbox.html#applying-metadata-to-sandbox-namespaces","title":"Applying Metadata to Sandbox Namespaces","text":"<p>For uniformity or to apply specific policies, Bill might need to add common metadata, such as labels or annotations, to all sandbox namespaces. This is achievable through the <code>namespaces.metadata.sandbox</code> configuration:</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n        - anthony@aurora.org\n    editors:\n      users:\n        - john@aurora.org\n      groups:\n        - alpha\n  namespaces:\n    sandboxes:\n      enabled: true\n      private: true\n    metadata:\n      sandbox:\n        labels:\n          app.kubernetes.io/part-of: che.eclipse.org\n        annotations:\n          che.eclipse.org/username: \"{{ TENANT.USERNAME }}\"\nEOF\n</code></pre> <p>The templated annotation \"{{ TENANT.USERNAME }}\" dynamically inserts the username of the sandbox owner, personalizing the sandbox environment. This capability is particularly useful for integrating with other systems or applications that might utilize this metadata for configuration or access control.</p> <p>Through the examples demonstrated, Bill can efficiently manage sandbox namespaces for tenant users, ensuring they have the necessary resources for development and testing while maintaining privacy and organizational policies.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-tenant.html","title":"Create a Tenant","text":"<p>Bill, a cluster admin, has been tasked by the CTO of Nordmart to set up a new tenant for Anna's team. Following the request, Bill proceeds to create a new tenant named bluesky in the Kubernetes cluster.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-tenant.html#setting-up-the-tenant","title":"Setting Up the Tenant","text":"<p>To establish the tenant, Bill crafts a Tenant Custom Resource (CR) with the necessary specifications:</p> <pre><code>kubectl create -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n    editors:\n      users:\n        - john@aurora.org\n      groups:\n        - alpha\n  namespaces:\n    sandboxes:\n      enabled: false\nEOF\n</code></pre> <p>In this configuration, Bill specifies anna@aurora.org as the owner, giving her full administrative rights over the tenant. The editor role is assigned to john@aurora.org and the group alpha, providing them with editing capabilities within the tenant's scope.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-tenant.html#verifying-the-tenant-creation","title":"Verifying the Tenant Creation","text":"<p>After creating the tenant, Bill checks its status to confirm it's active and operational:</p> <pre><code>kubectl get tenants.tenantoperator.stakater.com bluesky\nNAME       STATE    AGE\nbluesky    Active   3m\n</code></pre> <p>This output indicates that the tenant bluesky is successfully created and in an active state.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-tenant.html#checking-user-permissions","title":"Checking User Permissions","text":"<p>To ensure the roles and permissions are correctly assigned, Anna logs into the cluster to verify her capabilities:</p> <p>Namespace Creation:</p> <pre><code>kubectl auth can-i create namespaces\nyes\n</code></pre> <p>Anna is confirmed to have the ability to create namespaces within the tenant's scope.</p> <p>Cluster Resources Access:</p> <pre><code>kubectl auth can-i get namespaces\nno\n\nkubectl auth can-i get persistentvolumes\nno\n</code></pre> <p>As expected, Anna does not have access to broader cluster resources outside the tenant's confines.</p> <p>Tenant Resource Access:</p> <pre><code>kubectl auth can-i get tenants.tenantoperator.stakater.com\nno\n</code></pre> <p>Access to the Tenant resource itself is also restricted, aligning with the principle of least privilege.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/create-tenant.html#adding-multiple-owners-to-a-tenant","title":"Adding Multiple Owners to a Tenant","text":"<p>Later, if there's a need to grant administrative privileges to another user, such as Anthony, Bill can easily update the tenant's configuration to include multiple owners:</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n        - anthony@aurora.org\n    editors:\n      users:\n        - john@aurora.org\n      groups:\n        - alpha\n  namespaces:\n    sandboxes:\n      enabled: false\nEOF\n</code></pre> <p>With this update, both Anna and Anthony can administer the tenant bluesky, including the creation of namespaces:</p> <pre><code>kubectl auth can-i create namespaces\nyes\n</code></pre> <p>This flexible approach allows Bill to manage tenant access control efficiently, ensuring that the team's operational needs are met while maintaining security and governance standards.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/custom-roles.html","title":"Changing the default access level for tenant owners","text":"<p>This feature allows the cluster admins to change the default roles assigned to Tenant owner, editor, viewer groups.</p> <p>For example, if Bill as the cluster admin wants to reduce the privileges that tenant owners have, so they cannot create or edit Roles or bind them. As an admin of an OpenShift cluster, Bill can do this by assigning the <code>edit</code> role to all tenant owners. This is easily achieved by modifying the IntegrationConfig:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: IntegrationConfig\nmetadata:\n  name: tenant-operator-config\n  namespace: multi-tenant-operator\nspec:\n  accessControl:\n    rbac:\n      tenantRoles:\n        default:\n          owner:\n            clusterRoles:\n              - edit\n          editor:\n            clusterRoles:\n              - edit\n          viewer:\n            clusterRoles:\n              - view\n</code></pre> <p>Once all namespaces reconcile, the old <code>admin</code> RoleBindings should get replaced with the <code>edit</code> ones for each tenant owner.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/custom-roles.html#giving-specific-permissions-to-some-tenants","title":"Giving specific permissions to some tenants","text":"<p>Bill now wants the owners of the tenants <code>bluesky</code> and <code>alpha</code> to have <code>admin</code> permissions over their namespaces. Custom roles feature will allow Bill to do this, by modifying the IntegrationConfig like this:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: IntegrationConfig\nmetadata:\n  name: tenant-operator-config\n  namespace: multi-tenant-operator\nspec:\n  accessControl:\n    rbac:\n      tenantRoles:\n        default:\n          owner:\n            clusterRoles:\n              - edit\n          editor:\n            clusterRoles:\n              - edit\n          viewer:\n            clusterRoles:\n              - view\n        custom:\n        - labelSelector:\n            matchExpressions:\n            - key: stakater.com/tenant\n              operator: In\n              values:\n                - alpha\n          owner:\n            clusterRoles:\n              - admin\n        - labelSelector:\n            matchExpressions:\n            - key: stakater.com/tenant\n              operator: In\n              values:\n                - bluesky\n          owner:\n            clusterRoles:\n              - admin\n</code></pre> <p>New Bindings will be created for the Tenant owners of <code>bluesky</code> and <code>alpha</code>, corresponding to the <code>admin</code> Role. Bindings for editors and viewer will be inherited from the <code>default roles</code>. All other Tenant owners will have an <code>edit</code> Role bound to them within their namespaces</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/delete-tenant.html","title":"Delete a Tenant","text":"<p>When managing tenant lifecycles within Kubernetes, certain scenarios require the deletion of a tenant without removing associated namespaces or ArgoCD AppProjects. This ensures that resources and configurations tied to the tenant remain intact for archival or transition purposes.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/delete-tenant.html#configuration-for-retaining-resources","title":"Configuration for Retaining Resources","text":"<p>Bill decides to decommission the bluesky tenant but needs to preserve all related namespaces for continuity. To achieve this, he adjusts the Tenant Custom Resource (CR) to prevent the automatic cleanup of these resources upon tenant deletion.</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: bluesky\nspec:\n  quota: small\n  accessControl:\n    owners:\n      users:\n        - anna@aurora.org\n        - anthony@aurora.org\n  namespaces:\n    sandboxes:\n      enabled: true\n    withTenantPrefix:\n      - dev\n      - build\n      - prod\n    onDeletePurgeNamespaces: false\nEOF\n</code></pre> <p>With the <code>onDeletePurgeNamespaces</code> fields set to false, Bill ensures that the deletion of the bluesky tenant does not trigger the removal of its namespaces. This setup is crucial for cases where the retention of environment setups and deployments is necessary post-tenant deletion.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/delete-tenant.html#default-behavior","title":"Default Behavior","text":"<p>It's important to note the default behavior of the Tenant Operator regarding resource cleanup:</p> <p>Namespaces: By default, <code>onDeletePurgeNamespaces</code> is set to false, implying that namespaces are not automatically deleted with the tenant unless explicitly configured.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/delete-tenant.html#deleting-the-tenant","title":"Deleting the Tenant","text":"<p>Once the Tenant CR is configured as desired, Bill can proceed to delete the bluesky tenant:</p> <pre><code>kubectl delete tenant bluesky\n</code></pre> <p>This command removes the tenant resource from the cluster while leaving the specified namespaces untouched, adhering to the configured <code>onDeletePurgeNamespaces</code> policies. This approach provides flexibility in managing the lifecycle of tenant resources, catering to various operational strategies and compliance requirements.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/disable-intra-tenant-networking.html","title":"Disable intra-tenant networking","text":"Integration Configuration<pre><code>apiVersion: v1beta1\nkind: integrationconfigs.tenantoperator.stakater.com\nspec:\n    # other fields...\n    tenantPolicies:\n        network:\n            disableIntraTenantNetworking: true\n</code></pre> <p>The flag works by deploying a set of <code>NetworPolicies</code> for each tenant which filters incoming traffic coming from another tenants namespace. It allows all other traffic.</p> <p>The <code>NetworkPolicy</code> is as follows:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: disable-intra-tenant-networking-${tenant} # tenant will be substituted for the tenant-name\n  namespace: test # Will be deployed to all the tenants namespaces\nspec:\n  podSelector: {} # The rule selects all pods\n  policyTypes:\n    - Ingress # We only filter incoming traffic\n  ingress:\n    - from:\n      - namespaceSelector:\n          matchExpressions:\n            - key: stakater.com/tenant\n              operator: DoesNotExist\n      - namespaceSelector:\n          matchLabels:\n          stakater.com/tenant: ${tenant}\n</code></pre>"},{"location":"kubernetes-resources/tenant/how-to-guides/disable-intra-tenant-networking.html#demo","title":"Demo","text":""},{"location":"kubernetes-resources/tenant/how-to-guides/extend-default-roles.html","title":"Extending the default access level for tenant members","text":"<p>Bill as the cluster admin wants to extend the default access for tenant members. As an admin of an OpenShift Cluster, Bill can extend the admin, edit, and view ClusterRole using aggregation. Bill will first create a ClusterRole with privileges to resources which Bill wants to extend. Bill will add the aggregation label to the newly created ClusterRole for extending the default ClusterRoles.</p> <pre><code>kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: extend-admin-role\n  labels:\n    rbac.authorization.k8s.io/aggregate-to-admin: 'true'\nrules:\n  - verbs:\n      - create\n      - update\n      - patch\n      - delete\n    apiGroups:\n      - user.openshift.io\n    resources:\n      - groups\n</code></pre> <p>Note</p> <p>You can learn more about <code>aggregated-cluster-roles</code> here</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/hibernate-tenant.html","title":"Hibernate a Tenant","text":"<p>Implementing hibernation for tenants' namespaces efficiently manages cluster resources by temporarily reducing workload activities during off-peak hours. This guide demonstrates how to configure hibernation schedules for tenant namespaces, leveraging Tenant and ResourceSupervisor for precise control.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/hibernate-tenant.html#configuring-hibernation-for-tenant-namespaces","title":"Configuring Hibernation for Tenant Namespaces","text":"<p>You can manage workloads in your cluster with MTO by implementing a hibernation schedule for your tenants. Hibernation downsizes the running <code>Deployments</code> and <code>StatefulSets</code> in a tenant\u2019s namespace according to a defined cron schedule. You can set a hibernation schedule for your tenants by adding the \u2018spec.hibernation\u2019 field to the tenant's respective Custom Resource.</p> <pre><code>hibernation:\n  sleepSchedule: 23 * * * *\n  wakeSchedule: 26 * * * *\n</code></pre> <p><code>spec.hibernation.sleepSchedule</code> accepts a cron expression indicating the time to put the workloads in your tenant\u2019s namespaces to sleep.</p> <p><code>spec.hibernation.wakeSchedule</code> accepts a cron expression indicating the time to wake the workloads in your tenant\u2019s namespaces up.</p> <p>Note</p> <p>Both sleep and wake schedules must be specified for your tenant's hibernation schedule to be valid.</p> <p>Additionally, adding the <code>hibernation.stakater.com/exclude: 'true'</code> annotation to a namespace excludes it from hibernating.</p> <p>Note</p> <p>This is only true for hibernation applied via the Tenant Custom Resource, and does not apply for hibernation done by manually creating a ResourceSupervisor (details about that below).</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/hibernate-tenant.html#resource-supervisor","title":"Resource Supervisor","text":"<p>Adding a Hibernation Schedule to a Tenant creates an accompanying ResourceSupervisor Custom Resource.</p> <p>When the sleep timer is activated, the Resource Supervisor puts your applications to sleep and store their previous state. When the wake timer is activated, it uses the stored state to bring them back to running state.</p> <p>Enabling ArgoCD support for Tenants will also hibernate applications in the tenants' <code>appProjects</code>.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: ResourceSupervisor\nmetadata:\n  name: sigma-tenant\nspec:\n  argocd:\n    appProjects:\n      - sigma-tenant\n    namespace: openshift-gitops\n  schedule:\n    sleepSchedule: 42 * * * *\n    wakeSchedule: 45 * * * *\n\n  namespaces:\n    labelSelector:\n      matchLabels:\n        stakater.com/current-tenant: sigma\n      matchExpressions: {}\n    names:\n    - tenant-ns1\n    - tenant-ns2\n</code></pre> <p>Currently, Hibernation is available only for <code>StatefulSets</code> and <code>Deployments</code>.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/hibernate-tenant.html#manual-creation-of-resourcesupervisor","title":"Manual creation of ResourceSupervisor","text":"<p>Hibernation can also be applied by creating a ResourceSupervisor resource manually. The ResourceSupervisor definition will contain the hibernation cron schedule, the names of the namespaces to be hibernated, and the names of the ArgoCD AppProjects whose ArgoCD Applications have to be hibernated (as per the given schedule).</p> <p>This method can be used to hibernate:</p> <ul> <li>Some specific namespaces and AppProjects in a tenant</li> <li>A set of namespaces and AppProjects belonging to different tenants</li> <li>Namespaces and AppProjects belonging to a tenant that the cluster admin is not a member of</li> <li>Non-tenant namespaces and ArgoCD AppProjects</li> </ul> <p>As an example, the following ResourceSupervisor could be created manually, to apply hibernation explicitly to the 'ns1' and 'ns2' namespaces, and to the 'sample-app-project' AppProject.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: ResourceSupervisor\nmetadata:\n  name: hibernator\nspec:\n  argocd:\n    appProjects:\n      - sample-app-project\n    namespace: openshift-gitops\n  schedule:\n    sleepSchedule: 42 * * * *\n    wakeSchedule: 45 * * * *\n\n  namespaces:\n    labelSelector:\n      matchLabels: {}\n      matchExpressions: {}\n    names:\n    - ns1\n    - ns2\n</code></pre>"},{"location":"kubernetes-resources/tenant/how-to-guides/hibernate-tenant.html#freeing-up-unused-resources-with-hibernation","title":"Freeing up unused resources with hibernation","text":""},{"location":"kubernetes-resources/tenant/how-to-guides/hibernate-tenant.html#hibernating-a-tenant","title":"Hibernating a tenant","text":"<p>Bill is a cluster administrator who wants to free up unused cluster resources at nighttime, in an effort to reduce costs (when the cluster isn't being used).</p> <p>First, Bill creates a tenant with the <code>hibernation</code> schedules mentioned in the spec, or adds the hibernation field to an existing tenant:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: sigma\nspec:\n  hibernation:\n    sleepSchedule: \"0 20 * * 1-5\"  # Sleep at 8 PM on weekdays\n    wakeSchedule: \"0 8 * * 1-5\"    # Wake at 8 AM on weekdays\n  accessControl:\n    owners:\n      users:\n        - user@example.com\n  quota: medium\n  namespaces:\n    withoutTenantPrefix:\n      - dev\n      - stage\n      - build\n</code></pre> <p>The schedules above will put all the <code>Deployments</code> and <code>StatefulSets</code> within the tenant's namespaces to sleep, by reducing their pod count to 0 at 8 PM every weekday. At 8 AM on weekdays, the namespaces will then wake up by restoring their applications' previous pod counts.</p> <p>Bill can verify this behaviour by checking the newly created ResourceSupervisor resource at run time:</p> <pre><code>oc get ResourceSupervisor -A\nNAME           AGE\nsigma          5m\n</code></pre> <p>The ResourceSupervisor will look like this at 'running' time (as per the schedule):</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: ResourceSupervisor\nmetadata:\n  finalizers:\n  - tenantoperator.stakater.com/resourcesupervisor\n  generation: 1\n  name: sigma\n  ownerReferences:\n  - apiVersion: tenantoperator.stakater.com/v1beta3\n    blockOwnerDeletion: true\n    controller: true\n    kind: Tenant\n    name: sigma\nspec:\n  argocd:\n    appProjects: []\n    namespace: \"\"\n  namespaces:\n    names:\n    - stage\n    - build\n    - dev\n  schedule:\n    sleepSchedule: 0 20 * * 1-5\n    wakeSchedule: 0 8 * * 1-5\nstatus:\n  currentStatus: running\n  nextReconcileTime: \"2024-06-10T20:00:00Z\"\n</code></pre> <p>The ResourceSupervisor will look like this at 'sleeping' time (as per the schedule):</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: ResourceSupervisor\nmetadata:\n  name: example\nspec:\n  argocd:\n    appProjects: []\n    namespace: ''\n  schedule:\n    sleepSchedule: 0 20 * * 1-5\n    wakeSchedule: 0 8 * * 1-5\n  namespaces:\n    labelSelector:\n      matchLabels: {}\n      matchExpressions: {}\n    names:\n      - build\n      - stage\n      - dev\nstatus:\n  currentStatus: sleeping\n  nextReconcileTime: '2024-06-11T08:00:00Z'\n  sleepingNamespaces:\n  - Namespace: build\n    sleepingApplications:\n    - kind: Deployment\n      name: Example\n      replicas: 3\n  - Namespace: stage\n    sleepingApplications:\n    - kind: Deployment\n      name: Example\n      replicas: 3\n</code></pre> <p>Bill wants to prevent the <code>build</code> namespace from going to sleep, so he can add the <code>hibernation.stakater.com/exclude: 'true'</code> annotation to it. The ResourceSupervisor will now look like this after reconciling:</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: ResourceSupervisor\nmetadata:\n  name: example\nspec:\n  argocd:\n    appProjects: []\n    namespace: ''\n  schedule:\n    sleepSchedule: 0 20 * * 1-5\n    wakeSchedule: 0 8 * * 1-5\n  namespaces:\n    labelSelector:\n      matchLabels: {}\n      matchExpressions: {}\n    names:\n      - stage\n      - dev\nstatus:\n  currentStatus: sleeping\n  nextReconcileTime: '2024-07-12T08:00:00Z'\n  sleepingNamespaces:\n  - Namespace: build\n    sleepingApplications:\n    - kind: Deployment\n      name: example\n      replicas: 3\n</code></pre>"},{"location":"kubernetes-resources/tenant/how-to-guides/hibernate-tenant.html#hibernating-namespaces-andor-argocd-applications-with-resourcesupervisor","title":"Hibernating namespaces and/or ArgoCD Applications with ResourceSupervisor","text":"<p>Bill, the cluster administrator, wants to hibernate a collection of namespaces and AppProjects belonging to multiple different tenants. He can do so by creating a ResourceSupervisor manually, specifying the hibernation schedule in its spec, the namespaces and ArgoCD Applications that need to be hibernated as per the mentioned schedule. Bill can also use the same method to hibernate some namespaces and ArgoCD Applications that do not belong to any tenant on his cluster.</p> <p>The example given below will hibernate the ArgoCD Applications in the 'test-app-project' AppProject; and it will also hibernate the 'ns2' and 'ns4' namespaces.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: ResourceSupervisor\nmetadata:\n  name: test-resource-supervisor\nspec:\n  argocd:\n    appProjects:\n      - test-app-project\n    namespace: argocd-ns\n  schedule:\n    sleepSchedule: 0 20 * * 1-5\n    wakeSchedule: 0 8 * * 1-5\n  namespaces:\n    labelSelector:\n      matchLabels: {}\n      matchExpressions: {}\n    names:\n      - ns2\n      - ns4\nstatus:\n  currentStatus: sleeping\n  nextReconcileTime: '2022-10-13T08:00:00Z'\n  sleepingNamespaces:\n  - Namespace: build\n    sleepingApplications:\n    - kind: Deployment\n      name: test-deployment\n      replicas: 3\n    - kind: Deployment\n      name: test-deployment\n      replicas: 3\n</code></pre> <p>For more info see here</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/ingress-sharding.html","title":"Ingress Sharding","text":""},{"location":"kubernetes-resources/tenant/how-to-guides/ingress-sharding.html#ingress-sharding-on-kubernetes-using-mto","title":"Ingress Sharding on Kubernetes using MTO","text":"Tenant<pre><code>apiVersion: tenantoperator.stakater.com/v1beta3\nkind: Tenant\nmetadata:\n  name: tenant-sample\nspec:\n  # other fields\n  ingressClasses:\n    allowed:\n    - nginx\n    - traefik\n</code></pre> <p>The <code>ingressClasses</code> field in the Tenant specification is used to filter and control which ingress classes are allowed for the tenant. By specifying an <code>allowed</code> list, you can restrict the creation of ingress resources to only those with the specified ingress class names. In the example above, only ingress resources with the class names <code>nginx</code> and <code>traefik</code> are permitted for the tenant.</p> <p>This ensures that tenants can only use the ingress controllers that are explicitly allowed, providing better control and security over the ingress resources within the Kubernetes cluster.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/ingress-sharding.html#demo","title":"Demo","text":""},{"location":"kubernetes-resources/tenant/how-to-guides/ingress-sharding.html#ingress-sharding-on-openshift-container-platform","title":"Ingress Sharding on OpenShift Container Platform","text":"<p>You can use Ingress sharding, also known as router sharding, to distribute a set of routes across multiple routers by adding labels to routes, namespaces, or both. The Ingress Controller uses a corresponding set of selectors to admit only the routes that have a specified label. Each Ingress shard comprises the routes that are filtered using a given selection expression.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/ingress-sharding.html#example","title":"Example","text":"<p>An Ingress Controller <code>finops-router</code> is configured with the label selector to handle the routes in tenant <code>finops</code></p> <pre><code>apiVersion: operator.openshift.io/v1\nkind: IngressController\nmetadata:\n  name: finops-router\n  namespace: openshift-ingress-operator\nspec:\n  - namespaceSelector:\n      matchLabels:\n        stakater.com/tenant: finops\n</code></pre>"},{"location":"kubernetes-resources/tenant/how-to-guides/restrict-nodepool-per-tenant.html","title":"Restricting Tenant Workloads to Specific Nodes","text":"<p>Utilizing Kubernetes Node Selectors and the Multi-Tenant Operator's advanced templating capabilities allows administrators to restrict tenant workloads to specific nodes or node groups.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/restrict-nodepool-per-tenant.html#azure-kubernetes-service-aks-node-pools","title":"Azure Kubernetes Service (AKS) Node Pools","text":"<p>To confine a tenant's workloads to a specific AKS NodePool, the PodNodeSelector admission controller can be used. This controller ensures all workloads in specified namespaces are scheduled on the designated node pool by setting the annotation <code>scheduler.alpha.kubernetes.io/node-selector</code> to <code>agentpool=&lt;nodepool_name&gt;</code>.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/restrict-nodepool-per-tenant.html#prerequisites","title":"Prerequisites","text":"<ul> <li>An AKS cluster.</li> <li>An existing node pool, for example, one named <code>marketing-pool</code>.</li> </ul>"},{"location":"kubernetes-resources/tenant/how-to-guides/restrict-nodepool-per-tenant.html#how-to","title":"How To","text":"<p>To ensure all namespaces associated with the tenant <code>marketing</code> are scheduled on the <code>marketing-pool</code> node pool, the annotation <code>scheduler.alpha.kubernetes.io/node-selector</code> with value <code>agentpool=marketing-pool</code> is added to <code>tenant.spec.namespaces.metadata.common.annotations</code>.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: Tenant\nmetadata:\n  name: marketing\nspec:\n  # Some fields have been omitted for clarity\n  quota: quota-sample\n  namespaces:\n    withTenantPrefix:\n      - alpha\n      - beta\n    metadata:\n      common:\n        annotations: # these annotations will be added to *all* of the tenants namespaces\n          scheduler.alpha.kubernetes.io/node-selector: agentpool=marketing-pool\n</code></pre>"},{"location":"kubernetes-resources/tenant/how-to-guides/restrict-nodepool-per-tenant.html#result","title":"Result","text":"<p>Once the <code>Tenant</code> resource is deployed, all workloads for the <code>marketing</code> tenant will be scheduled in the <code>marketing-pool</code> node pool.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/restrict-nodepool-per-tenant.html#openshift","title":"OpenShift","text":"<p>For OpenShift environments, workloads can be restricted to specific machine pools. This process is applicable across different OpenShift versions, though the method of creating the machine pool might vary. To restrict workloads, first create a machine pool and assign it a unique label. Then, use the annotation <code>openshift.io/node-selector</code> in the tenant\u2019s namespaces, which ensures that workloads are scheduled on nodes with matching label key-value pairs.</p>"},{"location":"kubernetes-resources/tenant/how-to-guides/restrict-nodepool-per-tenant.html#prerequisites_1","title":"Prerequisites","text":"<ul> <li>A RedHat OpenShift cluster.</li> <li>A configured machine pool. Ensure the machine pool has a unique label, which can be added during creation or configured after creation. For instance, the machine pool in this example is labeled as <code>pool-name=marketing-pool</code>.</li> </ul>"},{"location":"kubernetes-resources/tenant/how-to-guides/restrict-nodepool-per-tenant.html#how-to_1","title":"How To","text":"<p>To restrict workloads for the tenant, add the annotation <code>openshift.io/node-selector</code> with a value of <code>pool-name=marketing-pool</code> to <code>tenant.spec.namespaces.metadata.common.annotations</code>.</p> <pre><code>apiVersion: tenantoperator.stakater.com/v1beta1\nkind: Tenant\nmetadata:\n  name: marketing\nspec:\n  # Some fields have been omitted for clarity\n  quota: quota-sample\n  namespaces:\n    withTenantPrefix:\n      - alpha\n      - beta\n    metadata:\n      common:\n        annotations: # these annotations will be added to *all* of the tenants namespaces\n          openshift.io/node-selector: pool-name=marketing-pool\n</code></pre>"},{"location":"kubernetes-resources/tenant/how-to-guides/restrict-nodepool-per-tenant.html#result_1","title":"Result","text":"<p>When deployed, this configuration ensures that all workloads in the tenant's namespaces are scheduled on nodes in the <code>marketing-pool</code>.</p>"}]}